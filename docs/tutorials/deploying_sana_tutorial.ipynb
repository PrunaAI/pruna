{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize and Deploy AI Models with Pruna and Hugging Face\n",
    "\n",
    "Objective: Build a complete tutorial demonstrating how to optimize the [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED) diffusion model using Pruna and deploy it seamlessly to the Hugging Face Hub.\n",
    "\n",
    "Model: [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED)\n",
    "\n",
    "Dataset: [data-is-better-together/open-image-preferences-v1-binarized](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized)\n",
    "\n",
    "To follow along, ensure that you have the Pruna SDK installed along with all required third-party libraries. Running this tutorial in a clean virtual environment is recommended for a smooth setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/deploying_sana_tutorial.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pruna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets huggingface_hub gradio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to login on the Hugging Face Hub for using the model weights. We also need to select the best available device for executing the notebook. Run the cells below to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cddba2c14fe4f0fa3761154584ebc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smash Configuration\n",
    "\n",
    "To optimize the model effectively, we first define the configuration methods that enhance its performance. For detailed options and parameter explanations, refer to the [SmashConfig guide](https://docs.pruna.ai/en/stable/docs_pruna/user_manual/configure.html).\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "* Select **hqq_diffusers** to reduce memory usage during inference.\n",
    "* Set the **weight bit precision** to 8 bits for the diffusers model:\n",
    "* Upload the optimized (“smashed”) model to the Hugging Face Hub for easy access, sharing, and deployment in downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "from pruna import PrunaModel, SmashConfig, smash\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"Efficient-Large-Model/Sana_600M_512px_diffusers\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "pipe = SanaPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# 2. Configure Pruna smash\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"quantizer\"] = \"hqq_diffusers\"  # Quantizer to reduce memory usage\n",
    "smash_config[\"hqq_diffusers_weight_bits\"] = 8\n",
    "\n",
    "# 3. Smash (optimize) the model\n",
    "smashed_pipe = smash(model=pipe, smash_config=smash_config)\n",
    "\n",
    "# 4. Push the smashed pipeline to Hugging Face Hub using save_to_hub\n",
    "smashed_pipe.save_to_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "print(\"✅ Smashed Sana model uploaded successfully to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load and Collate Dataset**\n",
    "\n",
    "In this step, we will load the dataset required for optimizing and evaluating the model. This dataset will provide the input data needed to assess the model’s performance after applying optimization techniques such as quantization.\n",
    "\n",
    "We will use the [`data-is-better-together/open-image-preferences-v1-binarized`](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized) dataset, which contains binarized user image preferences and prompts for image generation tasks. Correctly loading and collating the dataset ensures that the input is properly prepared, enabling smooth evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Multiple distributions found for package optimum. Picked distribution: optimum\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with image_generation_collate...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, Image\n",
    "\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"data-is-better-together/open-image-preferences-v1-binarized\")[\"train\"]\n",
    "\n",
    "dataset = dataset.rename_column(\"chosen\", \"image\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"text\")\n",
    "\n",
    "# Cast 'image' column to Image type so outputs are PIL Images\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "\n",
    "# Split train into train/val/test\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(dataset, seed=42)\n",
    "\n",
    "# Initialize PrunaDataModule\n",
    "datamodule = PrunaDataModule.from_datasets(\n",
    "    datasets=(train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"image_generation_collate\",\n",
    "    collate_fn_args={\"img_size\": 512,\"output_format\": \"float\"},\n",
    ")\n",
    "\n",
    "# Limit datasets to 5 samples each for quick testing\n",
    "datamodule.limit_datasets(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with image_generation_collate...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Compatibility test failed with error: 'list' object has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/data/pruna_datamodule.py:124\u001b[39m, in \u001b[36mPrunaDataModule.from_datasets\u001b[39m\u001b[34m(cls, datasets, collate_fn, tokenizer, collate_fn_args, dataloader_args)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m [train_ds, val_ds, test_ds]:\n\u001b[32m    123\u001b[39m         \u001b[38;5;66;03m# try collating two samples to test if batching works with the dataset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[43mcollate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/data/collate.py:85\u001b[39m, in \u001b[36mimage_generation_collate\u001b[39m\u001b[34m(data, img_size, output_format)\u001b[39m\n\u001b[32m     84\u001b[39m image = item[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m != \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     86\u001b[39m     image = image.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'mode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m train_ds, val_ds, test_ds = split_train_into_train_val_test(dataset, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Initialize PrunaDataModule\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m datamodule = \u001b[43mPrunaDataModule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_generation_collate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# If your collate handles tensors, else use simple stacking collate\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No img_size/output_format needed as tensors are preprocessed\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Limit datasets to 5 samples each for quick testing\u001b[39;00m\n\u001b[32m     48\u001b[39m datamodule.limit_datasets(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/data/pruna_datamodule.py:126\u001b[39m, in \u001b[36mPrunaDataModule.from_datasets\u001b[39m\u001b[34m(cls, datasets, collate_fn, tokenizer, collate_fn_args, dataloader_args)\u001b[39m\n\u001b[32m    124\u001b[39m         collate_func([\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(ds))] * \u001b[32m2\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompatibility test failed with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(train_ds, val_ds, test_ds, collate_func, dataloader_args)\n",
      "\u001b[31mValueError\u001b[39m: Compatibility test failed with error: 'list' object has no attribute 'mode'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"data-is-better-together/open-image-preferences-v1-binarized\")[\"train\"]\n",
    "\n",
    "dataset = dataset.rename_column(\"chosen\", \"image\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"text\")\n",
    "\n",
    "# Cast 'image' column to Image type so outputs are PIL Images\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "\n",
    "# Select only first 500 images\n",
    "dataset = dataset.select(range(200))\n",
    "\n",
    "# Define transform to convert images to half precision tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),        # Converts to float32 tensor [0,1]\n",
    "    lambda x: x.half(),           # Converts to float16\n",
    "])\n",
    "\n",
    "# Map transform to dataset\n",
    "def process_to_half(example):\n",
    "    image = example[\"image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    example[\"image\"] = transform(image)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(process_to_half)\n",
    "\n",
    "# Split train into train/val/test\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(dataset, seed=42)\n",
    "\n",
    "# Initialize PrunaDataModule\n",
    "datamodule = PrunaDataModule.from_datasets(\n",
    "    datasets=(train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"image_generation_collate\",  # If your collate handles tensors, else use simple stacking collate\n",
    "    collate_fn_args={\"img_size\": 512},  # No img_size/output_format needed as tensors are preprocessed\n",
    ")\n",
    "\n",
    "# Limit datasets to 5 samples each for quick testing\n",
    "datamodule.limit_datasets(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Model\n",
    "\n",
    "Now that the model and dataset are set up, we can proceed to evaluate the model using the **Pruna Evaluation Agent**. This evaluation helps us measure the model’s current performance before optimization, providing a baseline for comparison. It assesses how well the model performs on the given dataset and generates relevant metrics that will guide us in understanding the impact of our optimization configurations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:00<00:00, 101203.05it/s]\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n",
      "/teamspace/studios/this_studio/.venv/lib/python3.11/site-packages/diffusers/configuration_utils.py:248: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 231/231 [00:00<00:00, 53037.24it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 5286.23it/s]\n",
      "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.83s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "Loading pipeline components...: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "WARNING - Could not move model to device: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected diffusers model. Using DiffuserHandler with fixed seed.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .images attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (float) and bias type (c10::Half) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m smashed_pipe.move_to_device(device)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Evaluate the smashed model pipeline using the evaluation agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m smashed_model_results = \u001b[43meval_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmashed_pipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Optionally, print results for verification\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(smashed_model_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/evaluation/evaluation_agent.py:107\u001b[39m, in \u001b[36mEvaluationAgent.evaluate\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Compute stateless metrics.\u001b[39;00m\n\u001b[32m    106\u001b[39m pruna_logger.info(\u001b[33m\"\u001b[39m\u001b[33mEvaluating isolated inference metrics.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m results.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_stateless_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstateless_metrics\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    109\u001b[39m model.move_to_device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m safe_memory_cleanup()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/evaluation/evaluation_agent.py:260\u001b[39m, in \u001b[36mEvaluationAgent.compute_stateless_metrics\u001b[39m\u001b[34m(self, model, stateless_metrics)\u001b[39m\n\u001b[32m    257\u001b[39m parent_to_children, children_of_base = group_metrics_by_inheritance(stateless_metrics)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (parent, _), children \u001b[38;5;129;01min\u001b[39;00m parent_to_children.items():\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Get the metrics that share a common parent to share inference computation by calling the parent metric.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     raw_results = \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[32m    262\u001b[39m         results.append(\n\u001b[32m    263\u001b[39m             MetricResult.from_results_dict(child.metric_name, \u001b[38;5;28mdict\u001b[39m(children[\u001b[32m0\u001b[39m].\u001b[34m__dict__\u001b[39m), raw_results)\n\u001b[32m    264\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/evaluation/metrics/metric_elapsed_time.py:164\u001b[39m, in \u001b[36mInferenceTimeStats.compute\u001b[39m\u001b[34m(self, model, dataloader)\u001b[39m\n\u001b[32m    161\u001b[39m model.move_to_device(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_warmup_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# x is a dict\u001b[39;49;00m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# x is tensor/list\u001b[39;49;00m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Measurement\u001b[39;00m\n\u001b[32m    176\u001b[39m list_elapsed_times = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/evaluation/metrics/metric_elapsed_time.py:96\u001b[39m, in \u001b[36mInferenceTimeStats._measure\u001b[39m\u001b[34m(self, model, dataloader, iterations, measure_fn)\u001b[39m\n\u001b[32m     94\u001b[39m batch = model.inference_handler.move_inputs_to_device(batch, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     95\u001b[39m x = model.inference_handler.prepare_inputs(batch)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mmeasure_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m c += \u001b[32m1\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c >= iterations:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/evaluation/metrics/metric_elapsed_time.py:171\u001b[39m, in \u001b[36mInferenceTimeStats.compute.<locals>.<lambda>\u001b[39m\u001b[34m(m, x)\u001b[39m\n\u001b[32m    161\u001b[39m model.move_to_device(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mself\u001b[39m._measure(\n\u001b[32m    165\u001b[39m     model,\n\u001b[32m    166\u001b[39m     dataloader,\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_warmup_iterations,\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m m, x: (\n\u001b[32m    169\u001b[39m         m(**x, **m.inference_handler.model_args)  \u001b[38;5;66;03m# x is a dict\u001b[39;00m\n\u001b[32m    170\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# x is tensor/list\u001b[39;00m\n\u001b[32m    172\u001b[39m     ),\n\u001b[32m    173\u001b[39m )\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Measurement\u001b[39;00m\n\u001b[32m    176\u001b[39m list_elapsed_times = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/telemetry/metrics.py:218\u001b[39m, in \u001b[36mtrack_usage.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m smash_config = \u001b[38;5;28mrepr\u001b[39m(smash_config) \u001b[38;5;28;01mif\u001b[39;00m smash_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     increment_counter(function_name, success=\u001b[38;5;28;01mTrue\u001b[39;00m, smash_config=smash_config)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruna/src/pruna/engine/pruna_model.py:75\u001b[39m, in \u001b[36mPrunaModel.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/diffusers/pipelines/sana/pipeline_sana.py:942\u001b[39m, in \u001b[36mSanaPipeline.__call__\u001b[39m\u001b[34m(self, prompt, negative_prompt, num_inference_steps, timesteps, sigmas, guidance_scale, num_images_per_prompt, height, width, eta, generator, latents, prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask, output_type, return_dict, clean_caption, use_resolution_binning, attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length, complex_human_instruction)\u001b[39m\n\u001b[32m    939\u001b[39m timestep = timestep * \u001b[38;5;28mself\u001b[39m.transformer.config.timestep_scale\n\u001b[32m    941\u001b[39m \u001b[38;5;66;03m# predict noise model_output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    950\u001b[39m noise_pred = noise_pred.float()\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/diffusers/models/transformers/sana_transformer.py:532\u001b[39m, in \u001b[36mSanaTransformer2DModel.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, timestep, guidance, encoder_attention_mask, attention_mask, attention_kwargs, controlnet_block_samples, return_dict)\u001b[39m\n\u001b[32m    529\u001b[39m p = \u001b[38;5;28mself\u001b[39m.config.patch_size\n\u001b[32m    530\u001b[39m post_patch_height, post_patch_width = height // p, width // p\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m guidance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    535\u001b[39m     timestep, embedded_timestep = \u001b[38;5;28mself\u001b[39m.time_embed(\n\u001b[32m    536\u001b[39m         timestep, guidance=guidance, hidden_dtype=hidden_states.dtype\n\u001b[32m    537\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/diffusers/models/embeddings.py:547\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, latent)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    546\u001b[39m     height, width = latent.shape[-\u001b[32m2\u001b[39m] // \u001b[38;5;28mself\u001b[39m.patch_size, latent.shape[-\u001b[32m1\u001b[39m] // \u001b[38;5;28mself\u001b[39m.patch_size\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m latent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flatten:\n\u001b[32m    549\u001b[39m     latent = latent.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# BCHW -> BNC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (float) and bias type (c10::Half) should be the same"
     ]
    }
   ],
   "source": [
    "# Import required modules from Pruna\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    LatencyMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "from pruna import PrunaModel\n",
    "\n",
    "# Load the smashed (optimized) model pipeline from Hugging Face Hub\n",
    "smashed_pipe = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "# Define evaluation metrics (example: total time and latency)\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=1, n_warmup_iterations=1),\n",
    "    LatencyMetric(n_iterations=1, n_warmup_iterations=1),\n",
    "]\n",
    "\n",
    "# Define the evaluation task with metrics and datamodule\n",
    "# (Ensure `datamodule` and `device` are defined before this script runs)\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "\n",
    "# Initialize the evaluation agent\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Move smashed model to evaluation device (GPU or CPU)\n",
    "smashed_pipe.move_to_device(device)\n",
    "\n",
    "# Evaluate the smashed model pipeline using the evaluation agent\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)\n",
    "\n",
    "# Optionally, print results for verification\n",
    "print(smashed_model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Run garbage collector to free Python memory references\n",
    "gc.collect()\n",
    "\n",
    "# Empty cached GPU memory that is no longer needed\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally, if using PyTorch >=1.9, clear CUDA’s allocator memory as well\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradio Demo\n",
    "\n",
    "Once the model has been optimized, we can deploy the smashed model using **Gradio** to create an interactive demo. This allows anyone to test the model’s capabilities directly in their browser.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Show how to deploy the optimized model on the Hugging Face Hub with a Gradio demo\n",
    "* Discuss considerations such as **handling queuing**, especially if multiple users access the demo simultaneously\n",
    "* Highlight best practices for integrating Gradio demos in your Hugging Face Space to ensure a smooth and responsive user experience\n",
    "\n",
    "Creating a Gradio demo not only showcases your optimized model effectively but also enables easy sharing and real-world testing by the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468c1697d234010875d157eb8c72416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n",
      "/teamspace/studios/this_studio/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:248: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 231/231 [00:00<00:00, 59364.27it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 7521.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd4b952897485d8f8b493d23ab915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0ffb436674441c88cbea19d63f887c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Could not move model to device: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "/teamspace/studios/this_studio/.venv/lib/python3.10/site-packages/gradio/interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from pruna import PrunaModel\n",
    "\n",
    "# Load PrunaModel\n",
    "model = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def generate_image(prompt):\n",
    "    \"\"\"Generate an image from a given text prompt.\"\"\"\n",
    "    result = pipe(prompt, num_inference_steps=25, guidance_scale=7.5)\n",
    "    return result.images[0]\n",
    "\n",
    "\n",
    "# Create Gradio interface with queueing enabled\n",
    "demo = gr.Interface(\n",
    "    fn=generate_image,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Sana Smashed Text-to-Image Demo\",\n",
    "    description=\"Generate high-quality images using the smashed Sana diffusion model optimized with Pruna.\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "# Enable queueing to handle multiple users\n",
    "demo.queue()\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this tutorial, we have covered the end-to-end workflow for optimizing and evaluating a text-to-image diffusion model using Pruna.\n",
    "\n",
    "We began by loading the Sana base model and defining the SmashConfig with the desired optimization algorithms and parameters. We then smashed the base model, obtaining an optimized version, and ensured its performance improvements by running an evaluation with the EvaluationAgent.\n",
    "\n",
    "After optimization, we demonstrated how to deploy the smashed model using Gradio to create an interactive demo. This enables anyone to test the model’s capabilities directly in their browser. This end-to-end approach makes it easy to explore trade-offs, iterate on optimization configurations, and deploy robust, production-ready text-to-image models. \n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on optimizing and evaluating large language models, text-to-video models using Pruna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
