{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize and Deploy AI Models with Pruna and Hugging Face\n",
    "\n",
    "Objective: Build a complete tutorial demonstrating how to optimize the [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED) diffusion model using Pruna and deploy it seamlessly to the Hugging Face Hub.\n",
    "\n",
    "Model: [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED)\n",
    "\n",
    "Dataset: [data-is-better-together/open-image-preferences-v1-binarized](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized)\n",
    "\n",
    "To follow along, ensure that you have the Pruna SDK installed along with all required third-party libraries. Running this tutorial in a clean virtual environment is recommended for a smooth setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/deploying_sana_tutorial.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pruna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets huggingface_hub gradio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to login on the Hugging Face Hub for using the model weights. We also need to select the best available device for executing the notebook. Run the cells below to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smash Configuration\n",
    "\n",
    "To optimize the model effectively, we first define the configuration methods that enhance its performance. For detailed options and parameter explanations, refer to the [SmashConfig guide](https://docs.pruna.ai/en/stable/docs_pruna/user_manual/configure.html).\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "* Select **hqq_diffusers** to reduce memory usage during inference.\n",
    "* Set the **weight bit precision** to 8 bits for the diffusers model:\n",
    "* Upload the optimized (“smashed”) model to the Hugging Face Hub for easy access, sharing, and deployment in downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "from pruna import PrunaModel, SmashConfig, smash\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"Efficient-Large-Model/Sana_600M_512px_diffusers\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "pipe = SanaPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# 2. Configure Pruna smash\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"quantizer\"] = \"hqq_diffusers\"  # Quantizer to reduce memory usage\n",
    "smash_config[\"hqq_diffusers_weight_bits\"] = 8\n",
    "\n",
    "# 3. Smash (optimize) the model\n",
    "smashed_pipe = smash(model=pipe, smash_config=smash_config)\n",
    "\n",
    "# 4. Push the smashed pipeline to Hugging Face Hub using save_to_hub\n",
    "smashed_pipe.save_to_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "print(\"✅ Smashed Sana model uploaded successfully to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load and Collate Dataset**\n",
    "\n",
    "In this step, we will load the dataset required for optimizing and evaluating the model. This dataset will provide the input data needed to assess the model’s performance after applying optimization techniques such as quantization.\n",
    "\n",
    "We will use the [`data-is-better-together/open-image-preferences-v1-binarized`](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized) dataset, which contains binarized user image preferences and prompts for image generation tasks. Correctly loading and collating the dataset ensures that the input is properly prepared, enabling smooth evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"data-is-better-together/open-image-preferences-v1-binarized\")[\"train\"]\n",
    "\n",
    "dataset = dataset.rename_column(\"image_quality_dev\", \"image\")\n",
    "dataset = dataset.rename_column(\"quality_prompt\", \"text\")\n",
    "\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "\n",
    "# Split train into train/val/test\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(dataset, seed=42)\n",
    "\n",
    "# Initialize PrunaDataModule\n",
    "datamodule = PrunaDataModule.from_datasets(\n",
    "    datasets=(train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"image_generation_collate\",\n",
    "    collate_fn_args={\"img_size\": 512, \"input_format\": \"float\", \"output_format\": \"float\"},\n",
    ")\n",
    "\n",
    "# Limit datasets to 5 samples each for quick testing\n",
    "datamodule.limit_datasets(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Model\n",
    "\n",
    "Now that the model and dataset are set up, we can proceed to evaluate the model using the **Pruna Evaluation Agent**. This evaluation helps us measure the model’s current performance before optimization, providing a baseline for comparison. It assesses how well the model performs on the given dataset and generates relevant metrics that will guide us in understanding the impact of our optimization configurations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules from Pruna\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    LatencyMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "from pruna import PrunaModel\n",
    "\n",
    "# Load the smashed (optimized) model pipeline from Hugging Face Hub\n",
    "smashed_pipe = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "# Define evaluation metrics (example: total time and latency)\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=1, n_warmup_iterations=1),\n",
    "    LatencyMetric(n_iterations=1, n_warmup_iterations=1),\n",
    "]\n",
    "\n",
    "# Define the evaluation task with metrics and datamodule\n",
    "# (Ensure `datamodule` and `device` are defined before this script runs)\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "\n",
    "# Initialize the evaluation agent\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Move smashed model to evaluation device (GPU or CPU)\n",
    "smashed_pipe.move_to_device(device)\n",
    "\n",
    "# Evaluate the smashed model pipeline using the evaluation agent\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)\n",
    "\n",
    "# Optionally, print results for verification\n",
    "print(smashed_model_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradio Demo\n",
    "\n",
    "Once the model has been optimized, we can deploy the smashed model using **Gradio** to create an interactive demo. This allows anyone to test the model’s capabilities directly in their browser.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Show how to deploy the optimized model on the Hugging Face Hub with a Gradio demo\n",
    "* Discuss considerations such as **handling queuing**, especially if multiple users access the demo simultaneously\n",
    "* Highlight best practices for integrating Gradio demos in your Hugging Face Space to ensure a smooth and responsive user experience\n",
    "\n",
    "Creating a Gradio demo not only showcases your optimized model effectively but also enables easy sharing and real-world testing by the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from pruna import PrunaModel\n",
    "\n",
    "# Load PrunaModel\n",
    "model = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def generate_image(prompt):\n",
    "    \"\"\"Generate an image from a given text prompt.\"\"\"\n",
    "    result = pipe(prompt, num_inference_steps=25, guidance_scale=7.5)\n",
    "    return result.images[0]\n",
    "\n",
    "\n",
    "# Create Gradio interface with queueing enabled\n",
    "demo = gr.Interface(\n",
    "    fn=generate_image,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Sana Smashed Text-to-Image Demo\",\n",
    "    description=\"Generate high-quality images using the smashed Sana diffusion model optimized with Pruna.\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "# Enable queueing to handle multiple users\n",
    "demo.queue()\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this tutorial, we have covered the end-to-end workflow for optimizing and evaluating a text-to-image diffusion model using Pruna.\n",
    "\n",
    "We began by loading the Sana base model and defining the SmashConfig with the desired optimization algorithms and parameters. We then smashed the base model, obtaining an optimized version, and ensured its performance improvements by running an evaluation with the EvaluationAgent.\n",
    "\n",
    "After optimization, we demonstrated how to deploy the smashed model using Gradio to create an interactive demo. This enables anyone to test the model’s capabilities directly in their browser. This end-to-end approach makes it easy to explore trade-offs, iterate on optimization configurations, and deploy robust, production-ready text-to-image models. \n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on optimizing and evaluating large language models, text-to-video models using Pruna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
