{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing and Deploying AI Models with Pruna and Hugging Face\n",
    "\n",
    "Objective: Build a complete tutorial demonstrating how to optimize the [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED) diffusion model using Pruna and deploy it seamlessly to the Hugging Face Hub.\n",
    "\n",
    "Model: [Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px_ControlNet_HED)\n",
    "\n",
    "Dataset: [data-is-better-together/open-image-preferences-v1-binarized](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized)\n",
    "\n",
    "To follow along, ensure that you have the Pruna SDK installed along with all required third-party libraries. Running this tutorial in a clean virtual environment is recommended for a smooth setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pruna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets huggingface_hub gradio diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to login on the Hugging Face Hub for using the model weights. Run the cell below to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cddba2c14fe4f0fa3761154584ebc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smash Configuration\n",
    "\n",
    "To optimize the model, we first need to define the methods that will enhance its performance. For detailed options, refer to the [SmashConfig guide](https://docs.pruna.ai/en/stable/docs_pruna/user_manual/configure.html).\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "* Select a **quantizer** to reduce memory usage\n",
    "* Use a **cacher** to store intermediate computation results, accelerating future operations\n",
    "* Upload the optimized (smashed) model to the Hugging Face Hub for easy access and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pruna import smash, SmashConfig, PrunaModel\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"Efficient-Large-Model/Sana_600M_512px_diffusers\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "pipe = SanaPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# 2. Configure Pruna smash\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"quantizer\"] = \"hqq_diffusers\"  # Quantizer to reduce memory usage\n",
    "smash_config['hqq_diffusers_weight_bits'] = 8          # Cacher to speed up computations\n",
    "\n",
    "# 3. Smash (optimize) the model\n",
    "smashed_pipe = smash(model=pipe, smash_config=smash_config)\n",
    "\n",
    "# 4. Push the smashed pipeline to Hugging Face Hub using save_to_hub\n",
    "smashed_pipe.save_to_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "print(\"✅ Smashed Sana model uploaded successfully to Hugging Face Hub.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Collate Dataset\n",
    "\n",
    "In this step, we will load the dataset required for optimizing and evaluating the model. This dataset will serve as input data during the evaluation and help assess the model’s performance after applying quantization.\n",
    "\n",
    "We will use the [`data-is-better-together/open-image-preferences-v1-binarized`](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized) dataset, which contains binarized user image preferences. Loading the dataset correctly ensures that the input pipeline is ready for smooth optimization and deployment workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "from pruna.data.collate import image_generation_collate\n",
    "from pruna import PrunaModel\n",
    "\n",
    "\n",
    "# Load the smashed Sana model using PrunaModel\n",
    "pipe = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Load custom datasets\n",
    "train_ds = load_dataset(\"data-is-better-together/open-image-preferences-v1-binarized\")[\"train\"]\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(train_ds, seed=42)\n",
    "\n",
    "# Define collate function with desired img_size and output_format\n",
    "def custom_collate_fn(batch):\n",
    "    return image_generation_collate(batch, img_size=512, output_format=\"float\")\n",
    "\n",
    "# Create the data module with the custom collate function\n",
    "datamodule = PrunaDataModule.from_datasets(\n",
    "    datasets=(train_ds, val_ds, test_ds),\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Now that the model and dataset are set up, we can proceed to evaluate the model using the **Pruna Evaluation Agent**. This evaluation helps us measure the model’s current performance before optimization, providing a baseline for comparison. It assesses how well the model performs on the given dataset and generates relevant metrics that will guide us in understanding the impact of our optimization configurations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8513881d5bb7451bb0b2905a8de3fa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64538363df4c4e18af06d6a3ae151f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Pipeline <class 'diffusers.pipelines.flux.pipeline_flux.FluxPipeline'> expected ['scheduler', 'text_encoder', 'text_encoder_2', 'tokenizer', 'tokenizer_2', 'transformer', 'vae'], but only {'vae', 'tokenizer', 'text_encoder_2', 'text_encoder', 'scheduler', 'tokenizer_2'} were passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpruna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Load the smashed FLUX model from Hugging Face\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAINovice2005/smashed-FLUX.1-schnell-pruna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m pipe\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# or \"cpu\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 2: Save to local directory\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:1036\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1035\u001b[0m     passed_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(init_kwargs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(passed_class_obj\u001b[38;5;241m.\u001b[39mkeys())) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(optional_kwargs)\n\u001b[0;32m-> 1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# 10. Type checking init arguments\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kw, arg \u001b[38;5;129;01min\u001b[39;00m init_kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# Too complex to validate with type annotation alone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline <class 'diffusers.pipelines.flux.pipeline_flux.FluxPipeline'> expected ['scheduler', 'text_encoder', 'text_encoder_2', 'tokenizer', 'tokenizer_2', 'transformer', 'vae'], but only {'vae', 'tokenizer', 'text_encoder_2', 'text_encoder', 'scheduler', 'tokenizer_2'} were passed."
     ]
    }
   ],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    LatencyMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics\n",
    "metrics = ['clip_score', 'psnr']\n",
    "\n",
    "# Define the task and the evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_pipe.move_to_device(device)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Demo\n",
    "\n",
    "Once the model has been optimized, we can deploy the smashed model using **Gradio** to create an interactive demo. This allows anyone to test the model’s capabilities directly in their browser.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Show how to deploy the optimized model on the Hugging Face Hub with a Gradio demo\n",
    "* Discuss considerations such as **handling queuing**, especially if multiple users access the demo simultaneously\n",
    "* Highlight best practices for integrating Gradio demos in your Hugging Face Space to ensure a smooth and responsive user experience\n",
    "\n",
    "Creating a Gradio demo not only showcases your optimized model effectively but also enables easy sharing and real-world testing by the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468c1697d234010875d157eb8c72416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n",
      "/teamspace/studios/this_studio/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:248: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 231/231 [00:00<00:00, 59364.27it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 7521.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd4b952897485d8f8b493d23ab915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0ffb436674441c88cbea19d63f887c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Could not move model to device: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "/teamspace/studios/this_studio/.venv/lib/python3.10/site-packages/gradio/interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pruna import PrunaModel\n",
    "\n",
    "\n",
    "# ✅ Load PrunaModel\n",
    "model = PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "# ✅ Inference function\n",
    "def generate_image(prompt):\n",
    "    result = pipe(prompt, num_inference_steps=25, guidance_scale=7.5)\n",
    "    return result.images[0]\n",
    "\n",
    "# ✅ Create Gradio interface with queueing enabled\n",
    "demo = gr.Interface(\n",
    "    fn=generate_image,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Sana Smashed Text-to-Image Demo\",\n",
    "    description=\"Generate high-quality images using the smashed Sana diffusion model optimized with Pruna.\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# ✅ Enable queueing to handle multiple users\n",
    "demo.queue()\n",
    "\n",
    "# ✅ Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bb651911b5491b9dd91480cbf404ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'functools.partial' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ✅ Load PrunaModel\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m loaded_model= \u001b[43mPrunaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAINovice2005/Sana_600M_ControlNet_HED-smashed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m loaded_model = loaded_model.to(device)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ✅ Extract underlying pipeline using correct attribute\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/pruna/telemetry/metrics.py:218\u001b[39m, in \u001b[36mtrack_usage.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m smash_config = \u001b[38;5;28mrepr\u001b[39m(smash_config) \u001b[38;5;28;01mif\u001b[39;00m smash_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     increment_counter(function_name, success=\u001b[38;5;28;01mTrue\u001b[39;00m, smash_config=smash_config)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/pruna/engine/pruna_model.py:352\u001b[39m, in \u001b[36mPrunaModel.from_hub\u001b[39m\u001b[34m(repo_id, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@track_usage\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_hub\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    297\u001b[39m     **kwargs,\n\u001b[32m    298\u001b[39m ) -> PrunaModel:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m    Load a `PrunaModel` from the specified repository.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \u001b[33;03m        The loaded `PrunaModel` instance.\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     model, smash_config = \u001b[43mload_pruna_model_from_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Deprecated args\u001b[39;49;00m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, PrunaModel):\n\u001b[32m    377\u001b[39m         model = PrunaModel(model=model, smash_config=smash_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/pruna/engine/load.py:185\u001b[39m, in \u001b[36mload_pruna_model_from_hub\u001b[39m\u001b[34m(repo_id, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03mLoad a Pruna model from the Hugging Face Hub.\u001b[39;00m\n\u001b[32m    111\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m \u001b[33;03m    The loaded model and its SmashConfig.\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m path = snapshot_download(\n\u001b[32m    163\u001b[39m     repo_id=repo_id,\n\u001b[32m    164\u001b[39m     repo_type=\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m     resume_download=resume_download,\n\u001b[32m    184\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pruna_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/pruna/engine/load.py:69\u001b[39m, in \u001b[36mload_pruna_model\u001b[39m\u001b[34m(model_path, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLoad function has not been set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# load torch artifacts if they exist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mLOAD_FUNCTIONS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtorch_artifacts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m \u001b[38;5;129;01min\u001b[39;00m smash_config.load_fns:\n\u001b[32m     70\u001b[39m     load_torch_artifacts(model_path, **kwargs)\n\u001b[32m     71\u001b[39m     smash_config.load_fns.remove(LOAD_FUNCTIONS.torch_artifacts.name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'functools.partial' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from pruna import PrunaModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ✅ Load PrunaModel\n",
    "loaded_model= PrunaModel.from_hub(\"AINovice2005/Sana_600M_ControlNet_HED-smashed\")\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "# ✅ Extract underlying pipeline using correct attribute\n",
    "if hasattr(model, \"pipeline\"):\n",
    "    pipe = model.pipeline\n",
    "elif hasattr(model, \"to_pipeline\"):\n",
    "    pipe = model.to_pipeline()\n",
    "else:\n",
    "    raise AttributeError(\"Loaded PrunaModel has no pipeline attribute\")\n",
    "\n",
    "# ✅ Move pipeline to device if it's a diffusers pipeline\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# ✅ Inference function\n",
    "def generate_image(prompt):\n",
    "    result = pipe(prompt, num_inference_steps=25, guidance_scale=7.5)\n",
    "    return result.images[0]\n",
    "\n",
    "# ✅ Create Gradio interface with queueing enabled\n",
    "demo = gr.Interface(\n",
    "    fn=generate_image,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "    outputs=gr.Image(type=\"pil\"),\n",
    "    title=\"Sana Smashed Text-to-Image Demo\",\n",
    "    description=\"Generate high-quality images using the smashed Sana diffusion model optimized with Pruna.\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# ✅ Enable queueing to handle multiple users\n",
    "demo.queue()\n",
    "\n",
    "# ✅ Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
