{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate Video Generation Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/video_generation.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Goal** | Showcase a standard workflow for optimizing and evaluating a video generation model |\n",
    "| **Model** |[Wan-AI/Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) |\n",
    "| **Dataset** |  [nannullna/laion_subset](https://huggingface.co/datasets/nannullna/laion_subset) |\n",
    "| **Optimization Algorithms** | cacher(pab) |\n",
    "| **Evaluation Metrics** | `total time`, `latency` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To install the required dependencies, you can run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pruna\n",
    "%pip install ftfy imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will set the device to the best available option to maximize the optimization process's benefits. However, in this case, we recommend using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we must load the original model using the diffusers library to ensure it fits into memory. In this example, we will use a light model compatible with most of the consumer-grade GPUs, [Wan-AI/Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B).\n",
    "\n",
    "Pruna works at least as well with larger models, like the model version of Wan 2.1 14B or HuyuanVideo. The choice to use a smaller model is simply because itâ€™s a good starting point, so feel free to use any [text-to-image model available on Hugging Face](https://huggingface.co/models?pipeline_tag=text-to-video&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "\n",
    "vae = AutoencoderKLWan.from_pretrained(\n",
    "    model_id, subfolder=\"vae\", torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the pipeline, we can run some inference and check the output. The standard prompt structure for a video is **Subject + Subject Action + Scene**, which can become more complex as we add descriptions and details like the lighting, point of view, or visual style to achieve specific and refined results.\n",
    "\n",
    "Remember that you can improve the quality of the video by increasing the number of frames, the number of inference steps, and the guidance scale, but this will also increment the time and amount of resources required to generate the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A ripe prune slowly grows on a sunlit tree branch surrounded by green leaves.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        num_frames=33,\n",
    "        guidance_scale=3.0,\n",
    "        num_inference_steps=15,\n",
    "        generator=torch.Generator(device=device).manual_seed(42),\n",
    "    ).frames[0]\n",
    "\n",
    "export_to_video(output, \"base_video.mp4\", fps=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model has generated a nice short video based on our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the SmashConfig\n",
    "\n",
    "Now that we have correctly loaded and tested our base model, let's continue by defining the `SmashConfig` to customize the optimizations we want to apply when smashing.\n",
    "\n",
    "Take into account that not all optimization algorithms are available for all models, so you can learn about the requirements and compatibility in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html).\n",
    "\n",
    "In the current optimization, we will use [pab](https://docs.pruna.ai/en/stable/compression.html#pab) with an interval of `2`, which will speed up the model's inference time.\n",
    "\n",
    "Let's define the `SmashConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"cacher\"] = \"pab\"\n",
    "smash_config[\"pab_interval\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smash the Model\n",
    "\n",
    "Next, we need to apply our defined `SmashConfig` by smashing our model. The `smash` function will be in charge of this, so we just need to pass the `model` and the `smash_config`. To evaluate and compare the models in the upcoming sections, we will make a deep copy of the base model.\n",
    "\n",
    "Time to smash! This will take around 20 seconds, depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pruna import smash\n",
    "\n",
    "copy_pipe = copy.deepcopy(pipe).to(\"cpu\")\n",
    "smashed_pipe = smash(\n",
    "    model=pipe,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have an optimized smashed model, so let's check how it works using the previous prompt.\n",
    "\n",
    "Consider that if you are using `torch_compile` as a compiler, you can expect the first inference warmup to take a bit longer than the actual inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A ripe prune slowly grows on a sunlit tree branch surrounded by green leaves.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        num_frames=33,\n",
    "        guidance_scale=3.0,\n",
    "        num_inference_steps=15,\n",
    "    ).frames[0]\n",
    "\n",
    "export_to_video(output, \"smashed_video.mp4\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, it has also generated a short video similar to the original model.\n",
    "\n",
    "If you notice a significant difference, it might be due to the model, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case. However, feel free to reach out to us on [Discord]([https://discord.gg/Tun8YgzxZ9](https://discord.gg/Tun8YgzxZ9)) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Smashed Model\n",
    "\n",
    "Now that we have our smashed mode, the key question is how much has improved with our optimization. For this, we can run an evaluation of the performance using the EvaluationAgent. In this case, we will include metrics like the `total_time` and `latency`.\n",
    "\n",
    "A complete list of the available metrics can be found in [Evaluation](https://docs.pruna.ai/en/stable/reference/evaluation.html). At the moment, only efficiency metrics are available for video generation models and we are working on adding quality metrics in the future. So, stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    LatencyMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics. Increment the number of iterations and warmup iterations to get a more accurate result.\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    LatencyMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "]\n",
    "\n",
    "# Define the datamodule\n",
    "datamodule = PrunaDataModule.from_string(\"LAION256\")\n",
    "datamodule.limit_datasets(10)\n",
    "\n",
    "# Define the task and the evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Evaluate base model and offload it to CPU\n",
    "base_pipe = PrunaModel(model=copy_pipe)\n",
    "base_pipe.move_to_device(device)\n",
    "base_model_results = eval_agent.evaluate(base_pipe)\n",
    "base_pipe.move_to_device(\"cpu\")\n",
    "\n",
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_pipe.move_to_device(device)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)\n",
    "smashed_pipe.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize and compare the evaluation results of the base and smashed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display  # noqa\n",
    "\n",
    "\n",
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate differences and prepare table data\n",
    "table_data = []\n",
    "for base_metric_result in base_model_results:\n",
    "    for smashed_metric_result in smashed_model_results:\n",
    "        if base_metric_result.name == smashed_metric_result.name:\n",
    "            diff = calculate_percentage_diff(\n",
    "                base_metric_result.result, smashed_metric_result.result\n",
    "            )\n",
    "            table_data.append(\n",
    "                {\n",
    "                    \"Metric\": base_metric_result.name,\n",
    "                    \"Base Model\": f\"{base_metric_result.result:.4f}\",\n",
    "                    \"Compressed Model\": f\"{smashed_metric_result.result:.4f}\",\n",
    "                    \"Relative Difference\": f\"{diff:+.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            break\n",
    "\n",
    "# Create and display markdown table manually\n",
    "markdown_table = \"| Metric | Base Model | Compressed Model | Relative Difference |\\n\"\n",
    "markdown_table += \"|--------|----------|-----------|------------|\\n\"\n",
    "for row in table_data:\n",
    "    metric = [m for m in metrics if m.metric_name == row[\"Metric\"]][0]\n",
    "    unit = metric.metric_units if hasattr(metric, \"metric_units\") else \"\"\n",
    "    markdown_table += f\"| {row['Metric']} | {row['Base Model']} {unit} | {row['Compressed Model']} {unit} | {row['Relative Difference']} |\\n\"  # noqa: E501\n",
    "\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can observe a slight improvement in the speed of the model. So, we can save the optimized model to disk or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "smashed_pipe.save_pretrained(\"Wan2.1-T2V-1.3B-smashed\")\n",
    "# Load the model from disk\n",
    "# smashed_pipe = PrunaModel.from_pretrained(\"Wan2.1-T2V-1.3B-smashed/\")\n",
    "\n",
    "# Save the model to HuggingFace\n",
    "# smashed_pipe.save_to_hub(\"PrunaAI/Wan2.1-T2V-1.3B-smashed\")\n",
    "# smashed_pipe = PrunaModel.from_hub(\"PrunaAI/Wan2.1-T2V-1.3B-smashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we have gone over the standard workflow for optimizing and evaluating a text-to-video model.\n",
    "\n",
    "We started loading the base model and defining the SmashConfig with the desired optimization algorithms and parameters. Then we smashed the base model, obtaining an optimized version, and we ensured the improvement in performance by running an evaluation with the EvaluationAgent.\n",
    "\n",
    "The results show that we can significantly improve runtime performance and reduce memory usage and energy consumption, while maintaining a high level of output quality. This makes it easy to explore trade-offs and iterate on configurations to find the best optimization strategy for your specific use case.\n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on how to optimize and evaluate image generation models or LLM models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruna-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
