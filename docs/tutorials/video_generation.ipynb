{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate Video Generation Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/video_generation.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Goal** | Showcase a standard workflow for optimizing and evaluating a video generation model |\n",
    "| **Model** |[Wan-AI/Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) |\n",
    "| **Dataset** |  [nannullna/laion_subset](https://huggingface.co/datasets/nannullna/laion_subset) |\n",
    "| **Optimization Algorithms** | cacher(pab) |\n",
    "| **Evaluation Metrics** | `total time`, `latency`, `througput`, `co2_emissions`, and `energy_consumed` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To install the required dependencies, you can run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pruna\n",
      "  Downloading pruna-0.2.7-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting bitsandbytes (from pruna)\n",
      "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting codecarbon (from pruna)\n",
      "  Using cached codecarbon-3.0.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting colorama (from pruna)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting configspace>=1.2.1 (from pruna)\n",
      "  Using cached configspace-1.2.1-py3-none-any.whl\n",
      "Collecting ctranslate2==4.6.0 (from pruna)\n",
      "  Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets<=3.5.0 (from pruna)\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepcache (from pruna)\n",
      "  Using cached DeepCache-0.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting diffusers>=0.21.4 (from pruna)\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting gliner (from pruna)\n",
      "  Using cached gliner-0.2.21-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting hqq==0.2.6 (from pruna)\n",
      "  Downloading hqq-0.2.6.tar.gz (62 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub>=0.30.0 (from huggingface-hub[hf-xet]>=0.30.0->pruna)\n",
      "  Using cached huggingface_hub-0.34.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting librosa (from pruna)\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting llmcompressor (from pruna)\n",
      "  Using cached llmcompressor-0.6.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting opentelemetry-api>=1.30.0 (from pruna)\n",
      "  Using cached opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.29.0 (from pruna)\n",
      "  Using cached opentelemetry_exporter_otlp-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.30.0 (from pruna)\n",
      "  Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting openvino (from pruna)\n",
      "  Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting optimum (from pruna)\n",
      "  Using cached optimum-1.26.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting optimum-quanto>=0.2.5 (from pruna)\n",
      "  Using cached optimum_quanto-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pynvml (from pruna)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pytorch-lightning (from pruna)\n",
      "  Using cached pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting requests>=2.31.0 (from pruna)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sentencepiece (from pruna)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting soundfile (from pruna)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting thop (from pruna)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting timm (from pruna)\n",
      "  Using cached timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting torch-pruning (from pruna)\n",
      "  Using cached torch_pruning-1.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting torch==2.7.0 (from pruna)\n",
      "  Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchao (from pruna)\n",
      "  Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting torchmetrics[image] (from pruna)\n",
      "  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting torchvision==0.22.0 (from pruna)\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting transformers (from pruna)\n",
      "  Using cached transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting whisper-s2t==1.3.1 (from pruna)\n",
      "  Using cached whisper_s2t-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from ctranslate2==4.6.0->pruna) (78.1.1)\n",
      "Collecting pyyaml<7,>=5.3 (from ctranslate2==4.6.0->pruna)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting tqdm>=4.64.1 (from hqq==0.2.6->pruna)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting einops (from hqq==0.2.6->pruna)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate (from hqq==0.2.6->pruna)\n",
      "  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting termcolor (from hqq==0.2.6->pruna)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch==2.7.0->pruna)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch==2.7.0->pruna) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->pruna)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0->pruna)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.7.0->pruna)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->pruna)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->pruna)\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.22.0->pruna)\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting rich (from whisper-s2t==1.3.1->pruna)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from whisper-s2t==1.3.1->pruna) (4.3.8)\n",
      "Collecting tokenizers (from whisper-s2t==1.3.1->pruna)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting openai-whisper (from whisper-s2t==1.3.1->pruna)\n",
      "  Using cached openai_whisper-20250625-py3-none-any.whl\n",
      "Collecting nvidia-ml-py (from whisper-s2t==1.3.1->pruna)\n",
      "  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=3.5.0->pruna)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0->pruna)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=3.5.0->pruna)\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets<=3.5.0->pruna)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.5.0->pruna)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets<=3.5.0->pruna)\n",
      "  Using cached aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from datasets<=3.5.0->pruna) (25.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp->datasets<=3.5.0->pruna)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyparsing (from configspace>=1.2.1->pruna)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting scipy (from configspace>=1.2.1->pruna)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting more_itertools (from configspace>=1.2.1->pruna)\n",
      "  Using cached more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting importlib_metadata (from diffusers>=0.21.4->pruna)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers>=0.21.4->pruna)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers>=0.21.4->pruna)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.30.0->huggingface-hub[hf-xet]>=0.30.0->pruna)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers>=0.21.4->pruna)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.35.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.35.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf<7.0,>=5.0 (from opentelemetry-proto==1.35.0->opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp>=1.29.0->pruna)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.30.0->pruna)\n",
      "  Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.31.0->pruna)\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->pruna)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.31.0->pruna)\n",
      "  Using cached certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ninja (from optimum-quanto>=0.2.5->pruna)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0->pruna)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from accelerate->hqq==0.2.6->pruna) (5.9.1)\n",
      "Collecting arrow (from codecarbon->pruna)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting click (from codecarbon->pruna)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting fief-client[cli] (from codecarbon->pruna)\n",
      "  Using cached fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting prometheus_client (from codecarbon->pruna)\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting psutil (from accelerate->hqq==0.2.6->pruna)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting py-cpuinfo (from codecarbon->pruna)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from codecarbon->pruna)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting rapidfuzz (from codecarbon->pruna)\n",
      "  Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting questionary (from codecarbon->pruna)\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting typer (from codecarbon->pruna)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from arrow->codecarbon->pruna) (2.9.0.post0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon->pruna)\n",
      "  Using cached types_python_dateutil-2.9.0.20250708-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon->pruna) (1.17.0)\n",
      "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting cryptography>=3.4 (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached cryptography-45.0.5-cp37-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna) (1.3.0)\n",
      "Collecting onnxruntime (from gliner->pruna)\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.7.0->pruna)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->pruna)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->pruna)\n",
      "  Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->pruna)\n",
      "  Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa->pruna)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from librosa->pruna) (5.2.1)\n",
      "Collecting pooch>=1.1 (from librosa->pruna)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->pruna)\n",
      "  Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->pruna)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->pruna)\n",
      "  Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->pruna)\n",
      "  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->pruna)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting loguru (from llmcompressor->pruna)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from llmcompressor->pruna)\n",
      "  Using cached compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->codecarbon->pruna)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->codecarbon->pruna)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->codecarbon->pruna)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime->gliner->pruna)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime->gliner->pruna)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner->pruna)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting tiktoken (from openai-whisper->whisper-s2t==1.3.1->pruna)\n",
      "  Using cached tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino->pruna)\n",
      "  Using cached openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets<=3.5.0->pruna)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets<=3.5.0->pruna)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->pruna)\n",
      "  Using cached lightning_utilities-0.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from questionary->codecarbon->pruna) (3.0.51)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon->pruna) (0.2.13)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->whisper-s2t==1.3.1->pruna)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from rich->whisper-s2t==1.3.1->pruna) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->whisper-s2t==1.3.1->pruna)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]->pruna)\n",
      "  Using cached torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer->codecarbon->pruna)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting termcolor (from hqq==0.2.6->pruna)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading pruna-0.2.7-py3-none-any.whl (194 kB)\n",
      "Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "Using cached whisper_s2t-1.3.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Using cached huggingface_hub-0.34.2-py3-none-any.whl (558 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.35.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Using cached optimum_quanto-0.2.7-py3-none-any.whl (165 kB)\n",
      "Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "Using cached codecarbon-3.0.4-py3-none-any.whl (277 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250708-py3-none-any.whl (17 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached DeepCache-0.1.1-py3-none-any.whl (190 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached cryptography-45.0.5-cp37-abi3-manylinux_2_34_x86_64.whl (4.4 MB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached gliner-0.2.21-py3-none-any.whl (65 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached llmcompressor-0.6.0-py3-none-any.whl (253 kB)\n",
      "Using cached compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl (47.6 MB)\n",
      "Using cached openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached optimum-1.26.1-py3-none-any.whl (424 kB)\n",
      "Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
      "Using cached lightning_utilities-0.15.0-py3-none-any.whl (29 kB)\n",
      "Downloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Using cached tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "Using cached torch_pruning-1.6.0-py3-none-any.whl (68 kB)\n",
      "Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: hqq\n",
      "\u001b[33m  DEPRECATION: Building 'hqq' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'hqq'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for hqq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hqq: filename=hqq-0.2.6-py3-none-any.whl size=74127 sha256=019872697f47ccc98b67d9be965a77ad83d12a0f4c273fe26cba44668745a8a9\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/81/a3/55/df9aadbf556f038387fbb7b1a80b5210ddebe1424cc53cf3db\n",
      "Successfully built hqq\n",
      "Installing collected packages: torchao, sentencepiece, pytz, py-cpuinfo, openvino-telemetry, nvidia-ml-py, nvidia-cusparselt-cu12, mpmath, flatbuffers, zipp, xxhash, urllib3, tzdata, typing-inspection, types-python-dateutil, triton, tqdm, threadpoolctl, termcolor, sympy, sniffio, shellingham, safetensors, regex, rapidfuzz, pyyaml, pyparsing, pynvml, pydantic-core, pycparser, pyarrow, psutil, protobuf, propcache, prometheus_client, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, multidict, msgpack, more_itertools, mdurl, MarkupSafe, loguru, llvmlite, lightning-utilities, lazy_loader, joblib, idna, humanfriendly, hf-xet, h11, grpcio, fsspec, frozenlist, filelock, einops, dill, colorama, click, charset_normalizer, certifi, audioread, attrs, async-timeout, annotated-types, aiohappyeyeballs, yaspin, yarl, soxr, scipy, requests, questionary, pydantic, pandas, openvino, opentelemetry-proto, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, multiprocess, markdown-it-py, jinja2, importlib_metadata, httpcore, googleapis-common-protos, ctranslate2, coloredlogs, cffi, arrow, anyio, aiosignal, tiktoken, soundfile, scikit-learn, rich, pooch, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, nvidia-cusolver-cu12, huggingface-hub, httpx, cryptography, configspace, aiohttp, typer, torch, tokenizers, opentelemetry-semantic-conventions, librosa, jwcrypto, diffusers, transformers, torchvision, torchmetrics, torch-pruning, thop, optimum-quanto, opentelemetry-sdk, openai-whisper, fief-client, datasets, bitsandbytes, accelerate, torch-fidelity, timm, pytorch-lightning, optimum, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, hqq, gliner, deepcache, compressed-tensors, whisper-s2t, opentelemetry-exporter-otlp, llmcompressor, codecarbon, pruna\n",
      "\u001b[2K  Attempting uninstall: psutil[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 30/151\u001b[0m [pyarrow]r]ore]ion]cu12]\n",
      "\u001b[2K    Found existing installation: psutil 5.9.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 30/151\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling psutil-5.9.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 30/151\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled psutil-5.9.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 30/151\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151/151\u001b[0m [pruna]m [pruna]rbon]or]er-s2t]ensors]to-grpc]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.9.0 arrow-1.3.0 async-timeout-5.0.1 attrs-25.3.0 audioread-3.0.1 bitsandbytes-0.46.1 certifi-2025.7.14 cffi-1.17.1 charset_normalizer-3.4.2 click-8.2.1 codecarbon-3.0.4 colorama-0.4.6 coloredlogs-15.0.1 compressed-tensors-0.10.2 configspace-1.2.1 cryptography-45.0.5 ctranslate2-4.6.0 datasets-3.5.0 deepcache-0.1.1 diffusers-0.34.0 dill-0.3.8 einops-0.8.1 fief-client-0.20.0 filelock-3.18.0 flatbuffers-25.2.10 frozenlist-1.7.0 fsspec-2024.12.0 gliner-0.2.21 googleapis-common-protos-1.70.0 grpcio-1.74.0 h11-0.16.0 hf-xet-1.1.5 hqq-0.2.6 httpcore-1.0.9 httpx-0.27.2 huggingface-hub-0.34.2 humanfriendly-10.0 idna-3.10 importlib_metadata-8.7.0 jinja2-3.1.6 joblib-1.5.1 jwcrypto-1.5.6 lazy_loader-0.4 librosa-0.11.0 lightning-utilities-0.15.0 llmcompressor-0.6.0 llvmlite-0.44.0 loguru-0.7.3 markdown-it-py-3.0.0 mdurl-0.1.2 more_itertools-10.7.0 mpmath-1.3.0 msgpack-1.1.1 multidict-6.6.3 multiprocess-0.70.16 networkx-3.4.2 ninja-1.11.1.4 numba-0.61.2 numpy-1.26.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-ml-py-12.575.51 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 onnxruntime-1.22.1 openai-whisper-20250625 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-exporter-otlp-proto-http-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 openvino-2025.2.0 openvino-telemetry-2025.2.0 optimum-1.26.1 optimum-quanto-0.2.7 pandas-2.3.1 pillow-11.3.0 pooch-1.8.2 prometheus_client-0.22.1 propcache-0.3.2 protobuf-6.31.1 pruna-0.2.7 psutil-7.0.0 py-cpuinfo-9.0.0 pyarrow-21.0.0 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pynvml-12.0.0 pyparsing-3.2.3 pytorch-lightning-2.5.2 pytz-2025.2 pyyaml-6.0.2 questionary-2.1.0 rapidfuzz-3.13.0 regex-2024.11.6 requests-2.32.4 rich-14.1.0 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.15.3 sentencepiece-0.2.0 shellingham-1.5.4 sniffio-1.3.1 soundfile-0.13.1 soxr-0.5.0.post1 sympy-1.14.0 termcolor-2.3.0 thop-0.1.1.post2209072238 threadpoolctl-3.6.0 tiktoken-0.9.0 timm-1.0.19 tokenizers-0.21.2 torch-2.7.0 torch-fidelity-0.3.0 torch-pruning-1.6.0 torchao-0.12.0 torchmetrics-1.8.0 torchvision-0.22.0 tqdm-4.67.1 transformers-4.54.0 triton-3.3.0 typer-0.16.0 types-python-dateutil-2.9.0.20250708 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 whisper-s2t-1.3.1 xxhash-3.5.0 yarl-1.20.1 yaspin-3.1.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting imageio\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting imageio-ffmpeg\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from imageio) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from imageio) (11.3.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg, imageio, ftfy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ftfy][32m2/3\u001b[0m [ftfy]io]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ftfy-6.3.1 imageio-2.37.0 imageio-ffmpeg-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pruna\n",
    "%pip install ftfy imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==1.9.0\n",
      "aiohappyeyeballs==2.6.1\n",
      "aiohttp==3.12.14\n",
      "aiosignal==1.4.0\n",
      "annotated-types==0.7.0\n",
      "anyio==4.9.0\n",
      "arrow==1.3.0\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1733250440834/work\n",
      "async-timeout==5.0.1\n",
      "attrs==25.3.0\n",
      "audioread==3.0.1\n",
      "bitsandbytes==0.46.1\n",
      "certifi==2025.7.14\n",
      "cffi==1.17.1\n",
      "charset-normalizer==3.4.2\n",
      "click==8.2.1\n",
      "codecarbon==3.0.4\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_comm_1753453984/work\n",
      "compressed-tensors==0.10.2\n",
      "ConfigSpace==1.2.1\n",
      "cryptography==45.0.5\n",
      "ctranslate2==4.6.0\n",
      "datasets==3.5.0\n",
      "debugpy @ file:///croot/debugpy_1736267418885/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1740384970518/work\n",
      "DeepCache==0.1.1\n",
      "diffusers==0.34.0\n",
      "dill==0.3.8\n",
      "einops==0.8.1\n",
      "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1733327148154/work\n",
      "exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1746947292760/work\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1745502089858/work\n",
      "fief-client==0.20.0\n",
      "filelock==3.18.0\n",
      "flatbuffers==25.2.10\n",
      "frozenlist==1.7.0\n",
      "fsspec==2024.12.0\n",
      "ftfy==6.3.1\n",
      "gliner==0.2.21\n",
      "googleapis-common-protos==1.70.0\n",
      "grpcio==1.74.0\n",
      "h11==0.16.0\n",
      "hf-xet==1.1.5\n",
      "hqq==0.2.6\n",
      "httpcore==1.0.9\n",
      "httpx==0.27.2\n",
      "huggingface-hub==0.34.2\n",
      "humanfriendly==10.0\n",
      "idna==3.10\n",
      "imageio==2.37.0\n",
      "imageio-ffmpeg==0.6.0\n",
      "importlib_metadata==8.7.0\n",
      "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1719845459717/work\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_ipython_1748711175/work\n",
      "ipywidgets==8.1.7\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1733300866624/work\n",
      "Jinja2==3.1.6\n",
      "joblib==1.5.1\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1654730843242/work\n",
      "jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1748333051527/work\n",
      "jupyterlab_widgets==3.0.15\n",
      "jwcrypto==1.5.6\n",
      "lazy_loader==0.4\n",
      "librosa==0.11.0\n",
      "lightning-utilities==0.15.0\n",
      "llmcompressor==0.6.0\n",
      "llvmlite==0.44.0\n",
      "loguru==0.7.3\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1733416936468/work\n",
      "mdurl==0.1.2\n",
      "more-itertools==10.7.0\n",
      "mpmath==1.3.0\n",
      "msgpack==1.1.1\n",
      "multidict==6.6.3\n",
      "multiprocess==0.70.16\n",
      "nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1733325553580/work\n",
      "networkx==3.4.2\n",
      "ninja==1.11.1.4\n",
      "numba==0.61.2\n",
      "numpy==1.26.4\n",
      "nvidia-cublas-cu12==12.6.4.1\n",
      "nvidia-cuda-cupti-cu12==12.6.80\n",
      "nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "nvidia-cuda-runtime-cu12==12.6.77\n",
      "nvidia-cudnn-cu12==9.5.1.17\n",
      "nvidia-cufft-cu12==11.3.0.4\n",
      "nvidia-cufile-cu12==1.11.1.6\n",
      "nvidia-curand-cu12==10.3.7.77\n",
      "nvidia-cusolver-cu12==11.7.1.2\n",
      "nvidia-cusparse-cu12==12.5.4.2\n",
      "nvidia-cusparselt-cu12==0.6.3\n",
      "nvidia-ml-py==12.575.51\n",
      "nvidia-nccl-cu12==2.26.2\n",
      "nvidia-nvjitlink-cu12==12.6.85\n",
      "nvidia-nvtx-cu12==12.6.77\n",
      "onnxruntime==1.22.1\n",
      "openai-whisper==20250625\n",
      "opentelemetry-api==1.35.0\n",
      "opentelemetry-exporter-otlp==1.35.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.35.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.35.0\n",
      "opentelemetry-exporter-otlp-proto-http==1.35.0\n",
      "opentelemetry-proto==1.35.0\n",
      "opentelemetry-sdk==1.35.0\n",
      "opentelemetry-semantic-conventions==0.56b0\n",
      "openvino==2025.2.0\n",
      "openvino-telemetry==2025.2.0\n",
      "optimum==1.26.1\n",
      "optimum-quanto==0.2.7\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_packaging_1745345660/work\n",
      "pandas==2.3.1\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1733271261340/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1733301927746/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1733327343728/work\n",
      "pillow==11.3.0\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_platformdirs_1746710438/work\n",
      "pooch==1.8.2\n",
      "prometheus_client==0.22.1\n",
      "prompt_toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1744724089886/work\n",
      "propcache==0.3.2\n",
      "protobuf==6.31.1\n",
      "pruna==0.2.7\n",
      "psutil==7.0.0\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1733302279685/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl#sha256=92c32ff62b5fd8cf325bec5ab90d7be3d2a8ca8c8a3813ff487a8d2002630d1f\n",
      "pure_eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1733569405015/work\n",
      "py-cpuinfo==9.0.0\n",
      "pyarrow==21.0.0\n",
      "pycparser==2.22\n",
      "pydantic==2.11.7\n",
      "pydantic_core==2.33.2\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1750615794071/work\n",
      "pynvml==12.0.0\n",
      "pyparsing==3.2.3\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_python-dateutil_1751104122/work\n",
      "pytorch-lightning==2.5.2\n",
      "pytz==2025.2\n",
      "PyYAML==6.0.2\n",
      "pyzmq @ file:///croot/pyzmq_1734687138743/work\n",
      "questionary==2.1.0\n",
      "RapidFuzz==3.13.0\n",
      "regex==2024.11.6\n",
      "requests==2.32.4\n",
      "rich==14.1.0\n",
      "safetensors==0.5.3\n",
      "scikit-learn==1.7.1\n",
      "scipy==1.15.3\n",
      "sentencepiece==0.2.0\n",
      "shellingham==1.5.4\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_six_1753199211/work\n",
      "sniffio==1.3.1\n",
      "soundfile==0.13.1\n",
      "soxr==0.5.0.post1\n",
      "stack_data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1733569443808/work\n",
      "sympy==1.14.0\n",
      "termcolor==2.3.0\n",
      "thop==0.1.1.post2209072238\n",
      "threadpoolctl==3.6.0\n",
      "tiktoken==0.9.0\n",
      "timm==1.0.19\n",
      "tokenizers==0.21.2\n",
      "torch==2.7.0\n",
      "torch-fidelity==0.3.0\n",
      "torch-pruning==1.6.0\n",
      "torchao==0.12.0\n",
      "torchmetrics==1.8.0\n",
      "torchvision==0.22.0\n",
      "tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1648827254365/work\n",
      "tqdm==4.67.1\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1733367359838/work\n",
      "transformers==4.54.0\n",
      "triton==3.3.0\n",
      "typer==0.16.0\n",
      "types-python-dateutil==2.9.0.20250708\n",
      "typing-inspection==0.4.1\n",
      "typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_typing_extensions_1751643513/work\n",
      "tzdata==2025.2\n",
      "urllib3==2.5.0\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1733231326287/work\n",
      "whisper-s2t==1.3.1\n",
      "widgetsnbextension==4.0.14\n",
      "xxhash==3.5.0\n",
      "yarl==1.20.1\n",
      "yaspin==3.1.0\n",
      "zipp==3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how to install Pruna, please refer to the [Installation](https://docs.pruna.ai/en/stable/setup/install.html) page.\n",
    "\n",
    "Then, we will set the device to the best available option to maximize the optimization process's benefits. However, in this case, we recommend using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we must load the original model using the diffusers library to ensure it fits into memory. In this example, we will use a light model compatible with most of the consumer-grade GPUs, [Wan-AI/Wan2.1-T2V-1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B).\n",
    "\n",
    "Pruna works at least as well with larger models, like the model version of Wan 2.1 14B or HuyuanVideo. The choice to use a smaller model is simply because it’s a good starting point, so feel free to use any [text-to-video model available on Hugging Face](https://huggingface.co/models?pipeline_tag=text-to-video&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum-quanto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935e3a487362448ba19d3f7efe3c2e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6835c0fe50d4722ac954cd25ef5a890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d065e286994249abfb270bb3c3c353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n",
    "\n",
    "pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the pipeline, we can run some inference and check the output. The standard prompt structure for a video is **Subject + Subject Action + Scene**, which can become more complex as we add descriptions and details like the lighting, point of view, or visual style to achieve specific and refined results.\n",
    "\n",
    "Remember that you can improve the quality of the video by increasing the number of frames, the number of inference steps, and the guidance scale, but this will also increment the time and amount of resources required to generate the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38235d1ee39147619decd031144cc7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'base_video.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A dog runs on the beach, realistic.\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"  # noqa: E501\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        num_frames=33,\n",
    "        guidance_scale=3.0,\n",
    "        num_inference_steps=15,\n",
    "        generator=torch.Generator(device=device).manual_seed(42),\n",
    "    ).frames[0]\n",
    "\n",
    "export_to_video(output, \"base_video.mp4\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model has generated a nice short video based on our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the SmashConfig\n",
    "\n",
    "Now that we have correctly loaded and tested our base model, let's continue by defining the `SmashConfig` to customize the optimizations we want to apply when smashing.\n",
    "\n",
    "Take into account that not all optimization algorithms are available for all models, so you can learn about the requirements and compatibility in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html).\n",
    "\n",
    "In the current optimization, we will use [pab](https://docs.pruna.ai/en/stable/compression.html#pab) with an interval of `2`, which will speed up the model's inference time. \n",
    "\n",
    "Let's define the `SmashConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"torch_compile_target\"] = \"module_list\"\n",
    "smash_config[\"quantizer\"] = \"torchao\"\n",
    "smash_config[\"torchao_quant_type\"] = \"fp8dq\"\n",
    "smash_config[\"torchao_excluded_modules\"] = \"norm+embedding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smash the Model\n",
    "\n",
    "Next, we need to apply our defined `SmashConfig` by smashing our model. The `smash` function will be in charge of this, so we just need to pass the `model` and the `smash_config`. To evaluate and compare the models in the upcoming sections, we will make a deep copy of the base model.\n",
    "\n",
    "Time to smash! This will take around 20 seconds, depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting quantizer torchao...\n",
      "WARNING - You are using torchao with torch.compile. Please set `smash_config['torch_compile_mode']='max-autotune-no-cudagraphs'` for best results; otherwise you may encounter undesirable outcomes.\n",
      "INFO - quantizer torchao was applied successfully.\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pruna import smash\n",
    "\n",
    "copy_pipe = copy.deepcopy(pipe).to(\"cpu\")\n",
    "smashed_pipe = smash(\n",
    "    model=pipe,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have an optimized smashed model, so let's check how it works using the previous prompt.\n",
    "\n",
    "Consider that if you are using `torch_compile` as a compiler, you can expect the first inference warmup to take a bit longer than the actual inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b42902843f43faafa46812b5830fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'smashed_video.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = smashed_pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        num_frames=33,\n",
    "        guidance_scale=3.0,\n",
    "        num_inference_steps=15,\n",
    "        generator=torch.Generator(device=device).manual_seed(42),\n",
    "    ).frames[0]\n",
    "\n",
    "export_to_video(output, \"smashed_video.mp4\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, it has also generated a short video similar to the original model.\n",
    "\n",
    "If you notice a significant difference, it might be due to the model, the configuration, the hardware, etc. We encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case. However, feel free to reach out to us on [Discord]([https://discord.gg/Tun8YgzxZ9](https://discord.gg/Tun8YgzxZ9)) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Smashed Model\n",
    "\n",
    "Now that we have our smashed model, the key question is how much has improved with our optimization. For this, we can run an evaluation of the performance using the `EvaluationAgent`. In this case, we will include metrics like the `total_time`, `latency`, `throughput`, `co2_emissions`, and `energy_consumed`.\n",
    "\n",
    "A complete list of the available metrics can be found in [Evaluation](https://docs.pruna.ai/en/stable/reference/evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages/thop/profile.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.0.0\"):\n",
      "/home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages/thop/profile.py:68: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.1.0\"):\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with image_generation_collate...\n",
      "INFO - Using provided list of metric instances.\n"
     ]
    }
   ],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    CO2EmissionsMetric,\n",
    "    EnergyConsumedMetric,\n",
    "    LatencyMetric,\n",
    "    ThroughputMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics. Increment the number of iterations and warmup iterations to get a more accurate result.\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    LatencyMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    ThroughputMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    CO2EmissionsMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    EnergyConsumedMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "]\n",
    "\n",
    "# Define the datamodule\n",
    "datamodule = PrunaDataModule.from_string(\"LAION256\")\n",
    "datamodule.limit_datasets(10)\n",
    "\n",
    "# Define the task and the evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Evaluating a base model.\n",
      "INFO - Detected diffusers model. Using DiffuserHandler with fixed seed.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .images attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dd996da6fe434aab1a85a384eaaa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4ba92373da47bd818e70d581461a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8c9df0a6194efeb4dd0ff48ca29418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5594d4ab5a7e443fa06851ecef80d3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:26:07] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:26:07] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:26:07] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:26:09] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:26:09] CPU Model on constant consumption mode: AMD EPYC 7R13 Processor\n",
      "[codecarbon WARNING @ 15:26:09] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 15:26:09] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:26:09] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:26:09] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 15:26:09] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:26:09]   Platform system: Linux-6.8.0-1031-aws-x86_64-with-glibc2.39\n",
      "[codecarbon INFO @ 15:26:09]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 15:26:09]   CodeCarbon version: 3.0.4\n",
      "[codecarbon INFO @ 15:26:09]   Available RAM : 61.940 GB\n",
      "[codecarbon INFO @ 15:26:09]   CPU count: 8 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 15:26:09]   CPU model: AMD EPYC 7R13 Processor\n",
      "[codecarbon INFO @ 15:26:09]   GPU count: 1\n",
      "[codecarbon INFO @ 15:26:09]   GPU model: 1 x NVIDIA L40S\n",
      "[codecarbon INFO @ 15:26:09] Emissions data (if any) will be saved to file /home/ubuntu/sdiazlor/prunatree/pruna/docs/tutorials/emissions.csv\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32b50a0f8b544ae97eaef1bf324b55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a2478aef66403cb22a997e54b50ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:26:13] Background scheduler didn't run for a long period (4s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:26:13] Energy consumed for RAM : 0.000023 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:26:13] Delta energy consumed for CPU with constant : 0.000130 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 15:26:13] Energy consumed for All CPU : 0.000130 kWh\n",
      "[codecarbon INFO @ 15:26:13] Energy consumed for all GPUs : 0.000105 kWh. Total GPU Power : 91.04196829550723 W\n",
      "[codecarbon INFO @ 15:26:13] 0.000258 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33eb6694c4f4fdaa1c3915790a91d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdc6198317442508630559f51af2bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c353b5c32cb4705a8a05a3223f82381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457332738c084fb29bbb0aa48c11985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:40:51] Background scheduler didn't run for a long period (657s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for RAM : 0.003678 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:40:51] Delta energy consumed for CPU with constant : 0.020556 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for All CPU : 0.020686 kWh\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for all GPUs : 0.062260 kWh. Total GPU Power : 340.1569740178087 W\n",
      "[codecarbon INFO @ 15:40:51] 0.086624 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for RAM : 0.003678 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:40:51] Delta energy consumed for CPU with constant : 0.000000 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for All CPU : 0.020686 kWh\n",
      "[codecarbon INFO @ 15:40:51] Energy consumed for all GPUs : 0.062260 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 15:40:51] 0.086624 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base model and offload it to CPU\n",
    "base_pipe = PrunaModel(model=copy_pipe)\n",
    "base_pipe.move_to_device(device)\n",
    "base_model_results = eval_agent.evaluate(base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipe.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time: 655497.640625\n",
      "latency: 218499.21354166666\n",
      "throughput: 4.576675511966112e-06\n",
      "co2_emissions: 0.031975750293829194\n",
      "energy_consumed: 0.08662360432680613\n"
     ]
    }
   ],
   "source": [
    "for result in base_model_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected diffusers model. Using DiffuserHandler with fixed seed.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .images attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c7439436de4001bc43166b30ce93cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92442c781dd24382a452ebe4acbe1799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5673bfb70b8d4bc19b187309baf33067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832168e3e849452c9a0fd673f9983e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:55:12] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:55:12] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:55:12] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:55:13] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:55:13] CPU Model on constant consumption mode: AMD EPYC 7R13 Processor\n",
      "[codecarbon WARNING @ 14:55:13] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:55:13] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:55:13] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 14:55:13] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 14:55:13] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:55:13]   Platform system: Linux-6.8.0-1031-aws-x86_64-with-glibc2.39\n",
      "[codecarbon INFO @ 14:55:13]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 14:55:13]   CodeCarbon version: 3.0.4\n",
      "[codecarbon INFO @ 14:55:13]   Available RAM : 61.940 GB\n",
      "[codecarbon INFO @ 14:55:13]   CPU count: 8 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 14:55:13]   CPU model: AMD EPYC 7R13 Processor\n",
      "[codecarbon INFO @ 14:55:13]   GPU count: 1\n",
      "[codecarbon INFO @ 14:55:13]   GPU model: 1 x NVIDIA L40S\n",
      "[codecarbon INFO @ 14:55:13] Emissions data (if any) will be saved to file /home/ubuntu/sdiazlor/prunatree/pruna/docs/tutorials/emissions.csv\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2233acf87a4a6eb7dc2a9197a291ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f149aa394d04cca818d6b9a9d0890b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting quantizer torchao...\n",
      "WARNING - You are using torchao with torch.compile. Please set `smash_config['torch_compile_mode']='max-autotune-no-cudagraphs'` for best results; otherwise you may encounter undesirable outcomes.\n",
      "INFO - quantizer torchao was applied successfully.\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n",
      "[codecarbon WARNING @ 14:55:21] Background scheduler didn't run for a long period (7s), results might be inaccurate\n",
      "[codecarbon INFO @ 14:55:21] Energy consumed for RAM : 0.000044 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:55:21] Delta energy consumed for CPU with constant : 0.000246 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 14:55:21] Energy consumed for All CPU : 0.000246 kWh\n",
      "[codecarbon INFO @ 14:55:21] Energy consumed for all GPUs : 0.000207 kWh. Total GPU Power : 94.4638114281574 W\n",
      "[codecarbon INFO @ 14:55:21] 0.000497 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9dca7b6ce045b6851f9e3f286c979a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64974544596f420791355661a08a4de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3b15016af147afb3c6e8bd346c1ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7f082594ab45ea8fadd52027856800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:10:12] Background scheduler didn't run for a long period (667s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for RAM : 0.003754 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:10:12] Delta energy consumed for CPU with constant : 0.020869 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for All CPU : 0.021115 kWh\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for all GPUs : 0.060089 kWh. Total GPU Power : 322.813921365026 W\n",
      "[codecarbon INFO @ 15:10:12] 0.084958 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for RAM : 0.003754 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:10:12] Delta energy consumed for CPU with constant : 0.000000 kWh, power : 112.5 W\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for All CPU : 0.021115 kWh\n",
      "[codecarbon INFO @ 15:10:12] Energy consumed for all GPUs : 0.060089 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 15:10:12] 0.084958 kWh of electricity used since the beginning.\n",
      "/home/ubuntu/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n",
      "WARNING - Could not move model to device: _apply(): Couldn't swap Linear.weight\n"
     ]
    }
   ],
   "source": [
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_pipe.move_to_device(device)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashed_pipe.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_time: 668918.96875\n",
      "latency: 222972.98958333334\n",
      "throughput: 4.484848150749949e-06\n",
      "co2_emissions: 0.031360857993651445\n",
      "energy_consumed: 0.08495783614858526\n"
     ]
    }
   ],
   "source": [
    "for result in smashed_model_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize and compare the evaluation results of the base and smashed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Metric | Base Model | Compressed Model | Relative Difference |\n",
       "|--------|----------|-----------|------------|\n",
       "| total_time | 655497.6406250 ms | 668918.9687500 ms | +2.05% |\n",
       "| latency | 218499.2135417 ms/num_iterations | 222972.9895833 ms/num_iterations | +2.05% |\n",
       "| throughput | 0.0000046 num_iterations/ms | 0.0000045 num_iterations/ms | -2.01% |\n",
       "| co2_emissions | 0.0319758 kgCO2e | 0.0313609 kgCO2e | -1.92% |\n",
       "| energy_consumed | 0.0866236 kWh | 0.0849578 kWh | -1.92% |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display  # noqa\n",
    "\n",
    "\n",
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate differences and prepare table data\n",
    "table_data = []\n",
    "for base_metric_result in base_model_results:\n",
    "    for smashed_metric_result in smashed_model_results:\n",
    "        if base_metric_result.name == smashed_metric_result.name:\n",
    "            diff = calculate_percentage_diff(base_metric_result.result, smashed_metric_result.result)\n",
    "            table_data.append(\n",
    "                {\n",
    "                    \"Metric\": base_metric_result.name,\n",
    "                    \"Base Model\": f\"{base_metric_result.result:.7f}\",\n",
    "                    \"Compressed Model\": f\"{smashed_metric_result.result:.7f}\",\n",
    "                    \"Relative Difference\": f\"{diff:+.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            break\n",
    "\n",
    "# Create and display markdown table manually\n",
    "markdown_table = \"| Metric | Base Model | Compressed Model | Relative Difference |\\n\"\n",
    "markdown_table += \"|--------|----------|-----------|------------|\\n\"\n",
    "for row in table_data:\n",
    "    metric = [m for m in metrics if m.metric_name == row[\"Metric\"]][0]\n",
    "    unit = metric.metric_units if hasattr(metric, \"metric_units\") else \"\"\n",
    "    markdown_table += f\"| {row['Metric']} | {row['Base Model']} {unit} | {row['Compressed Model']} {unit} | {row['Relative Difference']} |\\n\"  # noqa: E501\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can observe a slight improvement in the speed of the model. So, we can save the optimized model to disk or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "smashed_pipe.save_pretrained(\"Wan2.1-T2V-1.3B-smashed\")\n",
    "# Load the model from disk\n",
    "# smashed_pipe = PrunaModel.from_pretrained(\"Wan2.1-T2V-1.3B-smashed/\")\n",
    "\n",
    "# Save the model to HuggingFace\n",
    "# smashed_pipe.save_to_hub(\"PrunaAI/Wan2.1-T2V-1.3B-smashed\")\n",
    "# smashed_pipe = PrunaModel.from_hub(\"PrunaAI/Wan2.1-T2V-1.3B-smashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we have gone over the standard workflow for optimizing and evaluating a text-to-video model.\n",
    "\n",
    "We started loading the base model and defining the SmashConfig with the desired optimization algorithms and parameters. Then we smashed the base model, obtaining an optimized version, and we ensured the improvement in performance by running an evaluation with the EvaluationAgent.\n",
    "\n",
    "The results show that we can significantly improve runtime performance and reduce memory usage and energy consumption, while maintaining a high level of output quality. This makes it easy to explore trade-offs and iterate on configurations to find the best optimization strategy for your specific use case.\n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on how to optimize and evaluate image generation models or LLM models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruna-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
