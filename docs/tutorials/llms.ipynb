{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate Large Language Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/llms.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Goal** | Show a standard workflow for optimizing and evaluating a large language model |\n",
    "| **Model** | [HuggingFaceTB/SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) |\n",
    "| **Dataset** | [SmolSmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk) |\n",
    "| **Libraries** | [transformers](https://github.com/huggingface/transformers), [datasets](https://github.com/huggingface/datasets) |\n",
    "| **Device** | 1 x RTX A5000 (24GB VRAM) |\n",
    "| **Optimization Algorithms** | quantizer(hqq), compiler(torch_compile) |\n",
    "| **Evaluation Metrics** | perplexity, throughput, total time, energy consumption |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Install the dependencies\n",
    "\n",
    "To install the dependencies, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pruna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the device\n",
    "\n",
    "Normally, we would set the device to the best available device to make the most out of the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Before we can optimize the model, we need to ensure that we can load the model and tokenizer correctly and that they can fit in memory. For this example, we will use a nice and small LLM, [HuggingFaceTB/SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct), but feel free to use any [text-generation model on Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation). \n",
    "\n",
    "Although Pruna works at least as good with much larger models, like Qwen or LLaMA, a small model is a good starting point to show and test the steps of the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've loaded the model and tokenizer. Let's see if we can run some inference with them. To make this easy for use, we will be using the `transformers` library's `pipeline.__call__` function and passing in a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I am a text-based AI assistant developed by Hugging Face, trained on a massive corpus of text data. My purpose is to assist with various tasks, including grammar, syntax, style, and language processing. I can provide explanations for grammatical structures, offer alternative phrasings for better clarity, and help with writing improvements.'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a response to the user's question, which is being cut-off after the allowed `max_new_tokens`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the SmashConfig\n",
    "\n",
    "Now we know the model is working, let's continue with the optimization process and define the `SmashConfig`, which we will use later on to optimize the model.\n",
    "\n",
    "Not all optimization algorithms are available for all models but we can learn a bit more about different optimization algorithms and their requirements in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html) section of the documentation.\n",
    "\n",
    "For the current optimization, we will be using the [`hqq` quantizer](https://docs.pruna.ai/en/stable/compression.html#hqq) and the [`torch_compile` compiler](https://docs.pruna.ai/en/stable/compression.html#torch-compile). We will updating some parameters for these algorithms, setting `hqq_weight_bits` to `4`, `hqq_compute_dtype` to `torch.bfloat16`, `torch_compile_fullgraph` to `True`, `torch_compile_dynamic` to `True`, and `torch_compile_mode` to `max-autotune`. This is one of the many configurations and will just serve as an example.\n",
    "\n",
    "Let's define the `SmashConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig(device=device)\n",
    "smash_config[\"quantizer\"] = \"hqq\"\n",
    "smash_config[\"hqq_weight_bits\"] = 8\n",
    "smash_config[\"hqq_compute_dtype\"] = \"torch.bfloat16\"\n",
    "# We can also use `torch_compile` as our compiler, but we will skip it for now as it will take a bit longer to compile.\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"torch_compile_fullgraph\"] = True\n",
    "smash_config[\"torch_compile_dynamic\"] = True\n",
    "smash_config[\"torch_compile_mode\"] = \"max-autotune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smash the model\n",
    "\n",
    "Now that we have defined the `SmashConfig` object, we can smash the model. We will be using the `smash` function to smash the model and pass the `model` and `smash_config` to it. We also make a deep copy of the model to avoid modifying the original model. \n",
    "\n",
    "Let's smash the model, which should take around 20 seconds for this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting quantizer hqq...\n",
      "100%|██████████| 99/99 [00:00<00:00, 1957.55it/s]\n",
      "100%|██████████| 225/225 [00:11<00:00, 19.45it/s]\n",
      "INFO - quantizer hqq was applied successfully.\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pruna import smash\n",
    "\n",
    "copy_model = copy.deepcopy(pipe.model).to(\"cpu\")\n",
    "smashed_model = smash(\n",
    "    model=pipe.model,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've optimized the model. Let's see if everything still works as expected and we can run some inference with the optimized model. In this case, we are running the inference by first encoding the prompt through the `tokenizer` and then passing the `input_ids` to the `PrunaModel.generate` method, which also allows us to specify additional parameters such as `max_new_tokens`.\n",
    "\n",
    "If you are using `torch_compile` as your compiler, you can expect the first inference warmup to take a bit longer than the actual inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"I am a helpful AI assistant named SmolLM, designed to aid creative thinking and facilitate dialogue. Your questions will be responded with innovative ideas and perspectives in a way that's easy to follow and understand. I'll try to provide useful information and insights, and I encourage you to share your own thoughts and ideas to enrich the conversation. So please, go ahead and ask away!\"}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who are you?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "pipe(messages, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a similar response to the original model. \n",
    "\n",
    "If you notice a significant difference, it might have several reasons, the models, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case but also feel free to reach out to us on [Discord](https://discord.gg/Tun8YgzxZ9) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the smashed model\n",
    "\n",
    "Now that we have optimized the model, we can evaluate the performance of the optimized model. We will be using the `EvaluationAgent` to evaluate the performance of the optimized model. We will do so with some basic metrics, the `elapsed_time`, as well as a stateful metrics, the `perplexity`. An overview of the different metrics can be found in our [documentation](https://docs.pruna.ai/).\n",
    "\n",
    "Let's define the `EvaluationAgent` object and start the evaluation process. Note that we are using the `datamodule.limit_datasets(10)` method to limit the number of datasets to 10, which is just for the sake of time. Additionally, we lower the `n_iterations` and `n_warmup_iterations` to ensure that we monitor the performance of the model whenever it is running smoothly.\n",
    "\n",
    "The evaluation can take anywhere from a couple of minutes to a couple of hours to complete, depending on your hardware, the number of samples in the dataset, and the configuration of the model. In our case it should only take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/thop/profile.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) < LooseVersion(\"1.0.0\"):\n",
      "/usr/local/lib/python3.10/dist-packages/thop/profile.py:68: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.1.0\"):\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Loaded only training and test, splitting train 90/10 into train and validation...\n",
      "INFO - Using max_seq_len of tokenizer: 8192\n",
      "INFO - Testing compatibility with text_generation_collate...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - No device specified. Using best available device: 'cuda'\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "[codecarbon WARNING @ 12:01:21] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 12:01:21] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:01:21] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:01:22] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 12:01:22] CPU Model on constant consumption mode: AMD EPYC 7H12 64-Core Processor\n",
      "[codecarbon WARNING @ 12:01:22] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 12:01:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:01:22] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:01:22] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 12:01:22] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:01:22]   Platform system: Linux-6.8.0-57-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:01:22]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 12:01:22]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 12:01:22]   Available RAM : 503.526 GB\n",
      "[codecarbon INFO @ 12:01:22]   CPU count: 128 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 12:01:22]   CPU model: AMD EPYC 7H12 64-Core Processor\n",
      "[codecarbon INFO @ 12:01:22]   GPU count: 1\n",
      "[codecarbon INFO @ 12:01:22]   GPU model: 1 x NVIDIA RTX A5000\n",
      "[codecarbon INFO @ 12:01:25] Emissions data (if any) will be saved to file /emissions.csv\n",
      "INFO - No device specified. Using best available device: 'cuda'\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\n",
      "ERROR - Error loading model using HQQ: 'LlamaAttention' object has no attribute 'rotary_emb'\n",
      "100%|██████████| 99/99 [00:00<00:00, 34597.24it/s]\n",
      "100%|██████████| 225/225 [00:00<00:00, 5479.19it/s]\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n",
      "[codecarbon WARNING @ 12:01:30] Background scheduler didn't run for a long period (4s), results might be inaccurate\n",
      "[codecarbon INFO @ 12:01:30] Energy consumed for RAM : 0.000081 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:01:30] Delta energy consumed for CPU with cpu_load : 0.000033 kWh, power : 28.0 W\n",
      "[codecarbon INFO @ 12:01:30] Energy consumed for All CPU : 0.000033 kWh\n",
      "[codecarbon INFO @ 12:01:30] Energy consumed for all GPUs : 0.000080 kWh. Total GPU Power : 61.35538995154204 W\n",
      "[codecarbon INFO @ 12:01:30] 0.000194 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 12:01:42] Background scheduler didn't run for a long period (10s), results might be inaccurate\n",
      "[codecarbon INFO @ 12:01:42] Energy consumed for RAM : 0.000286 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:01:42] Delta energy consumed for CPU with cpu_load : 0.000090 kWh, power : 30.90181818181819 W\n",
      "[codecarbon INFO @ 12:01:42] Energy consumed for All CPU : 0.000123 kWh\n",
      "[codecarbon INFO @ 12:01:42] Energy consumed for all GPUs : 0.000734 kWh. Total GPU Power : 213.61405273639406 W\n",
      "[codecarbon INFO @ 12:01:42] 0.001143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:01:42] Energy consumed for RAM : 0.000286 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:01:43] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 43.12 W\n",
      "[codecarbon INFO @ 12:01:43] Energy consumed for All CPU : 0.000123 kWh\n",
      "[codecarbon INFO @ 12:01:43] Energy consumed for all GPUs : 0.000747 kWh. Total GPU Power : 87.17426458602769 W\n",
      "[codecarbon INFO @ 12:01:43] 0.001156 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n",
      "INFO - No device specified. Using best available device: 'cuda'\n",
      "INFO - No device specified. Using best available device: 'cuda'\n",
      "INFO - Evaluating a base model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "[codecarbon WARNING @ 12:03:43] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 12:03:43] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:03:43] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:03:45] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 12:03:45] CPU Model on constant consumption mode: AMD EPYC 7H12 64-Core Processor\n",
      "[codecarbon WARNING @ 12:03:45] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 12:03:45] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:03:45] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:03:45] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 12:03:45] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:03:45]   Platform system: Linux-6.8.0-57-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:03:45]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 12:03:45]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 12:03:45]   Available RAM : 503.526 GB\n",
      "[codecarbon INFO @ 12:03:45]   CPU count: 128 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 12:03:45]   CPU model: AMD EPYC 7H12 64-Core Processor\n",
      "[codecarbon INFO @ 12:03:45]   GPU count: 1\n",
      "[codecarbon INFO @ 12:03:45]   GPU model: 1 x NVIDIA RTX A5000\n",
      "[codecarbon INFO @ 12:03:48] Emissions data (if any) will be saved to file /emissions.csv\n",
      "INFO - No device specified. Using best available device: 'cuda'\n",
      "[codecarbon WARNING @ 12:03:49] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
      "[codecarbon INFO @ 12:03:49] Energy consumed for RAM : 0.000017 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:03:50] Delta energy consumed for CPU with cpu_load : 0.000007 kWh, power : 28.0 W\n",
      "[codecarbon INFO @ 12:03:50] Energy consumed for All CPU : 0.000007 kWh\n",
      "[codecarbon INFO @ 12:03:50] Energy consumed for all GPUs : 0.000017 kWh. Total GPU Power : 43.85832666212704 W\n",
      "[codecarbon INFO @ 12:03:50] 0.000041 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 12:04:47] Background scheduler didn't run for a long period (53s), results might be inaccurate\n",
      "[codecarbon INFO @ 12:04:47] Energy consumed for RAM : 0.001051 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:04:48] Delta energy consumed for CPU with cpu_load : 0.000465 kWh, power : 31.489629629629626 W\n",
      "[codecarbon INFO @ 12:04:48] Energy consumed for All CPU : 0.000472 kWh\n",
      "[codecarbon INFO @ 12:04:48] Energy consumed for all GPUs : 0.003394 kWh. Total GPU Power : 226.4756651135923 W\n",
      "[codecarbon INFO @ 12:04:48] 0.004917 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 12:04:48] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
      "[codecarbon INFO @ 12:04:48] Energy consumed for RAM : 0.001058 kWh. RAM Power : 70.0 W\n",
      "[codecarbon INFO @ 12:04:49] Delta energy consumed for CPU with cpu_load : 0.000003 kWh, power : 28.0 W\n",
      "[codecarbon INFO @ 12:04:49] Energy consumed for All CPU : 0.000475 kWh\n",
      "[codecarbon INFO @ 12:04:49] Energy consumed for all GPUs : 0.003432 kWh. Total GPU Power : 161.7323153110041 W\n",
      "[codecarbon INFO @ 12:04:49] 0.004965 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    EnergyConsumedMetric,\n",
    "    ThroughputMetric,\n",
    "    TorchMetricWrapper,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics\n",
    "metrics = [\n",
    "    EnergyConsumedMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "    ThroughputMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "    TotalTimeMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "    TorchMetricWrapper(\"perplexity\", call_type=\"single\"),\n",
    "]\n",
    "\n",
    "# Define the datamodule\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "datamodule = PrunaDataModule.from_string(\"SmolSmolTalk\", tokenizer=pipe.tokenizer)\n",
    "datamodule.limit_datasets(100)\n",
    "\n",
    "# Define the task and evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Update the model args to ensure right generation arguments are passed after compilation\n",
    "inference_args = {\"max_new_tokens\": 250}\n",
    "\n",
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_model.move_to_device(device)\n",
    "smashed_model.inference_handler.model_args.update(inference_args)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_model)\n",
    "smashed_model.move_to_device(\"cpu\")\n",
    "\n",
    "# Evaluate base model and offload it to CPU\n",
    "base_model = PrunaModel(model=copy_model)\n",
    "base_model.move_to_device(device)\n",
    "base_model.inference_handler.model_args.update(inference_args)\n",
    "base_model_results = eval_agent.evaluate(base_model)\n",
    "base_model.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results of the evaluation and compare the performance of the original and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Metric | Original | Optimized | Difference |\n",
       "|--------|----------|-----------|------------|\n",
       "| perplexity | 41.8264 | 46.9769 | +12.31% |\n",
       "| energy_consumed | 0.0050 | 0.0012 | -76.72% |\n",
       "| throughput | 0.0009 | 0.0050 | +427.17% |\n",
       "| total_time | 53180.1827 | 10087.8858 | -81.03% |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display # noqa\n",
    "\n",
    "\n",
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate differences and prepare table data\n",
    "table_data = []\n",
    "for base_metric_result, smashed_metric_result in zip(base_model_results, smashed_model_results):\n",
    "    diff = calculate_percentage_diff(base_metric_result.result, smashed_metric_result.result)\n",
    "    table_data.append(\n",
    "        {\n",
    "            \"Metric\": base_metric_result.name,\n",
    "            \"Original\": f\"{base_metric_result.result:.4f}\",\n",
    "            \"Optimized\": f\"{smashed_metric_result.result:.4f}\",\n",
    "            \"Difference\": f\"{diff:+.2f}%\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create and display markdown table manually\n",
    "markdown_table = \"| Metric | Original | Optimized | Difference |\\n\"\n",
    "markdown_table += \"|--------|----------|-----------|------------|\\n\"\n",
    "for row in table_data:\n",
    "    markdown_table += f\"| {row['Metric']} | {row['Original']} | {row['Optimized']} | {row['Difference']} |\\n\"\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimized model handles 4x more throughput and consumes only 1/5 of the energy of the base model, while losing only a small portion of its performance based on the perplexity metric, which is expected given the nature of the optimization. Now, we can start to compare, iterate and see what optimization works best for our models, given the metrics we are interested in.\n",
    "\n",
    "We can now save the optimized model to disk and share it with others. Note that some optimizations, such as `torch_compile`, are device dependent and will be re-applied when loading the model on a different device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashed_model.save_pretrained(\"smashed_model\")\n",
    "smashed_model.save_to_hub(\"smashed_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have shown a standard workflow for optimizing and evaluating a large language model. We have used the `SmashConfig` object to define the optimization algorithms and the `EvaluationAgent` to evaluate the performance of the optimized model. We have also used the `PrunaDataModule` to load the dataset and the `Task` object to define the task and evaluation agent.\n",
    "\n",
    "We have shown how to optimize the model using the `smash` function and how to evaluate the performance of the optimized model using the `EvaluationAgent`.\n",
    "\n",
    "Proving we can optimize the model, by making it quicker, more energy efficient and using less memory, while only losing a small amount of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
