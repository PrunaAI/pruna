{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate Large Language Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/llms.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "- **Goal:** Show a standard workflow for optimizing and evaluating a large language model.\n",
    "- **Model:** [HuggingFaceTB/SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct)\n",
    "- **Dataset:** [SmolSmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk)\n",
    "- **Libraries:** [transformers](https://github.com/huggingface/transformers), [datasets](https://github.com/huggingface/datasets)\n",
    "- **Optimization Algorithms:** quantizer(hqq), compiler(torch_compile)\n",
    "- **Evaluation Metrics:**\n",
    "    - **Base Metrics:** elapsed_time\n",
    "    - **Stateful Metrics:** perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Install the dependencies\n",
    "\n",
    "To install the dependencies, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pruna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the device\n",
    "\n",
    "Normally, we would set the device to the best available device to make the most out of the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Before we can optimize the model, we need to ensure that we can load the model and tokenizer correctly and that they can fit in memory. For this example, we will use a nice and small LLM, [SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct), but feel free to use any [text-generation model on Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation). \n",
    "\n",
    "Although Pruna works at least as good with smaller models, a small model is a good starting point to show the steps of the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've loaded the model and tokenizer. Let's see if we can run some inference with them. To make this easy for use, we will be using the `transformers` library's `pipeline` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"I'm a chatbot designed to assist users with their queries and provide helpful information. I was trained on a vast amount of text data from various sources, including Hugging Face's TensorFlow and PyTorch libraries, which enables me to understand and respond to user queries in a structured and efficient manner.\"}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a response to the user's question, which is being cut-off after the allowed `max_new_tokens`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the SmashConfig\n",
    "\n",
    "Now we know the model is working, let's continue with the optimization process and define the `SmashConfig`, which we will use later on to optimize the model.\n",
    "\n",
    "Not all optimization algorithms are available for all models but we can learn a bit more about different optimization algorithms and their requirements in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html) section of the documentation.\n",
    "\n",
    "For the current optimization, we will be using the [`hqq` quantizer](https://docs.pruna.ai/en/stable/compression.html#hqq) and the [`torch_compile` compiler](https://docs.pruna.ai/en/stable/compression.html#torch-compile). We will updating some parameters for these algorithms, setting `hqq_weight_bits` to `4`, `hqq_compute_dtype` to `torch.bfloat16`, `torch_compile_fullgraph` to `True`, `torch_compile_dynamic` to `True`, and `torch_compile_mode` to `max-autotune`. This is one of the many configurations and will just serve as an example.\n",
    "\n",
    "Let's define the `SmashConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n",
      "/Users/davidberenstein/Documents/programming/pruna/prunatree/pruna/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig(device=device)\n",
    "smash_config[\"quantizer\"] = \"hqq\"\n",
    "smash_config[\"hqq_weight_bits\"] = 4\n",
    "smash_config[\"hqq_compute_dtype\"] = \"torch.bfloat16\"\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"torch_compile_fullgraph\"] = True\n",
    "smash_config[\"torch_compile_dynamic\"] = True\n",
    "smash_config[\"torch_compile_mode\"] = \"max-autotune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smash the model\n",
    "\n",
    "Now that we have defined the `SmashConfig` object, we can smash the model. We will be using the `smash` function to smash the model and pass the `model` and `smash_config` to it. We also make a deep copy of the model to avoid modifying the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting quantizer hqq...\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 99/99 [00:00<00:00, 2898.32it/s]\n",
      "100%|██████████| 225/225 [00:10<00:00, 21.95it/s]\n",
      "INFO - quantizer hqq was applied successfully.\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pruna import smash\n",
    "\n",
    "copy_model = copy.deepcopy(model)\n",
    "smashed_model = smash(\n",
    "    model=copy_model,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've optimized the model. Let's see if everything still works as expected and we can run some inference with the optimized model. In this case, we are running the inference by first encoding the prompt through the `tokenizer` and then passing the `input_ids` to the `PrunaModel.generate` method, which also allows us to specify additional parameters such as `max_new_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Unhandled kwargs in generate method: {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n",
      "INFO - Cache size changed from 1x400 to 1x1000. Re-initializing StaticCache.\n",
      "/Users/davidberenstein/Documents/programming/pruna/prunatree/pruna/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:1269: UserWarning: The operator 'aten::index_copy.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "  k_out.index_copy_(2, cache_position, key_states)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m an educational AI assistant, specifically designed to support creative thinking and innovation in various subjects, including creative writing, design thinking, problem-solving, and innovation and entrepreneurship. I don\\'t have any direct personal information or experience related to being \"you\" as a person. If you ever encounter a challenge or need guidance in any way, I\\'m here to help.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who are you?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "tokenized_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([tokenized_prompt], return_tensors=\"pt\").to(smashed_model.device)\n",
    "generated_ids = smashed_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "# Extract only the assistant's message from the decoded output\n",
    "full_response = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "assistant_message = full_response.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a similar response to the original model. \n",
    "\n",
    "If you notice a significant difference, it might have several reasons, the models, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case but also feel free to reach out to us on [Discord](https://discord.gg/Tun8YgzxZ9) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the smashed model\n",
    "\n",
    "Now that we have optimized the model, we can evaluate the performance of the optimized model. We will be using the `EvaluationAgent` to evaluate the performance of the optimized model. We will do so with some basic metrics, the `elapsed_time`, as well as a stateful metrics, the `perplexity`. An overview of the different metrics can be found in our [documentation](https://docs.pruna.ai/).\n",
    "\n",
    "Let's define the `EvaluationAgent` object and start the evaluation process. Note that we are using the `datamodule.limit_datasets(10)` method to limit the number of datasets to 10, which is just for the sake of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Loaded only training and test, splitting train 90/10 into train and validation...\n",
      "/Users/davidberenstein/Documents/programming/pruna/prunatree/pruna/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "INFO - Testing compatibility with text_generation_collate...\n",
      "INFO - Using provided list of metric instances.\n",
      "INFO - Evaluating a base model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "INFO - Evaluating isolated inference metrics.\n"
     ]
    }
   ],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    ElapsedTimeMetric,\n",
    "    TorchMetricWrapper,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics\n",
    "base_metrics = [ElapsedTimeMetric(device=device, n_iterations=2, n_warmup_iterations=1, timing_type=\"async\")]\n",
    "stateful_metrics = [TorchMetricWrapper(\"perplexity\", device=device)]\n",
    "metrics = base_metrics + stateful_metrics\n",
    "\n",
    "# Define the datamodule\n",
    "datamodule = PrunaDataModule.from_string(\"SmolSmolTalk\", tokenizer=tokenizer)\n",
    "datamodule.limit_datasets(10)\n",
    "\n",
    "# Define the task and evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Evaluate base model, and smashed model\n",
    "wrapped_model = PrunaModel(model=model)\n",
    "first_results = eval_agent.evaluate(wrapped_model)\n",
    "smashed_results = eval_agent.evaluate(smashed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at the results of the evaluation and compare the performance of the original and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexity_y_gt': 23.96961784362793,\n",
       " 'inference_elapsed_time_ms_@1': 15294.075012207031,\n",
       " 'inference_latency_ms_@1': 7647.037506103516,\n",
       " 'inference_throughput_batches_per_ms_@1': 0.00013076959531084368}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexity_y_gt': 2071.646240234375,\n",
       " 'inference_elapsed_time_ms_@1': 7000.480890274048,\n",
       " 'inference_latency_ms_@1': 3500.240445137024,\n",
       " 'inference_throughput_batches_per_ms_@1': 0.00028569465888817044}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smashed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results of the evaluation and compare the performance of the original and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage differences between original and optimized model:\n",
      "perplexity_y_gt: 8542.80%\n",
      "inference_elapsed_time_ms_@1: -54.23%\n",
      "inference_latency_ms_@1: -54.23%\n",
      "inference_throughput_batches_per_ms_@1: 118.47%\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa: D103\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate and display percentage differences\n",
    "print(\"Percentage differences between original and optimized model:\")\n",
    "for metric_name in first_results:\n",
    "    diff = calculate_percentage_diff(first_results[metric_name], smashed_results[metric_name])\n",
    "    print(f\"{metric_name}: {diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimized model is roughly 2x faster, while lose some of its performance on perplexity, which is expected given the nature of the optimization. Now, we can start to compare, iterate and see what optimization works best for our models, given the metrics we are interested in.\n",
    "\n",
    "We can now save the optimized model to disk and share it with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashed_model.save_pretrained(\"smashed_model\")\n",
    "smashed_model.save_to_hub(\"smashed_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have shown a standard workflow for optimizing and evaluating a large language model. We have used the `SmashConfig` object to define the optimization algorithms and the `EvaluationAgent` to evaluate the performance of the optimized model. We have also used the `PrunaDataModule` to load the dataset and the `Task` object to define the task and evaluation agent.\n",
    "\n",
    "We have shown how to optimize the model using the `smash` function and how to evaluate the performance of the optimized model using the `EvaluationAgent`.\n",
    "\n",
    "Proving we can optimize the model, by making it quicker, more energy efficient and using less memory, while only losing a small amount of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
