{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize and Evaluate Image Generation Models\n",
    "\n",
    "- **Goal:** Demonstrate a standard workflow for optimizing and evaluating an image generation model.\n",
    "- **Model:** [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)\n",
    "- **Dataset:** [nannullna/laion_subset](https://huggingface.co/datasets/nannullna/laion_subset)\n",
    "- **Libraries:** [diffusers](https://github.com/huggingface/diffusers)\n",
    "- **Optimization Algorithms:** \n",
    "    - **Cacher:** `deepcache`\n",
    "    - **Compiler:** `torch_compile`\n",
    "    - **Quantizer:** `hqq_diffusers`\n",
    "- **Evaluation Metrics:**\n",
    "    - **Base Metrics:** `elapsed_time`\n",
    "    - **Stateful Metrics:** `lpips`, `ssim`, `psnr`, `clip_score`\n",
    "## Getting Started\n",
    "\n",
    "\n",
    "### Install the Dependencies\n",
    "\n",
    "To install the dependencies, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pruna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Set the Device\n",
    "\n",
    "The device is set to the best available option to maximize the benefits of the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Before optimizing the model, we first ensure that it loads correctly and fits into memory. For this example, we will use a lightweight image generation model, [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1), but feel free to use any [text-to-image model on Hugging Face](https://huggingface.co/models?pipeline_tag=text-to-image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.bfloat16)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the pipeline, let's examine some of the outputs it can generate. We use an example from [this amazing prompt guide](https://strikingloo.github.io/stable-diffusion-vs-dalle-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Editorial Style Photo, Bonsai Apple Tree, Task Lighting, Inspiring and Sunset, Afternoon, Beautiful, 4k\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "image.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a beautiful image based on the provided input prompt.\n",
    "\n",
    "## Define the SmashConfig\n",
    "\n",
    "Now that we've confirmed the model is functioning correctly, let's proceed with the optimization process by defining the `SmashConfig`, which will be used later to optimize the model.\n",
    "\n",
    "For diffusion models, the most important categories of optimization algorithms are cachers, compilers, and quantizers. Note that not all algorithms are compatible with every model. For Stable Diffusion models, the following options are available:\n",
    "\n",
    "<img src=\"../assets/images/stable_diffusion_algorithms.png\" alt=\"Stable Diffusion Algorithms\" width=\"600\"/>\n",
    "\n",
    "You can learn more about the various optimization algorithms and their hyperparameters in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html) section of the documentation.\n",
    "\n",
    "In this optimization, we'll combine [``deepcache``](https://docs.pruna.ai/en/stable/compression.html#deepcache), [``torch_compile``](https://docs.pruna.ai/en/stable/compression.html#torch-compile), and [`hqq-diffusers`](https://docs.pruna.ai/en/stable/compression.html#hqq-diffusers). We'll also update some of the parameters for these algorithms, setting `hqq_diffusers_weight_bits` to `4`. This is just one of many possible configurations and is intended to serve as an example.\n",
    "\n",
    "<img src=\"../assets/images/stable_diffusion_quantized.png\" alt=\"Stable Diffusion Algorithms\" width=\"400\"/>\n",
    "\n",
    "Let's define the `SmashConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig(device=device)\n",
    "# configure the deepcache cacher\n",
    "smash_config[\"cacher\"] = \"deepcache\"\n",
    "smash_config[\"deepcache_interval\"] = 2\n",
    "# configure the torch_compile compiler\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "# configure the hqq_diffusers quantizer\n",
    "smash_config[\"quantizer\"] = \"hqq_diffusers\"\n",
    "smash_config[\"hqq_diffusers_weight_bits\"] = 4\n",
    "smash_config[\"hqq_diffusers_group_size\"] = 64\n",
    "smash_config[\"hqq_diffusers_backend\"] = \"marlin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smash the Model\n",
    "\n",
    "Now that we've defined the `SmashConfig` object, we can proceed to smash the model. We'll use the `smash` function, passing both the `model` and the `smash_config` as arguments. We make a deep copy of the model to avoid modifying the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from pruna import smash\n",
    "\n",
    "copy_pipe = copy.deepcopy(pipe)\n",
    "smashed_pipe = smash(\n",
    "    model=copy_pipe,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've smashed the model, let's verify that everything still works as expected by running inference with the smashed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Editorial Style Photo, Bonsai Apple Tree, Task Lighting, Inspiring and Sunset, Afternoon, Beautiful, 4k\"\n",
    "image = smashed_pipe(\n",
    "    prompt,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "image.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to generate a similar image as the original model. \n",
    "\n",
    "If you notice a significant difference, it might have several reasons, the model, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case but also feel free to reach out to us on [Discord](https://discord.gg/Tun8YgzxZ9) if you have any questions or feedback.\n",
    "\n",
    "## Evaluate the Smashed Model\n",
    "\n",
    "Now that the model has been optimized, we can evaluate its performance using the `EvaluationAgent`. This evaluation will include basic metrics like `elapsed_time`, as well as stateful metrics such as `lpips`, `ssim`, `psnr`, and `clip_score`.\n",
    "\n",
    "You can find a complete overview of all available metrics in our [documentation](https://docs.pruna.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    ElapsedTimeMetric,\n",
    "    TorchMetricWrapper,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics\n",
    "base_metrics = [ElapsedTimeMetric(device=device, n_iterations=2, n_warmup_iterations=1, timing_type=\"async\")]\n",
    "stateful_metrics = [TorchMetricWrapper(metric) for metric in [\"lpips\", \"ssim\", \"psnr\", \"clip_score\"]]\n",
    "metrics = base_metrics + stateful_metrics\n",
    "\n",
    "# Define the datamodule\n",
    "datamodule = PrunaDataModule.from_string(\"LAION256\")\n",
    "datamodule.limit_datasets(10)\n",
    "\n",
    "# Define the task and evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Evaluate base model, and smashed model\n",
    "wrapped_pipe = PrunaModel(model=pipe)\n",
    "first_results = eval_agent.evaluate(wrapped_pipe)\n",
    "smashed_results = eval_agent.evaluate(smashed_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now review the evaluation results and compare the performance of the original model with the optimized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smashed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the performance of the original model against the optimized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa: D103\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate and display percentage differences\n",
    "print(\"Percentage differences between original and optimized model:\")\n",
    "for metric_name in first_results:\n",
    "    diff = calculate_percentage_diff(first_results[metric_name], smashed_results[metric_name])\n",
    "    print(f\"{metric_name}: {diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimized model is approximately 2× faster and smaller than the base model. While the CLIP score remains nearly unchanged, the fidelity metrics (LPIPS, SSIM, and PSNR) indicate slight differences in output quality. This is expected, given the nature of the optimization process.\n",
    "\n",
    "We can now save the optimized model to disk or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "smashed_pipe.save_pretrained(\"stable-diffusion-2-1-smashed\")\n",
    "# after saving the model, you can load it with\n",
    "# smashed_pipe = PrunaModel.from_pretrained(\"stable-diffusion-2-1-smashed\")\n",
    "\n",
    "# save the model to HuggingFace\n",
    "smashed_pipe.save_to_hub(\"PrunaAI/stable-diffusion-2-1-smashed\")\n",
    "# smashed_pipe = PrunaModel.from_hub(\"PrunaAI/stable-diffusion-2-1-smashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated a standard workflow for optimizing and evaluating an image generation model using Pruna.\n",
    "\n",
    "We defined our optimization strategy using the `SmashConfig` object and applied it to the model with the `smash` function. We then evaluated the performance of the optimized model using the `EvaluationAgent`, comparing key metrics such as `elapsed_time`, `CLIP score`, and fidelity measures like `LPIPS`, `SSIM`, and `PSNR`.\n",
    "\n",
    "To support the workflow, we also used the `PrunaDataModule` to load the dataset and the `Task` object to configure the task and link it to the evaluation process.\n",
    "\n",
    "The results show that we can significantly improve runtime performance and reduce memory usage and energy consumption, while maintaining a high level of output quality. This makes it easy to explore trade-offs and iterate on configurations to find the best optimization strategy for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
