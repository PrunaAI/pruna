{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruna\n",
    "\n",
    "Pruna is a model-optimization framework designed for developers, enabling you to deliver faster, more efficient models. With Pruna, you can combine multiple optimization algorithms in less than 10 lines of code.\n",
    "\n",
    "There are four main algorithm groups for optimizing ðŸ¤— Diffusers pipelines: quantizers, compilers, cachers, and factorizers. The overview below shows which algorithms are currently supported in each category. Depending on your pipeline, different combinations yield the best results.\n",
    "\n",
    "<img src=\"../assets/images/diffusers_combinations.png\" alt=\"Algorithms for ðŸ¤— Diffusers pipelines\" width=\"400\" />\n",
    "\n",
    "Pruna is available on PyPI, so you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install pruna\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLUX.1 [dev]\n",
    "\n",
    "To optimize FLUX, combine Prunaâ€™s FORA cacher with torch.compile and TorchAOâ€™s dynamic quantization for the best results.\n",
    "\n",
    "<img src=\"../assets/images/flux_combination.png\" alt=\"Algorithm Combination for FLUX\" width=\"400\" />\n",
    "\n",
    "The snippet below is all you need to use this combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "from pruna import SmashConfig, smash\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"cacher\"] = \"fora\"\n",
    "smash_config[\"fora_interval\"] = 2  # or 3 for even faster inference\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"torch_compile_mode\"] = \"max-autotune-no-cudagraphs\"\n",
    "smash_config[\"quantizer\"] = \"torchao\"\n",
    "smash_config[\"torchao_quant_type\"] = \"int8dq\"\n",
    "smash_config[\"torchao_excluded_modules\"] = \"norm+embedding\"\n",
    "smashed_pipe = smash(pipe, smash_config)\n",
    "\n",
    "smashed_pipe(\"a knitted purple prune\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combiantion accelerates inference by up to 4.2Ã— and cuts peak GPU memory usage from 34.7 GB to 28.0 GB, all while maintaining virtually the same output quality. The times were measured on an NVIDIA L40S GPU, so your results may vary on different hardware.\n",
    "\n",
    "<img src=\"../assets/images/flux_smashed_comparison.png\" alt=\"FLUX image comparison\"/>\n",
    "\n",
    "Youâ€™re now set to enjoy lightning-fast inference with FLUX.1 [dev]. To dive into the algorithms we used, explore their hyperparameters, and learn how each impacts output quality, keep reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cacher\n",
    "\n",
    "Diffusion models generate images by starting with pure noise and gradually removing it over multiple inference steps until the final image emerges. At each step, a blurry image is fed into a neural network backbone (for example a transformer), which predicts the noise that should be subtracted from the image.\n",
    "\n",
    "Recent papers have shown that consecutive backbone passes share many similarities. In particular, the outputs of expensive operations within the backbone tend to remain almost the same from one step to the next. This finding motivates the use of caching: if these outputs only differ slightly, we can compute them once and reuse them in subsequent steps.\n",
    "\n",
    "<img src=\"../assets/images/fora_caching.png\" alt=\"FORA caching\" width=\"600\" />\n",
    "\n",
    "In Pruna, each caching algorithm provides an `interval` hyperparameter to tune caching aggressiveness. An `interval` of 3, for example, runs the backbone every third step and reuses the cached outputs for the two intervening steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLUX.1 [dev]\n",
    "\n",
    "For FLUX you can choose between 3 cachers: FORA, PAB, and FasterCache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "cacher = \"fora\"  # or \"pab\" or \"fastercache\"\n",
    "interval = 2  # 3, 4\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"cacher\"] = cacher\n",
    "smash_config[f\"{cacher}_interval\"] = interval\n",
    "smashed_pipe = smash(pipe, smash_config)\n",
    "\n",
    "smashed_pipe(\"a knitted purple prune\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how different cacher-interval configurations impact our model, we measured both inference time and quality metrics on DrawBench:\n",
    "\n",
    "<img src=\"../assets/images/flux_cacher_comparison.png\" alt=\"FLUX cacher comparison\" width=\"600\" />\n",
    "\n",
    "As the chart shows, the FORA cacher achieves the largest speedup while also scoring highest on the ARNIQA metric. We can clearly see the trade-off controlled by the interval parameter: as the interval increases, inference becomes faster but at the cost of quality. In fact, across all cachers, increasing the interval from 3 to 4 causes a drop in quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiler\n",
    "\n",
    "While caching reduces the number of times expensive operations in the backbone are computed, another way of obtaining a speedup is to optimize these operations themselves. Compilers analyze the computations in the backbone and determine which operations can be fused or performed with existing Triton kernels to make computations more efficient. The only draw back is that the first execution takes longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLUX.1 [dev]\n",
    "\n",
    "For FLUX you can choose between 2 compilers: torch.compile and Stable Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "compiler = \"torch_compile\"  # or \"stable_fast\"\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"cacher\"] = \"fora\"\n",
    "smash_config[\"fora_interval\"] = 2  # 3, 4\n",
    "smash_config[\"compiler\"] = compiler\n",
    "smashed_pipe = smash(pipe, smash_config)\n",
    "\n",
    "smashed_pipe(\"a knitted purple prune\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When paired with FORA, we donâ€™t compile the entire Transformer at onceâ€”instead, we compile it block by block. This approach slashes cold-start latency by nearly 50%.\n",
    "\n",
    "<img src=\"../assets/images/flux_latency.png\" alt=\"FLUX compiler warm up time\" width=\"600\" />\n",
    "\n",
    "As the plots below demonstrate, compilation has virtually no impact on output quality. For FLUX, torch.compile achieves greater speedups than Stable Fast.\n",
    "\n",
    "<img src=\"../assets/images/flux_compiler_comparison.png\" alt=\"FLUX compiler warm up time\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizer\n",
    "\n",
    "Quantization lowers the precision of the numbers used to represent a modelâ€™s parameters and calculations. By employing lower bit widths (for example, converting 16-bit floating-point values to 8-bit integers), it reduces model size and speeds up inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLUX.1 [dev]\n",
    "\n",
    "Given its compute-intensive attention mechanism, FLUX benefits most from dynamic quantization, which quantizes both weights and activations. For instance, torchao's dynamic quantization can yield an additional speedup on top of torch.compile. Because modules such as normalization layers can be sensitive to dynamic quantization, we make it easy to exclude them. To get speedups with torchao, we have to use it with torch.compiles \"max-autotune-no-cudagraphs\" mode that will increase the cold start time compared to the default compile mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "smash_config[\"cacher\"] = \"fora\"\n",
    "smash_config[\"fora_interval\"] = 2  # 3, 4\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"quantizer\"] = \"torchao\"\n",
    "smash_config[\"torchao_quant_type\"] = \"int8dq\"\n",
    "smash_config[\"torchao_excluded_modules\"] = \"norm+embedding\"  # or \"none\"\n",
    "smashed_pipe = smash(pipe, smash_config)\n",
    "\n",
    "smashed_pipe(\"a knitted purple prune\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plots below, if these critical modules are filtered out correctly applying dynamic quantization does barely affect the quality while giving a speedup. Further it cuts peak GPU memory usage from 34.7 GB to 28.0 GB.\n",
    "\n",
    "<img src=\"../assets/images/flux_quantizer_comparison.png\" alt=\"FLUX compiler warm up time\" width=\"600\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prunatree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
