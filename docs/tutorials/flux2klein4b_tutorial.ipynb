{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize Flux2 Klein (4B) Image Generation"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/flux2klein4b_tutorial.ipynb\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Component | Details |\n",
        "|-----------|---------|\n",
        "| **Goal** | Demonstrate optimizing and evaluating Flux2 Klein 4B with FORA, quantization, and torch compile |\n",
        "| **Model** | [black-forest-labs/FLUX.2-klein-base-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-base-4B) |\n",
        "| **Optimization Algorithms** | cacher(fora), quantizer(torchao fp8), compiler(torch_compile) |\n",
        "| **Evaluation** | Baseline vs optimized latency comparison |\n",
        "\n",
        "This tutorial demonstrates how to use **Pruna** to speed up image generation with Flux2 using a combination of three optimization techniques:\n",
        "\n",
        "1. **FORA (Fast Output Reuse Acceleration)** - Caches transformer block outputs and reuses them for subsequent diffusion steps\n",
        "2. **TorchAO Quantization (FP8)** - Reduces memory bandwidth by using 8-bit floating point weights\n",
        "3. **Torch Compile** - JIT compiles the model for optimized GPU execution\n",
        "\n",
        "Together, these optimizations can achieve **2-3x speedup** while maintaining image quality.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA GPU with CUDA support (compute capability â‰¥ 8.9 for FP8). Note that FP8 quantization is hardware-specific and requires modern GPUs such as H100.\n",
        "- `pruna` library installed (`pip install pruna`)\n",
        "- `diffusers` with Flux2 support\n",
        "- HuggingFace account with access to Flux2 models\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "To install the dependencies, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pruna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The device is set to the best available option to maximize the benefits of the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Model\n",
        "\n",
        "Before optimizing the model, we load the Flux2-Klein-Base-4B pipeline. You need a HuggingFace account with access to the model; run the login cell below with your token. Do not use `enable_model_cpu_offload()` as it interferes with FORA's caching mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to HuggingFace (required for Flux2 model access)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace with your HuggingFace token\n",
        "HF_TOKEN = \"your_hf_token_here\"\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import Flux2KleinPipeline\n",
        "from pruna import SmashConfig, smash\n",
        "\n",
        "# Check CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the Flux2-Klein-Base-4B pipeline. This is a 4 billion parameter model optimized for high-quality image generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Flux2 pipeline...\")\n",
        "pipe = Flux2KleinPipeline.from_pretrained(\n",
        "    \"black-forest-labs/FLUX.2-klein-base-4B\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "print(\"Pipeline loaded!\")\n",
        "\n",
        "# NOTE: Do NOT use pipe.enable_model_cpu_offload()\n",
        "# It breaks FORA's caching mechanism by re-initializing model hooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Generation Parameters\n",
        "\n",
        "Set up the common parameters for image generation. We use the same parameters for both baseline and optimized models to ensure a fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation parameters\n",
        "prompt = \"A humanoid robot or cyborg set against a bold red background\"\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    image=None,\n",
        "    prompt=prompt,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    guidance_scale=4.0,\n",
        "    num_inference_steps=50,\n",
        ")\n",
        "\n",
        "# Use same seed for reproducible results\n",
        "def get_generator():\n",
        "    return torch.Generator(device=device).manual_seed(42)\n",
        "\n",
        "# FORA parameters\n",
        "FORA_START_STEP = 4  # Start caching after this many steps\n",
        "FORA_INTERVAL = 3    # Recompute every N steps (higher = faster but lower quality)\n",
        "\n",
        "# Benchmarking parameters\n",
        "NUM_WARMUP = 2  # Warmup runs (not timed)\n",
        "NUM_TIMED = 3   # Timed runs for averaging\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Image size: {gen_kwargs['height']}x{gen_kwargs['width']}\")\n",
        "print(f\"Inference steps: {gen_kwargs['num_inference_steps']}\")\n",
        "print(f\"FORA start_step: {FORA_START_STEP}, interval: {FORA_INTERVAL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Benchmark Baseline (No Optimization)\n",
        "\n",
        "First, we measure the baseline performance without any optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move pipeline to GPU\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "# ============================================================\n",
        "# BASELINE: Warmup then Measure\n",
        "# ============================================================\n",
        "print(f\"--- Warming up BASELINE ({NUM_WARMUP} runs) ---\")\n",
        "for i in range(NUM_WARMUP):\n",
        "    print(f\"  Warmup {i+1}/{NUM_WARMUP}...\")\n",
        "    _ = pipe(**gen_kwargs, generator=get_generator())\n",
        "torch.cuda.synchronize()\n",
        "print(\"Baseline warmup complete.\")\n",
        "\n",
        "print(f\"\\n--- Timing BASELINE ({NUM_TIMED} runs) ---\")\n",
        "baseline_times = []\n",
        "for i in range(NUM_TIMED):\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "    \n",
        "    torch.cuda.synchronize()\n",
        "    start_event.record()\n",
        "    image_baseline = pipe(**gen_kwargs, generator=get_generator()).images[0]\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed = start_event.elapsed_time(end_event) / 1000  # ms to seconds\n",
        "    baseline_times.append(elapsed)\n",
        "    print(f\"  Run {i+1}: {elapsed:.2f}s\")\n",
        "\n",
        "avg_baseline = sum(baseline_times) / len(baseline_times)\n",
        "print(f\"\\nBaseline average: {avg_baseline:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define SmashConfig and Smash the Model\n",
        "\n",
        "Now we create an optimized model using Pruna's `smash` function with three techniques:\n",
        "\n",
        "### FORA Parameters\n",
        "\n",
        "- **`fora_interval`**: How often to recompute transformer outputs (higher = faster, but lower quality)\n",
        "- **`fora_start_step`**: Number of initial steps to run without caching (higher = better quality early steps)\n",
        "- **`fora_backbone_calls_per_step`**: Set to 2 for Classifier-Free Guidance (CFG)\n",
        "\n",
        "### TorchAO Quantization\n",
        "\n",
        "- **`quant_type: fp8wo`**: FP8 weight-only quantization - reduces memory bandwidth while keeping activations in full precision\n",
        "- **`target_modules`**: Specify which layers to quantize (we target `single_transformer_blocks` and exclude norms/embeddings)\n",
        "\n",
        "### Torch Compile\n",
        "\n",
        "- **`torch_compile: True`**: Enables JIT compilation for optimized GPU kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Create optimized model with FORA + Quantization + Compile\n",
        "# ============================================================\n",
        "print(\"--- Creating optimized model ---\")\n",
        "\n",
        "smash_config = SmashConfig({\n",
        "    # FORA: Cache and reuse transformer outputs\n",
        "    \"fora\": {\n",
        "        \"fora_interval\": FORA_INTERVAL,\n",
        "        \"fora_start_step\": FORA_START_STEP,\n",
        "        \"fora_backbone_calls_per_step\": 2,  # 2 for CFG (conditional + unconditional)\n",
        "    },\n",
        "    # TorchAO: FP8 weight-only quantization for reduced memory bandwidth\n",
        "    \"torchao\": {\n",
        "        \"quant_type\": \"fp8wo\",\n",
        "        \"target_modules\": {\n",
        "            \"include\": [\"*single_transformer_blocks.*\"],  # Quantize single-stream blocks\n",
        "            \"exclude\": [\"pe_embedder\", \"*norm*\", \"*embed*\"]  # Skip norms and embeddings\n",
        "        },\n",
        "    },\n",
        "    # Torch Compile: JIT compilation for optimized kernels\n",
        "    \"torch_compile\": True\n",
        "})\n",
        "\n",
        "# Create a deep copy of the pipeline and apply optimizations\n",
        "smashed_model = smash(model=copy.deepcopy(pipe), smash_config=smash_config)\n",
        "print(\"Optimized model created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Benchmark Optimized Model\n",
        "\n",
        "The first warmup run may be slower due to torch.compile JIT compilation. Subsequent runs show the optimized performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Optimized Model: Warmup then Measure\n",
        "# ============================================================\n",
        "print(f\"--- Warming up optimized model ({NUM_WARMUP} runs) ---\")\n",
        "print(\"(First run includes torch.compile JIT compilation overhead)\")\n",
        "for i in range(NUM_WARMUP):\n",
        "    print(f\"  Warmup {i+1}/{NUM_WARMUP}...\")\n",
        "    _ = smashed_model(**gen_kwargs, generator=get_generator())\n",
        "torch.cuda.synchronize()\n",
        "print(\"Warmup complete.\")\n",
        "\n",
        "print(f\"\\n--- Timing optimized model ({NUM_TIMED} runs) ---\")\n",
        "fora_times = []\n",
        "for i in range(NUM_TIMED):\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "    \n",
        "    torch.cuda.synchronize()\n",
        "    start_event.record()\n",
        "    image_fora = smashed_model(**gen_kwargs, generator=get_generator()).images[0]\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed = start_event.elapsed_time(end_event) / 1000  # ms to seconds\n",
        "    fora_times.append(elapsed)\n",
        "    print(f\"  Run {i+1}: {elapsed:.2f}s\")\n",
        "\n",
        "avg_fora = sum(fora_times) / len(fora_times)\n",
        "print(f\"\\nOptimized average: {avg_fora:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Results Summary\n",
        "# ============================================================\n",
        "speedup = avg_baseline / avg_fora\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"RESULTS: FORA + FP8 Quantization + Torch Compile\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"FORA: start_step={FORA_START_STEP}, interval={FORA_INTERVAL}\")\n",
        "print(f\"Quantization: FP8 weight-only on single_transformer_blocks\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Baseline:   {avg_baseline:.2f}s\")\n",
        "print(f\"Optimized:  {avg_fora:.2f}s\")\n",
        "print(f\"Speedup:    {speedup:.2f}x\")\n",
        "print(f\"Time saved: {avg_baseline - avg_fora:.2f}s per image\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Visual Comparison\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "axes[0].imshow(image_baseline)\n",
        "axes[0].set_title(f\"Baseline\\n{avg_baseline:.2f}s\", fontsize=14)\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(image_fora)\n",
        "axes[1].set_title(f\"Optimized (FORA + FP8 + Compile)\\n{avg_fora:.2f}s ({speedup:.2f}x faster)\", fontsize=14)\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"baseline_vs_fora.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free GPU memory\n",
        "del smashed_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we demonstrated a workflow for optimizing and evaluating the Flux2 Klein 4B image generation model using Pruna. We defined a `SmashConfig` combining FORA (cacher), TorchAO FP8 quantization, and torch compile, applied it with `smash`, and compared baseline vs optimized latency. The results show that these optimizations can achieve significant speedup while maintaining image quality. You can adapt the configuration to your use case or reach out on [Discord](https://discord.gg/JFQmtFKCjd) for questions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fluxnew",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
