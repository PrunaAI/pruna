{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate Reasoning Large Language Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/reasoning_llm.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Goal** | Showcase a standard workflow for optimizing and evaluating a reasoning Large Language Model |\n",
    "| **Model** |[Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) |\n",
    "| **Dataset** | [SmolSmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk)  |\n",
    "| **Optimization Algorithms** | quantizer(hqq), compiler(torch_compile) |\n",
    "| **Evaluation Metrics** | `total time`, `perplexity`, `throughput`, `energy_consumed` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To install the required dependencies, you can run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /root/miniconda3/envs/pruna0/bin/python\n",
      "Torch: 2.7.0+cu126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HQQ: 0.2.7.post1\n",
      "Pruna: 0.2.9\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, os\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "try:\n",
    "    import hqq, pruna\n",
    "    print(\"HQQ:\", getattr(hqq, \"__version__\", \"unknown\"))\n",
    "    print(\"Pruna:\", getattr(pruna, \"__version__\", \"unknown\"))\n",
    "except Exception as e:\n",
    "    print(\"Import check:\", e)\n",
    "\n",
    "# Safety shim: only triggers for meta-tensor case\n",
    "import torch.nn as nn\n",
    "_orig_to = nn.Module.to\n",
    "def _safe_to(self, *args, **kwargs):\n",
    "    try:\n",
    "        return _orig_to(self, *args, **kwargs)\n",
    "    except NotImplementedError as e:\n",
    "        if \"Cannot copy out of meta tensor\" in str(e):\n",
    "            device = kwargs.get(\"device\", None)\n",
    "            dtype = kwargs.get(\"dtype\", None)\n",
    "            if device is None and args:\n",
    "                device = args[0]\n",
    "            if dtype is None and len(args) > 1:\n",
    "                dtype = args[1]\n",
    "            return self.to_empty(device=device, dtype=dtype)\n",
    "        raise\n",
    "nn.Module.to = _safe_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pruna0/bin/python\n",
      "Torch 2.7.0+cu126\n",
      "Name: hqq\n",
      "Version: 0.2.7.post1\n",
      "Summary: Half-Quadratic Quantization (HQQ)\n",
      "Home-page: https://github.com/mobiusml/hqq/\n",
      "Author: Dr. Hicham Badri\n",
      "Author-email: hicham@mobiuslabs.com\n",
      "License: Apache 2\n",
      "Location: /root/miniconda3/envs/pruna0/lib/python3.10/site-packages\n",
      "Requires: accelerate, einops, huggingface_hub, numpy, termcolor, tqdm, transformers\n",
      "Required-by: pruna\n",
      "---\n",
      "Name: pruna\n",
      "Version: 0.2.9\n",
      "Summary: Smash your AI models\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Pruna AI <hello@pruna.ai>\n",
      "License: Copyright 2025 - Pruna AI GmbH. All rights reserved.\n",
      "        \n",
      "                                         Apache License\n",
      "                                   Version 2.0, January 2004\n",
      "                                http://www.apache.org/licenses/\n",
      "        \n",
      "           TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "        \n",
      "           1. Definitions.\n",
      "        \n",
      "              \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "              and distribution as defined by Sections 1 through 9 of this document.\n",
      "        \n",
      "              \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "              the copyright owner that is granting the License.\n",
      "        \n",
      "              \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "              other entities that control, are controlled by, or are under common\n",
      "              control with that entity. For the purposes of this definition,\n",
      "              \"control\" means (i) the power, direct or indirect, to cause the\n",
      "              direction or management of such entity, whether by contract or\n",
      "              otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "              outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "        \n",
      "              \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "              exercising permissions granted by this License.\n",
      "        \n",
      "              \"Source\" form shall mean the preferred form for making modifications,\n",
      "              including but not limited to software source code, documentation\n",
      "              source, and configuration files.\n",
      "        \n",
      "              \"Object\" form shall mean any form resulting from mechanical\n",
      "              transformation or translation of a Source form, including but\n",
      "              not limited to compiled object code, generated documentation,\n",
      "              and conversions to other media types.\n",
      "        \n",
      "              \"Work\" shall mean the work of authorship, whether in Source or\n",
      "              Object form, made available under the License, as indicated by a\n",
      "              copyright notice that is included in or attached to the work\n",
      "              (an example is provided in the Appendix below).\n",
      "        \n",
      "              \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "              form, that is based on (or derived from) the Work and for which the\n",
      "              editorial revisions, annotations, elaborations, or other modifications\n",
      "              represent, as a whole, an original work of authorship. For the purposes\n",
      "              of this License, Derivative Works shall not include works that remain\n",
      "              separable from, or merely link (or bind by name) to the interfaces of,\n",
      "              the Work and Derivative Works thereof.\n",
      "        \n",
      "              \"Contribution\" shall mean any work of authorship, including\n",
      "              the original version of the Work and any modifications or additions\n",
      "              to that Work or Derivative Works thereof, that is intentionally\n",
      "              submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "              or by an individual or Legal Entity authorized to submit on behalf of\n",
      "              the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "              means any form of electronic, verbal, or written communication sent\n",
      "              to the Licensor or its representatives, including but not limited to\n",
      "              communication on electronic mailing lists, source code control systems,\n",
      "              and issue tracking systems that are managed by, or on behalf of, the\n",
      "              Licensor for the purpose of discussing and improving the Work, but\n",
      "              excluding communication that is conspicuously marked or otherwise\n",
      "              designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "        \n",
      "              \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "              on behalf of whom a Contribution has been received by Licensor and\n",
      "              subsequently incorporated within the Work.\n",
      "        \n",
      "           2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "              this License, each Contributor hereby grants to You a perpetual,\n",
      "              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "              copyright license to reproduce, prepare Derivative Works of,\n",
      "              publicly display, publicly perform, sublicense, and distribute the\n",
      "              Work and such Derivative Works in Source or Object form.\n",
      "        \n",
      "           3. Grant of Patent License. Subject to the terms and conditions of\n",
      "              this License, each Contributor hereby grants to You a perpetual,\n",
      "              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "              (except as stated in this section) patent license to make, have made,\n",
      "              use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "              where such license applies only to those patent claims licensable\n",
      "              by such Contributor that are necessarily infringed by their\n",
      "              Contribution(s) alone or by combination of their Contribution(s)\n",
      "              with the Work to which such Contribution(s) was submitted. If You\n",
      "              institute patent litigation against any entity (including a\n",
      "              cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "              or a Contribution incorporated within the Work constitutes direct\n",
      "              or contributory patent infringement, then any patent licenses\n",
      "              granted to You under this License for that Work shall terminate\n",
      "              as of the date such litigation is filed.\n",
      "        \n",
      "           4. Redistribution. You may reproduce and distribute copies of the\n",
      "              Work or Derivative Works thereof in any medium, with or without\n",
      "              modifications, and in Source or Object form, provided that You\n",
      "              meet the following conditions:\n",
      "        \n",
      "              (a) You must give any other recipients of the Work or\n",
      "                  Derivative Works a copy of this License; and\n",
      "        \n",
      "              (b) You must cause any modified files to carry prominent notices\n",
      "                  stating that You changed the files; and\n",
      "        \n",
      "              (c) You must retain, in the Source form of any Derivative Works\n",
      "                  that You distribute, all copyright, patent, trademark, and\n",
      "                  attribution notices from the Source form of the Work,\n",
      "                  excluding those notices that do not pertain to any part of\n",
      "                  the Derivative Works; and\n",
      "        \n",
      "              (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "                  distribution, then any Derivative Works that You distribute must\n",
      "                  include a readable copy of the attribution notices contained\n",
      "                  within such NOTICE file, excluding those notices that do not\n",
      "                  pertain to any part of the Derivative Works, in at least one\n",
      "                  of the following places: within a NOTICE text file distributed\n",
      "                  as part of the Derivative Works; within the Source form or\n",
      "                  documentation, if provided along with the Derivative Works; or,\n",
      "                  within a display generated by the Derivative Works, if and\n",
      "                  wherever such third-party notices normally appear. The contents\n",
      "                  of the NOTICE file are for informational purposes only and\n",
      "                  do not modify the License. You may add Your own attribution\n",
      "                  notices within Derivative Works that You distribute, alongside\n",
      "                  or as an addendum to the NOTICE text from the Work, provided\n",
      "                  that such additional attribution notices cannot be construed\n",
      "                  as modifying the License.\n",
      "        \n",
      "              You may add Your own copyright statement to Your modifications and\n",
      "              may provide additional or different license terms and conditions\n",
      "              for use, reproduction, or distribution of Your modifications, or\n",
      "              for any such Derivative Works as a whole, provided Your use,\n",
      "              reproduction, and distribution of the Work otherwise complies with\n",
      "              the conditions stated in this License.\n",
      "        \n",
      "           5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "              any Contribution intentionally submitted for inclusion in the Work\n",
      "              by You to the Licensor shall be under the terms and conditions of\n",
      "              this License, without any additional terms or conditions.\n",
      "              Notwithstanding the above, nothing herein shall supersede or modify\n",
      "              the terms of any separate license agreement you may have executed\n",
      "              with Licensor regarding such Contributions.\n",
      "        \n",
      "           6. Trademarks. This License does not grant permission to use the trade\n",
      "              names, trademarks, service marks, or product names of the Licensor,\n",
      "              except as required for reasonable and customary use in describing the\n",
      "              origin of the Work and reproducing the content of the NOTICE file.\n",
      "        \n",
      "           7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "              agreed to in writing, Licensor provides the Work (and each\n",
      "              Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "              WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "              implied, including, without limitation, any warranties or conditions\n",
      "              of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "              PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "              appropriateness of using or redistributing the Work and assume any\n",
      "              risks associated with Your exercise of permissions under this License.\n",
      "        \n",
      "           8. Limitation of Liability. In no event and under no legal theory,\n",
      "              whether in tort (including negligence), contract, or otherwise,\n",
      "              unless required by applicable law (such as deliberate and grossly\n",
      "              negligent acts) or agreed to in writing, shall any Contributor be\n",
      "              liable to You for damages, including any direct, indirect, special,\n",
      "              incidental, or consequential damages of any character arising as a\n",
      "              result of this License or out of the use or inability to use the\n",
      "              Work (including but not limited to damages for loss of goodwill,\n",
      "              work stoppage, computer failure or malfunction, or any and all\n",
      "              other commercial damages or losses), even if such Contributor\n",
      "              has been advised of the possibility of such damages.\n",
      "        \n",
      "           9. Accepting Warranty or Additional Liability. While redistributing\n",
      "              the Work or Derivative Works thereof, You may choose to offer,\n",
      "              and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "              or other liability obligations and/or rights consistent with this\n",
      "              License. However, in accepting such obligations, You may act only\n",
      "              on Your own behalf and on Your sole responsibility, not on behalf\n",
      "              of any other Contributor, and only if You agree to indemnify,\n",
      "              defend, and hold each Contributor harmless for any liability\n",
      "              incurred by, or claims asserted against, such Contributor by reason\n",
      "              of your accepting any such warranty or additional liability.\n",
      "        \n",
      "           END OF TERMS AND CONDITIONS\n",
      "        \n",
      "           APPENDIX: How to apply the Apache License to your work.\n",
      "        \n",
      "              To apply the Apache License to your work, attach the following\n",
      "              boilerplate notice, with the fields enclosed by brackets \"[]\"\n",
      "              replaced with your own identifying information. (Don't include\n",
      "              the brackets!)  The text should be enclosed in the appropriate\n",
      "              comment syntax for the file format. We also recommend that a\n",
      "              file or class name and description of purpose be included on the\n",
      "              same \"printed page\" as the copyright notice for easier\n",
      "              identification within third-party archives.\n",
      "        \n",
      "           Copyright 2025 Pruna AI.\n",
      "        \n",
      "           Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "           you may not use this file except in compliance with the License.\n",
      "           You may obtain a copy of the License at\n",
      "        \n",
      "               http://www.apache.org/licenses/LICENSE-2.0\n",
      "        \n",
      "           Unless required by applicable law or agreed to in writing, software\n",
      "           distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "           WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "           See the License for the specific language governing permissions and\n",
      "           limitations under the License.\n",
      "Location: /root/miniconda3/envs/pruna0/lib/python3.10/site-packages\n",
      "Requires: aenum, bitsandbytes, codecarbon, colorama, configspace, ctranslate2, datasets, deepcache, diffusers, gliner, hqq, huggingface-hub, kernels, librosa, llmcompressor, numpy, opencv-python, opentelemetry-api, opentelemetry-exporter-otlp, opentelemetry-sdk, openvino, optimum, optimum-quanto, piq, pynvml, pytorch-lightning, requests, sentencepiece, soundfile, thop, timm, torch, torch-pruning, torchao, torchmetrics, torchvision, transformers, whisper-s2t\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!python -c \"import sys, torch; print(sys.executable); print('Torch', torch.__version__)\"\n",
    "!pip show hqq pruna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pruna==0.2.9\n",
      "  Using cached pruna-0.2.9-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting aenum (from pruna==0.2.9)\n",
      "  Using cached aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bitsandbytes (from pruna==0.2.9)\n",
      "  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting codecarbon (from pruna==0.2.9)\n",
      "  Using cached codecarbon-3.0.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting colorama (from pruna==0.2.9)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting configspace>=1.2.1 (from pruna==0.2.9)\n",
      "  Using cached configspace-1.2.1-py3-none-any.whl\n",
      "Collecting ctranslate2==4.6.0 (from pruna==0.2.9)\n",
      "  Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets<=3.5.0 (from pruna==0.2.9)\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepcache (from pruna==0.2.9)\n",
      "  Using cached DeepCache-0.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting diffusers>=0.21.4 (from pruna==0.2.9)\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting gliner (from pruna==0.2.9)\n",
      "  Using cached gliner-0.2.21-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting hqq==0.2.7.post1 (from pruna==0.2.9)\n",
      "  Using cached hqq-0.2.7.post1-py3-none-any.whl\n",
      "Collecting huggingface-hub>=0.30.0 (from huggingface-hub[hf-xet]>=0.30.0->pruna==0.2.9)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting kernels (from pruna==0.2.9)\n",
      "  Using cached kernels-0.9.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting librosa (from pruna==0.2.9)\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting llmcompressor (from pruna==0.2.9)\n",
      "  Using cached llmcompressor-0.6.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna==0.2.9)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting opencv-python (from pruna==0.2.9)\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting opentelemetry-api>=1.30.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.29.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.30.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting openvino (from pruna==0.2.9)\n",
      "  Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting optimum (from pruna==0.2.9)\n",
      "  Using cached optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting optimum-quanto>=0.2.5 (from pruna==0.2.9)\n",
      "  Using cached optimum_quanto-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting piq (from pruna==0.2.9)\n",
      "  Using cached piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pynvml (from pruna==0.2.9)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pytorch-lightning (from pruna==0.2.9)\n",
      "  Using cached pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting requests>=2.31.0 (from pruna==0.2.9)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sentencepiece (from pruna==0.2.9)\n",
      "  Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting soundfile (from pruna==0.2.9)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting thop (from pruna==0.2.9)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting timm (from pruna==0.2.9)\n",
      "  Using cached timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting torch-pruning (from pruna==0.2.9)\n",
      "  Using cached torch_pruning-1.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting torch==2.7.0 (from pruna==0.2.9)\n",
      "  Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchao (from pruna==0.2.9)\n",
      "  Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting torchmetrics==1.7.4 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting torchvision==0.22.0 (from pruna==0.2.9)\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting transformers (from pruna==0.2.9)\n",
      "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting whisper-s2t==1.3.1 (from pruna==0.2.9)\n",
      "  Using cached whisper_s2t-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from ctranslate2==4.6.0->pruna==0.2.9)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pyyaml<7,>=5.3 (from ctranslate2==4.6.0->pruna==0.2.9)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting tqdm>=4.64.1 (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting einops (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting termcolor (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting packaging>17.1 (from torchmetrics==1.7.4->torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.7.4->torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting scipy>1.0.0 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.22.0->pruna==0.2.9)\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting rich (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting platformdirs (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting openai-whisper (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached openai_whisper-20250625-py3-none-any.whl\n",
      "Collecting nvidia-ml-py (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached nvidia_ml_py-13.580.65-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyparsing (from configspace>=1.2.1->pruna==0.2.9)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting more_itertools (from configspace>=1.2.1->pruna==0.2.9)\n",
      "  Using cached more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting importlib_metadata (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.30.0->huggingface-hub[hf-xet]>=0.30.0->pruna==0.2.9)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.36.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.36.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf<7.0,>=5.0 (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.30.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ninja (from optimum-quanto>=0.2.5->pruna==0.2.9)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting psutil (from accelerate->hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting arrow (from codecarbon->pruna==0.2.9)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting click (from codecarbon->pruna==0.2.9)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting fief-client[cli] (from codecarbon->pruna==0.2.9)\n",
      "  Using cached fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting prometheus_client (from codecarbon->pruna==0.2.9)\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting py-cpuinfo (from codecarbon->pruna==0.2.9)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from codecarbon->pruna==0.2.9)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting rapidfuzz (from codecarbon->pruna==0.2.9)\n",
      "  Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting questionary (from codecarbon->pruna==0.2.9)\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting typer (from codecarbon->pruna==0.2.9)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting python-dateutil>=2.7.0 (from arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7.0->arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting cryptography>=3.4 (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached cryptography-45.0.6-cp37-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting onnxruntime (from gliner->pruna==0.2.9)\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tomli>=2.0 (from kernels->pruna==0.2.9)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->pruna==0.2.9)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting decorator>=4.3.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pooch>=1.1 (from librosa->pruna==0.2.9)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->pruna==0.2.9)\n",
      "  Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->pruna==0.2.9)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->pruna==0.2.9)\n",
      "  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->pruna==0.2.9)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting loguru (from llmcompressor->pruna==0.2.9)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna==0.2.9)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting transformers (from pruna==0.2.9)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from llmcompressor->pruna==0.2.9)\n",
      "  Using cached compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting tiktoken (from openai-whisper->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached tiktoken-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python (from pruna==0.2.9)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino->pruna==0.2.9)\n",
      "  Using cached openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-ml-py (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting prompt_toolkit<4.0,>=2.0 (from questionary->codecarbon->pruna==0.2.9)\n",
      "  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon->pruna==0.2.9)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer->codecarbon->pruna==0.2.9)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting termcolor (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Using cached pruna-0.2.9-py3-none-any.whl (207 kB)\n",
      "Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
      "Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "Using cached whisper_s2t-1.3.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached optimum_quanto-0.2.7-py3-none-any.whl (165 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Using cached aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "Using cached codecarbon-3.0.4-py3-none-any.whl (277 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl (17 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached DeepCache-0.1.1-py3-none-any.whl (190 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached cryptography-45.0.6-cp37-abi3-manylinux_2_34_x86_64.whl (4.4 MB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached gliner-0.2.21-py3-none-any.whl (65 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached kernels-0.9.0-py3-none-any.whl (37 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached llmcompressor-0.6.0.1-py3-none-any.whl (253 kB)\n",
      "Using cached compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl (47.6 MB)\n",
      "Using cached openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached optimum-1.27.0-py3-none-any.whl (425 kB)\n",
      "Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached piq-0.8.0-py3-none-any.whl (106 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
      "Using cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Using cached tiktoken-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "Using cached torch_pruning-1.6.0-py3-none-any.whl (68 kB)\n",
      "Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: wcwidth, torchao, pytz, py-cpuinfo, openvino-telemetry, nvidia-ml-py, nvidia-cusparselt-cu12, mpmath, flatbuffers, aenum, zipp, xxhash, urllib3, tzdata, typing-extensions, types-python-dateutil, tqdm, tomli, threadpoolctl, termcolor, sympy, sniffio, six, shellingham, setuptools, sentencepiece, safetensors, regex, rapidfuzz, pyyaml, pyparsing, pynvml, pygments, pycparser, pyarrow, psutil, protobuf, propcache, prompt_toolkit, prometheus_client, platformdirs, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, msgpack, more_itertools, mdurl, MarkupSafe, loguru, llvmlite, joblib, idna, humanfriendly, hf-xet, h11, grpcio, fsspec, frozenlist, filelock, einops, dill, decorator, colorama, click, charset_normalizer, certifi, audioread, attrs, async-timeout, annotated-types, aiohappyeyeballs, yaspin, typing-inspection, triton, soxr, scipy, requests, questionary, python-dateutil, pydantic-core, openvino, opentelemetry-proto, opencv-python, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, multiprocess, multidict, markdown-it-py, lightning-utilities, lazy_loader, jinja2, importlib_metadata, httpcore, googleapis-common-protos, exceptiongroup, ctranslate2, coloredlogs, cffi, aiosignal, yarl, tiktoken, soundfile, scikit-learn, rich, pydantic, pooch, pandas, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, nvidia-cusolver-cu12, huggingface-hub, cryptography, configspace, arrow, anyio, typer, torch, tokenizers, opentelemetry-semantic-conventions, librosa, kernels, jwcrypto, httpx, diffusers, aiohttp, transformers, torchvision, torchmetrics, torch-pruning, thop, optimum-quanto, opentelemetry-sdk, openai-whisper, fief-client, bitsandbytes, accelerate, torch-fidelity, timm, pytorch-lightning, piq, optimum, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, hqq, gliner, deepcache, datasets, compressed-tensors, whisper-s2t, opentelemetry-exporter-otlp, llmcompressor, codecarbon, pruna\n",
      "\u001b[2K  Attempting uninstall: wcwidth\n",
      "\u001b[2K    Found existing installation: wcwidth 0.2.13\n",
      "\u001b[2K    Uninstalling wcwidth-0.2.13:\n",
      "\u001b[2K      Successfully uninstalled wcwidth-0.2.13\n",
      "\u001b[2K  Attempting uninstall: zippm\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]]cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: zipp 3.23.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling zipp-3.23.0:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled zipp-3.23.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.1\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: six[90m\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]ensions]\n",
      "\u001b[2K    Found existing installation: six 1.17.0\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling six-1.17.0:\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: setuptools\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: setuptools 78.1.1\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling setuptools-78.1.1:\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled setuptools-78.1.1\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: pygmentsm\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]]\n",
      "\u001b[2K    Found existing installation: Pygments 2.19.2\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling Pygments-2.19.2:\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K      Successfully uninstalled Pygments-2.19.2\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K  Attempting uninstall: psutil\u001b[90m\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]r]\n",
      "\u001b[2K    Found existing installation: psutil 5.9.1\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling psutil-5.9.1:m\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled psutil-5.9.1\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: prompt_toolkit\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: prompt_toolkit 3.0.51\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling prompt_toolkit-3.0.51:\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled prompt_toolkit-3.0.51\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: platformdirs\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Found existing installation: platformdirs 4.3.8\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Uninstalling platformdirs-4.3.8:\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K      Successfully uninstalled platformdirs-4.3.8\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K  Attempting uninstall: packaging90m\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]kit]\n",
      "\u001b[2K    Found existing installation: packaging 25.0\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: decoratorm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 71/167\u001b[0m [dill]o]e]blas-cu12]u12]2]\n",
      "\u001b[2K    Found existing installation: decorator 5.2.1\u001b[0m \u001b[32m 71/167\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling decorator-5.2.1:0m\u001b[90m\u001b[0m \u001b[32m 71/167\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled decorator-5.2.1\u001b[0m \u001b[32m 71/167\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil[0m\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]]]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: importlib_metadata\u001b[0m\u001b[90m\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]cudnn-cu12]12]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:\u001b[90m\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K  Attempting uninstall: exceptiongroup\u001b[0m\u001b[90m\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K    Found existing installation: exceptiongroup 1.3.0\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K    Uninstalling exceptiongroup-1.3.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K      Successfully uninstalled exceptiongroup-1.3.0\u001b[0m \u001b[32m103/167\u001b[0m [jinja2]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m167/167\u001b[0m [pruna]m [llmcompressor]ets]]ghtning]12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.10.0 aenum-3.1.16 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 arrow-1.3.0 async-timeout-5.0.1 attrs-25.3.0 audioread-3.0.1 bitsandbytes-0.47.0 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 click-8.2.1 codecarbon-3.0.4 colorama-0.4.6 coloredlogs-15.0.1 compressed-tensors-0.10.2 configspace-1.2.1 cryptography-45.0.6 ctranslate2-4.6.0 datasets-3.5.0 decorator-5.2.1 deepcache-0.1.1 diffusers-0.34.0 dill-0.3.8 einops-0.8.1 exceptiongroup-1.3.0 fief-client-0.20.0 filelock-3.19.1 flatbuffers-25.2.10 frozenlist-1.7.0 fsspec-2024.12.0 gliner-0.2.21 googleapis-common-protos-1.70.0 grpcio-1.74.0 h11-0.16.0 hf-xet-1.1.7 hqq-0.2.7.post1 httpcore-1.0.9 httpx-0.27.2 huggingface-hub-0.34.4 humanfriendly-10.0 idna-3.10 importlib_metadata-8.7.0 jinja2-3.1.6 joblib-1.5.1 jwcrypto-1.5.6 kernels-0.9.0 lazy_loader-0.4 librosa-0.11.0 lightning-utilities-0.15.2 llmcompressor-0.6.0.1 llvmlite-0.44.0 loguru-0.7.3 markdown-it-py-4.0.0 mdurl-0.1.2 more_itertools-10.7.0 mpmath-1.3.0 msgpack-1.1.1 multidict-6.6.4 multiprocess-0.70.16 networkx-3.4.2 ninja-1.13.0 numba-0.61.2 numpy-1.26.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-ml-py-12.575.51 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 onnxruntime-1.22.1 openai-whisper-20250625 opencv-python-4.11.0.86 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 openvino-2025.2.0 openvino-telemetry-2025.2.0 optimum-1.27.0 optimum-quanto-0.2.7 packaging-25.0 pandas-2.3.1 pillow-11.3.0 piq-0.8.0 platformdirs-4.3.8 pooch-1.8.2 prometheus_client-0.22.1 prompt_toolkit-3.0.51 propcache-0.3.2 protobuf-6.32.0 pruna-0.2.9 psutil-7.0.0 py-cpuinfo-9.0.0 pyarrow-21.0.0 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pygments-2.19.2 pynvml-12.0.0 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytorch-lightning-2.5.3 pytz-2025.2 pyyaml-6.0.2 questionary-2.1.0 rapidfuzz-3.13.0 regex-2025.7.34 requests-2.32.4 rich-14.1.0 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.15.3 sentencepiece-0.2.1 setuptools-80.9.0 shellingham-1.5.4 six-1.17.0 sniffio-1.3.1 soundfile-0.13.1 soxr-0.5.0.post1 sympy-1.14.0 termcolor-2.3.0 thop-0.1.1.post2209072238 threadpoolctl-3.6.0 tiktoken-0.11.0 timm-1.0.19 tokenizers-0.21.4 tomli-2.2.1 torch-2.7.0 torch-fidelity-0.3.0 torch-pruning-1.6.0 torchao-0.12.0 torchmetrics-1.7.4 torchvision-0.22.0 tqdm-4.67.1 transformers-4.52.4 triton-3.3.0 typer-0.16.0 types-python-dateutil-2.9.0.20250809 typing-extensions-4.14.1 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 wcwidth-0.2.13 whisper-s2t-1.3.1 xxhash-3.5.0 yarl-1.20.1 yaspin-3.1.0 zipp-3.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall pruna==0.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrunaAI/pruna.git@main\n",
      "  Cloning https://github.com/PrunaAI/pruna.git (to revision main) to /tmp/pip-req-build-q80ihaak\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PrunaAI/pruna.git /tmp/pip-req-build-q80ihaak\n",
      "  Resolved https://github.com/PrunaAI/pruna.git to commit 15876bb39ca33b0c93a5de844c8d23c1bd88a610\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aenum (from pruna==0.2.9)\n",
      "  Using cached aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bitsandbytes (from pruna==0.2.9)\n",
      "  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting codecarbon (from pruna==0.2.9)\n",
      "  Using cached codecarbon-3.0.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting colorama (from pruna==0.2.9)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting configspace>=1.2.1 (from pruna==0.2.9)\n",
      "  Using cached configspace-1.2.1-py3-none-any.whl\n",
      "Collecting ctranslate2==4.6.0 (from pruna==0.2.9)\n",
      "  Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets<=3.5.0 (from pruna==0.2.9)\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepcache (from pruna==0.2.9)\n",
      "  Using cached DeepCache-0.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting diffusers>=0.21.4 (from pruna==0.2.9)\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting gliner (from pruna==0.2.9)\n",
      "  Using cached gliner-0.2.21-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting hqq==0.2.7.post1 (from pruna==0.2.9)\n",
      "  Using cached hqq-0.2.7.post1-py3-none-any.whl\n",
      "Collecting huggingface-hub>=0.30.0 (from huggingface-hub[hf-xet]>=0.30.0->pruna==0.2.9)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting kernels (from pruna==0.2.9)\n",
      "  Using cached kernels-0.9.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting librosa (from pruna==0.2.9)\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting llmcompressor (from pruna==0.2.9)\n",
      "  Using cached llmcompressor-0.6.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna==0.2.9)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting opencv-python (from pruna==0.2.9)\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting opentelemetry-api>=1.30.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.29.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.30.0 (from pruna==0.2.9)\n",
      "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting openvino (from pruna==0.2.9)\n",
      "  Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting optimum (from pruna==0.2.9)\n",
      "  Using cached optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting optimum-quanto>=0.2.5 (from pruna==0.2.9)\n",
      "  Using cached optimum_quanto-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting piq (from pruna==0.2.9)\n",
      "  Using cached piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pynvml (from pruna==0.2.9)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pytorch-lightning (from pruna==0.2.9)\n",
      "  Using cached pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting requests>=2.31.0 (from pruna==0.2.9)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sentencepiece (from pruna==0.2.9)\n",
      "  Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting soundfile (from pruna==0.2.9)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting thop (from pruna==0.2.9)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting timm (from pruna==0.2.9)\n",
      "  Using cached timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting torch-pruning (from pruna==0.2.9)\n",
      "  Using cached torch_pruning-1.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting torch==2.7.0 (from pruna==0.2.9)\n",
      "  Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchao (from pruna==0.2.9)\n",
      "  Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting torchmetrics==1.7.4 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting torchvision==0.22.0 (from pruna==0.2.9)\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting transformers (from pruna==0.2.9)\n",
      "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting whisper-s2t==1.3.1 (from pruna==0.2.9)\n",
      "  Using cached whisper_s2t-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from ctranslate2==4.6.0->pruna==0.2.9)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pyyaml<7,>=5.3 (from ctranslate2==4.6.0->pruna==0.2.9)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting tqdm>=4.64.1 (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting einops (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting termcolor (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting packaging>17.1 (from torchmetrics==1.7.4->torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.7.4->torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting scipy>1.0.0 (from torchmetrics[image]==1.7.4->pruna==0.2.9)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.22.0->pruna==0.2.9)\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting rich (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting platformdirs (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting openai-whisper (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached openai_whisper-20250625-py3-none-any.whl\n",
      "Collecting nvidia-ml-py (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached nvidia_ml_py-13.580.65-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyparsing (from configspace>=1.2.1->pruna==0.2.9)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting more_itertools (from configspace>=1.2.1->pruna==0.2.9)\n",
      "  Using cached more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting importlib_metadata (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.30.0->huggingface-hub[hf-xet]>=0.30.0->pruna==0.2.9)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers>=0.21.4->pruna==0.2.9)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.36.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.36.0 (from opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf<7.0,>=5.0 (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp>=1.29.0->pruna==0.2.9)\n",
      "  Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.30.0->pruna==0.2.9)\n",
      "  Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.31.0->pruna==0.2.9)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ninja (from optimum-quanto>=0.2.5->pruna==0.2.9)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting psutil (from accelerate->hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting arrow (from codecarbon->pruna==0.2.9)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting click (from codecarbon->pruna==0.2.9)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting fief-client[cli] (from codecarbon->pruna==0.2.9)\n",
      "  Using cached fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting prometheus_client (from codecarbon->pruna==0.2.9)\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting py-cpuinfo (from codecarbon->pruna==0.2.9)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from codecarbon->pruna==0.2.9)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting rapidfuzz (from codecarbon->pruna==0.2.9)\n",
      "  Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting questionary (from codecarbon->pruna==0.2.9)\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting typer (from codecarbon->pruna==0.2.9)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting python-dateutil>=2.7.0 (from arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7.0->arrow->codecarbon->pruna==0.2.9)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting cryptography>=3.4 (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached cryptography-45.0.6-cp37-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pruna==0.2.9)\n",
      "  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting onnxruntime (from gliner->pruna==0.2.9)\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.7.0->pruna==0.2.9)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tomli>=2.0 (from kernels->pruna==0.2.9)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->pruna==0.2.9)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting decorator>=4.3.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pooch>=1.1 (from librosa->pruna==0.2.9)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->pruna==0.2.9)\n",
      "  Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->pruna==0.2.9)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->pruna==0.2.9)\n",
      "  Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->pruna==0.2.9)\n",
      "  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->pruna==0.2.9)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting loguru (from llmcompressor->pruna==0.2.9)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting numpy>=1.24.4 (from pruna==0.2.9)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting transformers (from pruna==0.2.9)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from llmcompressor->pruna==0.2.9)\n",
      "  Using cached compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->codecarbon->pruna==0.2.9)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner->pruna==0.2.9)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting tiktoken (from openai-whisper->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached tiktoken-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python (from pruna==0.2.9)\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino->pruna==0.2.9)\n",
      "  Using cached openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets<=3.5.0->pruna==0.2.9)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-ml-py (from whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting prompt_toolkit<4.0,>=2.0 (from questionary->codecarbon->pruna==0.2.9)\n",
      "  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon->pruna==0.2.9)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->whisper-s2t==1.3.1->pruna==0.2.9)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer->codecarbon->pruna==0.2.9)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting termcolor (from hqq==0.2.7.post1->pruna==0.2.9)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Using cached ctranslate2-4.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
      "Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "Using cached whisper_s2t-1.3.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached optimum_quanto-0.2.7-py3-none-any.whl (165 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Using cached aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "Using cached codecarbon-3.0.4-py3-none-any.whl (277 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl (17 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached DeepCache-0.1.1-py3-none-any.whl (190 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached cryptography-45.0.6-cp37-abi3-manylinux_2_34_x86_64.whl (4.4 MB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached gliner-0.2.21-py3-none-any.whl (65 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached kernels-0.9.0-py3-none-any.whl (37 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Using cached numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached llmcompressor-0.6.0.1-py3-none-any.whl (253 kB)\n",
      "Using cached compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "Using cached openvino-2025.2.0-19140-cp310-cp310-manylinux2014_x86_64.whl (47.6 MB)\n",
      "Using cached openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached optimum-1.27.0-py3-none-any.whl (425 kB)\n",
      "Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached piq-0.8.0-py3-none-any.whl (106 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
      "Using cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Using cached tiktoken-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "Using cached torch_pruning-1.6.0-py3-none-any.whl (68 kB)\n",
      "Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: pruna\n",
      "  Building wheel for pruna (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pruna: filename=pruna-0.2.9-py3-none-any.whl size=207600 sha256=71619c384ddd3cac6411aafd6d74a53d5fbd8466730dc1862e19e753caf0b9c4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ydegb28t/wheels/f1/13/10/be19eaf737dedb55f532bd149451175bb3d0e062e310ddfc47\n",
      "Successfully built pruna\n",
      "Installing collected packages: wcwidth, torchao, pytz, py-cpuinfo, openvino-telemetry, nvidia-ml-py, nvidia-cusparselt-cu12, mpmath, flatbuffers, aenum, zipp, xxhash, urllib3, tzdata, typing-extensions, types-python-dateutil, tqdm, tomli, threadpoolctl, termcolor, sympy, sniffio, six, shellingham, setuptools, sentencepiece, safetensors, regex, rapidfuzz, pyyaml, pyparsing, pynvml, pygments, pycparser, pyarrow, psutil, protobuf, propcache, prompt_toolkit, prometheus_client, platformdirs, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, msgpack, more_itertools, mdurl, MarkupSafe, loguru, llvmlite, joblib, idna, humanfriendly, hf-xet, h11, grpcio, fsspec, frozenlist, filelock, einops, dill, decorator, colorama, click, charset_normalizer, certifi, audioread, attrs, async-timeout, annotated-types, aiohappyeyeballs, yaspin, typing-inspection, triton, soxr, scipy, requests, questionary, python-dateutil, pydantic-core, openvino, opentelemetry-proto, opencv-python, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, multiprocess, multidict, markdown-it-py, lightning-utilities, lazy_loader, jinja2, importlib_metadata, httpcore, googleapis-common-protos, exceptiongroup, ctranslate2, coloredlogs, cffi, aiosignal, yarl, tiktoken, soundfile, scikit-learn, rich, pydantic, pooch, pandas, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, nvidia-cusolver-cu12, huggingface-hub, cryptography, configspace, arrow, anyio, typer, torch, tokenizers, opentelemetry-semantic-conventions, librosa, kernels, jwcrypto, httpx, diffusers, aiohttp, transformers, torchvision, torchmetrics, torch-pruning, thop, optimum-quanto, opentelemetry-sdk, openai-whisper, fief-client, bitsandbytes, accelerate, torch-fidelity, timm, pytorch-lightning, piq, optimum, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, hqq, gliner, deepcache, datasets, compressed-tensors, whisper-s2t, opentelemetry-exporter-otlp, llmcompressor, codecarbon, pruna\n",
      "\u001b[2K  Attempting uninstall: wcwidth\n",
      "\u001b[2K    Found existing installation: wcwidth 0.2.13\n",
      "\u001b[2K    Uninstalling wcwidth-0.2.13:\n",
      "\u001b[2K      Successfully uninstalled wcwidth-0.2.13\n",
      "\u001b[2K  Attempting uninstall: torchao\n",
      "\u001b[2K    Found existing installation: torchao 0.12.0\n",
      "\u001b[2K    Uninstalling torchao-0.12.0:\n",
      "\u001b[2K      Successfully uninstalled torchao-0.12.0\n",
      "\u001b[2K  Attempting uninstall: pytz\u001b[0m \u001b[32m  1/167\u001b[0m [torchao]\n",
      "\u001b[2K    Found existing installation: pytz 2025.2\u001b[0m \u001b[32m  1/167\u001b[0m [torchao]\n",
      "\u001b[2K    Uninstalling pytz-2025.2:\u001b[0m \u001b[32m  1/167\u001b[0m [torchao]\n",
      "\u001b[2K      Successfully uninstalled pytz-2025.2\u001b[0m \u001b[32m  1/167\u001b[0m [torchao]\n",
      "\u001b[2K  Attempting uninstall: py-cpuinfo\u001b[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: py-cpuinfo 9.0.0\u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling py-cpuinfo-9.0.0:\u001b[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled py-cpuinfo-9.0.0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: openvino-telemetry\u001b[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: openvino-telemetry 2025.2.07\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling openvino-telemetry-2025.2.0:[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled openvino-telemetry-2025.2.0167\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: nvidia-ml-py\u001b[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: nvidia-ml-py 12.575.51 2/167\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling nvidia-ml-py-12.575.51:\u001b[0m \u001b[32m  2/167\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled nvidia-ml-py-12.575.51\u001b[0m \u001b[32m  5/167\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\u001b[0m \u001b[32m  5/167\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.3\u001b[0m \u001b[32m  5/167\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:\u001b[0m \u001b[32m  5/167\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\u001b[0m \u001b[32m  5/167\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K  Attempting uninstall: mpmath\u001b[0m \u001b[32m  6/167\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0\u001b[0m \u001b[32m  6/167\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:\u001b[0m \u001b[32m  6/167\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0\u001b[0m \u001b[32m  6/167\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: flatbuffers\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]arselt-cu12]\n",
      "\u001b[2K    Found existing installation: flatbuffers 25.2.10\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling flatbuffers-25.2.10:\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled flatbuffers-25.2.10\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: aenum\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: aenum 3.1.16\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling aenum-3.1.16:\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled aenum-3.1.16\u001b[0m \u001b[32m  7/167\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: zippm\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Found existing installation: zipp 3.23.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling zipp-3.23.0:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled zipp-3.23.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K  Attempting uninstall: xxhash\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Found existing installation: xxhash 3.5.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling xxhash-3.5.0:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled xxhash-3.5.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K  Attempting uninstall: urllib3\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K  Attempting uninstall: tzdata\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Found existing installation: tzdata 2025.2\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K    Uninstalling tzdata-2025.2:\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K      Successfully uninstalled tzdata-2025.2\u001b[0m \u001b[32m  9/167\u001b[0m [aenum]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\u001b[0m \u001b[32m 13/167\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.1\u001b[0m \u001b[32m 13/167\u001b[0m [tzdata]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:\u001b[0m \u001b[32m 13/167\u001b[0m [tzdata]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: types-python-dateutil\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: types-python-dateutil 2.9.0.2025080932m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling types-python-dateutil-2.9.0.20250809:\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled types-python-dateutil-2.9.0.20250809\u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdm\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tomli\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tomli 2.2.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tomli-2.2.1:\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tomli-2.2.1\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: threadpoolctl\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:\u001b[0m \u001b[32m 14/167\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.0\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: termcolor\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: termcolor 2.3.0\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling termcolor-2.3.0:\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled termcolor-2.3.0\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: sympy\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0\u001b[0m \u001b[32m 18/167\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:m\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]ctl]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: sniffio\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: sniffio 1.3.1\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling sniffio-1.3.1:\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sniffio-1.3.1\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: six\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: six 1.17.0\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling six-1.17.0:\u001b[0m \u001b[32m 20/167\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: shellingham\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: shellingham 1.5.4\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling shellingham-1.5.4:\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled shellingham-1.5.4\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: setuptools\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:\u001b[0m \u001b[32m 22/167\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: sentencepiece\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: sentencepiece 0.2.1\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling sentencepiece-0.2.1:\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled sentencepiece-0.2.1\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: safetensors\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: safetensors 0.6.2\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling safetensors-0.6.2:\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.6.2\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: regex\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: regex 2025.7.34\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling regex-2025.7.34:\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled regex-2025.7.34\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: rapidfuzz\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: RapidFuzz 3.13.0\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling RapidFuzz-3.13.0:\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled RapidFuzz-3.13.0\u001b[0m \u001b[32m 24/167\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyyaml90m\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K  Attempting uninstall: pyparsing\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Found existing installation: pyparsing 3.2.3\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling pyparsing-3.2.3:\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K      Successfully uninstalled pyparsing-3.2.3\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K  Attempting uninstall: pynvml\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Found existing installation: pynvml 12.0.0\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling pynvml-12.0.0:\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K      Successfully uninstalled pynvml-12.0.0\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K  Attempting uninstall: pygments\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Found existing installation: Pygments 2.19.2\u001b[0m \u001b[32m 28/167\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling Pygments-2.19.2:m\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled Pygments-2.19.2\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: pycparserm\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K    Found existing installation: pycparser 2.22\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling pycparser-2.22:\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled pycparser-2.22\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: pyarrow\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K    Found existing installation: pyarrow 21.0.0\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling pyarrow-21.0.0:\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled pyarrow-21.0.0\u001b[0m \u001b[32m 32/167\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: psutil\u001b[90m\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: psutil 7.0.0\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling psutil-7.0.0:m\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled psutil-7.0.0\u001b[0m \u001b[32m 34/167\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: protobuf90m\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: protobuf 6.32.0\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling protobuf-6.32.0:\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.32.0\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: propcache\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: propcache 0.3.2\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling propcache-0.3.2:\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled propcache-0.3.2\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: prompt_toolkit\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: prompt_toolkit 3.0.51\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling prompt_toolkit-3.0.51:\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled prompt_toolkit-3.0.51\u001b[0m \u001b[32m 35/167\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: prometheus_client\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Found existing installation: prometheus_client 0.22.1\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Uninstalling prometheus_client-0.22.1:\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K      Successfully uninstalled prometheus_client-0.22.1\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K  Attempting uninstall: platformdirs\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Found existing installation: platformdirs 4.3.8\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Uninstalling platformdirs-4.3.8:\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K      Successfully uninstalled platformdirs-4.3.8\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K  Attempting uninstall: pillow0m\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Found existing installation: pillow 11.3.0\u001b[0m \u001b[32m 38/167\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K    Uninstalling pillow-11.3.0:\u001b[90m\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]kit]\n",
      "\u001b[2K      Successfully uninstalled pillow-11.3.0\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: packaging90m\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: packaging 25.0\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\u001b[0m \u001b[32m 41/167\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\u001b[0m \u001b[32m 44/167\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2\u001b[0m \u001b[32m 44/167\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:\u001b[0m \u001b[32m 44/167\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2\u001b[0m \u001b[32m 44/167\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\u001b[0m \u001b[32m 45/167\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77\u001b[0m \u001b[32m 45/167\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\u001b[0m \u001b[32m 45/167\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\u001b[0m \u001b[32m 45/167\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.11.1.6\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.11.1.6:\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\u001b[0m \u001b[32m 46/167\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\u001b[0m \u001b[32m 49/167\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80[0m \u001b[32m 49/167\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\u001b[0m \u001b[32m 49/167\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\u001b[0m \u001b[32m 49/167\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[0m \u001b[32m 50/167\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1\u001b[0m \u001b[32m 50/167\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\u001b[0m \u001b[32m 50/167\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\u001b[0m \u001b[32m 50/167\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[0m\u001b[90m\u001b[0m \u001b[32m 51/167\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\u001b[0m \u001b[32m 51/167\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: ninja\u001b[0m\u001b[90m\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: ninja 1.13.0\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling ninja-1.13.0:\u001b[90m\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled ninja-1.13.0\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: networkx90m\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: networkx 3.4.2\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.4.2:90m\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.4.2\u001b[0m \u001b[32m 52/167\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: msgpack[0m\u001b[90m\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: msgpack 1.1.1\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling msgpack-1.1.1:[90m\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled msgpack-1.1.1\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: more_itertools\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: more-itertools 10.7.0\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling more-itertools-10.7.0:\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled more-itertools-10.7.0\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: mdurlm\u001b[90m\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: mdurl 0.1.2\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling mdurl-0.1.2:m\u001b[90m\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled mdurl-0.1.2\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafem\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:m\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2\u001b[0m \u001b[32m 54/167\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: loguru\u001b[0m\u001b[90m\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: loguru 0.7.3\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling loguru-0.7.3:m\u001b[90m\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled loguru-0.7.3\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: llvmlite[90m\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: llvmlite 0.44.0\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling llvmlite-0.44.0:90m\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled llvmlite-0.44.0\u001b[0m \u001b[32m 58/167\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: joblibm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.1\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K    Uninstalling joblib-1.5.1:0m\u001b[90m\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.1\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K  Attempting uninstall: idna\u001b[0m\u001b[90m\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K    Found existing installation: idna 3.10\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K    Uninstalling idna-3.10:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10\u001b[0m \u001b[32m 60/167\u001b[0m [llvmlite]\n",
      "\u001b[2K  Attempting uninstall: humanfriendly[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: humanfriendly 10.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling humanfriendly-10.0:m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled humanfriendly-10.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: hf-xet0m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.7\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.7:0m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.7\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: h11\u001b[0m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: h11 0.16.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling h11-0.16.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled h11-0.16.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: grpcio0m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: grpcio 1.74.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling grpcio-1.74.0:m\u001b[90m\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.74.0\u001b[0m \u001b[32m 62/167\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 66/167\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: fsspec 2024.12.0\u001b[0m \u001b[32m 66/167\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling fsspec-2024.12.0:[90m\u001b[0m \u001b[32m 66/167\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2024.12.0\u001b[0m \u001b[32m 66/167\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: frozenlist\u001b[0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: frozenlist 1.7.0\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling frozenlist-1.7.0:\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled frozenlist-1.7.0\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: filelock0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.19.1\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.19.1:m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.19.1\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: einops\u001b[0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: einops 0.8.1\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling einops-0.8.1:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled einops-0.8.1\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: dillm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.3.8\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.3.8:m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.8\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: decoratorm\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: decorator 5.2.1\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling decorator-5.2.1:m\u001b[90m\u001b[0m \u001b[32m 67/167\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled decorator-5.2.1\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K  Attempting uninstall: colorama[0m\u001b[90m\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K    Found existing installation: colorama 0.4.6\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K    Uninstalling colorama-0.4.6:[0m\u001b[90m\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K      Successfully uninstalled colorama-0.4.6\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K  Attempting uninstall: clickm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 72/167\u001b[0m [decorator]\n",
      "\u001b[2K    Found existing installation: click 8.2.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling click-8.2.1:m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled click-8.2.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: certifi\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.3\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: audioread0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: audioread 3.0.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling audioread-3.0.1:0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled audioread-3.0.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: attrsm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: attrs 25.3.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling attrs-25.3.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled attrs-25.3.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: async-timeout90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: async-timeout 5.0.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling async-timeout-5.0.1:90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled async-timeout-5.0.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: annotated-typesm\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: annotated-types 0.7.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling annotated-types-0.7.0:m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled annotated-types-0.7.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: aiohappyeyeballs\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: aiohappyeyeballs 2.6.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling aiohappyeyeballs-2.6.1:\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled aiohappyeyeballs-2.6.1\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: yaspin\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Found existing installation: yaspin 3.1.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K    Uninstalling yaspin-3.1.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K      Successfully uninstalled yaspin-3.1.0\u001b[0m \u001b[32m 74/167\u001b[0m [click]\n",
      "\u001b[2K  Attempting uninstall: typing-inspection\u001b[90m\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K    Found existing installation: typing-inspection 0.4.1\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K    Uninstalling typing-inspection-0.4.1:m\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K      Successfully uninstalled typing-inspection-0.4.1\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K  Attempting uninstall: triton1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K    Found existing installation: triton 3.3.0\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K    Uninstalling triton-3.3.0:1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.0\u001b[0m \u001b[32m 82/167\u001b[0m [yaspin]\n",
      "\u001b[2K  Attempting uninstall: soxr\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: soxr 0.5.0.post1\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling soxr-0.5.0.post1:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled soxr-0.5.0.post1\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: scipy[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.3\u001b[0m \u001b[32m 84/167\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: requests\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.4\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: questionary[0m\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: questionary 2.1.0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling questionary-2.1.0:[0m\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled questionary-2.1.0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: pydantic-corem\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.33.2\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.33.2:\u001b[90m\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.33.2\u001b[0m \u001b[32m 86/167\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: openvinom\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 90/167\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Found existing installation: openvino 2025.2.0\u001b[0m \u001b[32m 90/167\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Uninstalling openvino-2025.2.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 90/167\u001b[0m [pydantic-core]\n",
      "\u001b[2K      Successfully uninstalled openvino-2025.2.0\u001b[0m \u001b[32m 90/167\u001b[0m [pydantic-core]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-proto\u001b[90m\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]]\n",
      "\u001b[2K    Found existing installation: opentelemetry-proto 1.36.0\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]\n",
      "\u001b[2K    Uninstalling opentelemetry-proto-1.36.0:\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-proto-1.36.0\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]\n",
      "\u001b[2K  Attempting uninstall: opencv-python0m\u001b[90m\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]\n",
      "\u001b[2K    Found existing installation: opencv-python 4.11.0.86\u001b[0m \u001b[32m 91/167\u001b[0m [openvino]\n",
      "\u001b[2K    Uninstalling opencv-python-4.11.0.86:[0m\u001b[90m\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K      Successfully uninstalled opencv-python-4.11.0.86\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\u001b[90m\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\u001b[0m \u001b[32m 93/167\u001b[0m [opencv-python]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12[0m\u001b[90m\u001b[0m \u001b[32m 94/167\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4\u001b[0m \u001b[32m 94/167\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:m\u001b[0m \u001b[32m 94/167\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\u001b[0m \u001b[32m 94/167\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[0m\u001b[90m\u001b[0m \u001b[32m 95/167\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\u001b[0m \u001b[32m 95/167\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:m\u001b[0m \u001b[32m 95/167\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\u001b[0m \u001b[32m 95/167\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: numba\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 96/167\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: numba 0.61.2\u001b[0m \u001b[32m 96/167\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling numba-0.61.2:\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 96/167\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled numba-0.61.20m\u001b[0m \u001b[32m 96/167\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: multiprocess[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 97/167\u001b[0m [numba]nn-cu12]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.16\u001b[0m \u001b[32m 97/167\u001b[0m [numba]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.16:[0m\u001b[90m\u001b[0m \u001b[32m 97/167\u001b[0m [numba]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.16\u001b[0m \u001b[32m 97/167\u001b[0m [numba]\n",
      "\u001b[2K  Attempting uninstall: multidict0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: multidict 6.6.4\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling multidict-6.6.4:90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled multidict-6.6.4\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: markdown-it-py[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: markdown-it-py 4.0.0\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling markdown-it-py-4.0.0:[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled markdown-it-py-4.0.0\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: lightning-utilities90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: lightning-utilities 0.15.2\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling lightning-utilities-0.15.2:0m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled lightning-utilities-0.15.2\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: lazy_loaderm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: lazy_loader 0.4\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling lazy_loader-0.4:90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled lazy_loader-0.4\u001b[0m \u001b[32m 98/167\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: jinja2\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.60m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.6[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K  Attempting uninstall: importlib_metadata\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K  Attempting uninstall: httpcore\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Found existing installation: httpcore 1.0.9\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K    Uninstalling httpcore-1.0.9:\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K      Successfully uninstalled httpcore-1.0.90m\u001b[0m \u001b[32m102/167\u001b[0m [lazy_loader]\n",
      "\u001b[2K  Attempting uninstall: googleapis-common-protos[90m\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: googleapis-common-protos 1.70.00m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling googleapis-common-protos-1.70.0:\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled googleapis-common-protos-1.70.0\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: exceptiongroup\u001b[0m\u001b[90m\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: exceptiongroup 1.3.0\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling exceptiongroup-1.3.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled exceptiongroup-1.3.0\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: ctranslate290m\u001b[0m\u001b[90m\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Found existing installation: ctranslate2 4.6.0\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K    Uninstalling ctranslate2-4.6.0:90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K      Successfully uninstalled ctranslate2-4.6.0\u001b[0m \u001b[32m105/167\u001b[0m [httpcore]\n",
      "\u001b[2K  Attempting uninstall: coloredlogs0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m108/167\u001b[0m [ctranslate2]\n",
      "\u001b[2K    Found existing installation: coloredlogs 15.0.1\u001b[0m \u001b[32m108/167\u001b[0m [ctranslate2]\n",
      "\u001b[2K    Uninstalling coloredlogs-15.0.1:1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m108/167\u001b[0m [ctranslate2]\n",
      "\u001b[2K      Successfully uninstalled coloredlogs-15.0.1[90m\u001b[0m \u001b[32m109/167\u001b[0m [coloredlogs]\n",
      "\u001b[2K  Attempting uninstall: cffi\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m109/167\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Found existing installation: cffi 1.17.1\u001b[90m\u001b[0m \u001b[32m109/167\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Uninstalling cffi-1.17.1:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m109/167\u001b[0m [coloredlogs]\n",
      "\u001b[2K      Successfully uninstalled cffi-1.17.1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]gs]\n",
      "\u001b[2K  Attempting uninstall: aiosignalm\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K    Found existing installation: aiosignal 1.4.0m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K    Uninstalling aiosignal-1.4.0:m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K      Successfully uninstalled aiosignal-1.4.090m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K  Attempting uninstall: yarl\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K    Found existing installation: yarl 1.20.1\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K    Uninstalling yarl-1.20.1:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K      Successfully uninstalled yarl-1.20.10m\u001b[90m\u001b[0m \u001b[32m110/167\u001b[0m [cffi]\n",
      "\u001b[2K  Attempting uninstall: tiktoken\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K    Found existing installation: tiktoken 0.11.0m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K    Uninstalling tiktoken-0.11.0:m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K      Successfully uninstalled tiktoken-0.11.090m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K  Attempting uninstall: soundfilem\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K    Found existing installation: soundfile 0.13.1\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K    Uninstalling soundfile-0.13.1:\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m112/167\u001b[0m [yarl]\n",
      "\u001b[2K      Successfully uninstalled soundfile-0.13.10m\u001b[90m\u001b[0m \u001b[32m114/167\u001b[0m [soundfile]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m114/167\u001b[0m [soundfile]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.1\u001b[0m \u001b[32m114/167\u001b[0m [soundfile]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.1:[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m114/167\u001b[0m [soundfile]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.1m\u001b[0m \u001b[32m114/167\u001b[0m [soundfile]\n",
      "\u001b[2K  Attempting uninstall: rich\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: rich 14.1.0m\u001b[90m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling rich-14.1.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled rich-14.1.0[0m\u001b[90m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: pydantic[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: pydantic 2.11.70m\u001b[0m \u001b[32m115/167\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling pydantic-2.11.7:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.11.7\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K  Attempting uninstall: pooch\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K    Found existing installation: pooch 1.8.20m\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling pooch-1.8.2:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled pooch-1.8.2\u001b[0m\u001b[90m\u001b[0m \u001b[32m117/167\u001b[0m [pydantic]\n",
      "\u001b[2K  Attempting uninstall: pandas\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m118/167\u001b[0m [pooch]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.1m\u001b[90m\u001b[0m \u001b[32m118/167\u001b[0m [pooch]\n",
      "\u001b[2K    Uninstalling pandas-2.3.1:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m118/167\u001b[0m [pooch]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.3.1[0m\u001b[90m\u001b[0m \u001b[32m118/167\u001b[0m [pooch]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.36.0[0m [pandas]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-proto-common-1.36.0:m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.36.07\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-api\u001b[0m\u001b[90m\u001b[0m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.36.0\u001b[0m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.36.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.36.0\u001b[0m \u001b[32m119/167\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: onnxruntime\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m121/167\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Found existing installation: onnxruntime 1.22.1\u001b[0m \u001b[32m121/167\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K    Uninstalling onnxruntime-1.22.1:\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m121/167\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K      Successfully uninstalled onnxruntime-1.22.10m\u001b[0m \u001b[32m121/167\u001b[0m [opentelemetry-api]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu120m\u001b[0m\u001b[90m\u001b[0m \u001b[32m122/167\u001b[0m [onnxruntime]i]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\u001b[0m \u001b[32m122/167\u001b[0m [onnxruntime]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\u001b[90m\u001b[0m \u001b[32m122/167\u001b[0m [onnxruntime]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\u001b[0m \u001b[32m122/167\u001b[0m [onnxruntime]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m123/167\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.4\u001b[0m \u001b[32m123/167\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.4:0m\u001b[0m\u001b[90m\u001b[0m \u001b[32m123/167\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.4\u001b[0m \u001b[32m123/167\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: cryptography\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m124/167\u001b[0m [huggingface-hub]]\n",
      "\u001b[2K    Found existing installation: cryptography 45.0.6\u001b[0m \u001b[32m124/167\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling cryptography-45.0.6:\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m124/167\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled cryptography-45.0.60m\u001b[0m \u001b[32m124/167\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: configspace\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: ConfigSpace 1.2.10m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling ConfigSpace-1.2.1:0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled ConfigSpace-1.2.1[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: arrow\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: arrow 1.3.0[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling arrow-1.3.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled arrow-1.3.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: anyio\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: anyio 4.10.00m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling anyio-4.10.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled anyio-4.10.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: typer\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: typer 0.16.00m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling typer-0.16.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled typer-0.16.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m125/167\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: torch\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m129/167\u001b[0m [typer]hy]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m129/167\u001b[0m [typer]\n",
      "\u001b[2K    Uninstalling torch-2.7.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.00m\u001b[0m\u001b[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: tokenizers\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.4[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.4:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.4m\u001b[90m\u001b[0m \u001b[32m130/167\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.57b01/167\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.57b0:\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K  Attempting uninstall: librosa\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: librosa 0.11.00m\u001b[90m\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling librosa-0.11.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled librosa-0.11.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m131/167\u001b[0m [tokenizers]\n",
      "\u001b[2K  Attempting uninstall: kernels\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Found existing installation: kernels 0.9.0[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Uninstalling kernels-0.9.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K      Successfully uninstalled kernels-0.9.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K  Attempting uninstall: jwcrypto\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Found existing installation: jwcrypto 1.5.60m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Uninstalling jwcrypto-1.5.6:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K      Successfully uninstalled jwcrypto-1.5.6\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K  Attempting uninstall: httpx\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Found existing installation: httpx 0.27.2\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Uninstalling httpx-0.27.2:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K      Successfully uninstalled httpx-0.27.2m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K  Attempting uninstall: diffusers\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Found existing installation: diffusers 0.34.0\u001b[90m\u001b[0m \u001b[32m133/167\u001b[0m [librosa]\n",
      "\u001b[2K    Uninstalling diffusers-0.34.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K      Successfully uninstalled diffusers-0.34.0[0m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K  Attempting uninstall: aiohttp\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K    Found existing installation: aiohttp 3.12.150m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K    Uninstalling aiohttp-3.12.15:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K      Successfully uninstalled aiohttp-3.12.15\u001b[0m\u001b[90m\u001b[0m \u001b[32m137/167\u001b[0m [diffusers]\n",
      "\u001b[2K  Attempting uninstall: transformers\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m138/167\u001b[0m [aiohttp]\n",
      "\u001b[2K    Found existing installation: transformers 4.52.4[90m\u001b[0m \u001b[32m138/167\u001b[0m [aiohttp]\n",
      "\u001b[2K    Uninstalling transformers-4.52.4:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.52.4m\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torchvision\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: torchvision 0.22.0\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling torchvision-0.22.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.22.00m\u001b[90m\u001b[0m \u001b[32m139/167\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torchmetrics\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m140/167\u001b[0m [torchvision]\n",
      "\u001b[2K    Found existing installation: torchmetrics 1.7.4\u001b[90m\u001b[0m \u001b[32m140/167\u001b[0m [torchvision]\n",
      "\u001b[2K    Uninstalling torchmetrics-1.7.4:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m140/167\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchmetrics-1.7.40m\u001b[90m\u001b[0m \u001b[32m140/167\u001b[0m [torchvision]\n",
      "\u001b[2K  Attempting uninstall: torch-pruning\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Found existing installation: torch-pruning 1.6.0[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Uninstalling torch-pruning-1.6.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K      Successfully uninstalled torch-pruning-1.6.0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K  Attempting uninstall: thop\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Found existing installation: thop 0.1.1.post2209072238\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Uninstalling thop-0.1.1.post2209072238:91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K      Successfully uninstalled thop-0.1.1.post2209072238\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K  Attempting uninstall: optimum-quanto[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Found existing installation: optimum-quanto 0.2.790m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Uninstalling optimum-quanto-0.2.7:[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K      Successfully uninstalled optimum-quanto-0.2.7\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdk\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.36.0\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.36.0:[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.36.0m\u001b[0m \u001b[32m141/167\u001b[0m [torchmetrics]\n",
      "\u001b[2K  Attempting uninstall: openai-whisper\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: openai-whisper 20250625m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling openai-whisper-20250625:m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled openai-whisper-2025062590m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K  Attempting uninstall: fief-client\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: fief-client 0.20.0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling fief-client-0.20.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled fief-client-0.20.0[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K  Attempting uninstall: bitsandbytes\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Found existing installation: bitsandbytes 0.47.0\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K    Uninstalling bitsandbytes-0.47.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K      Successfully uninstalled bitsandbytes-0.47.00m\u001b[90m\u001b[0m \u001b[32m145/167\u001b[0m [opentelemetry-sdk]\n",
      "\u001b[2K  Attempting uninstall: accelerate\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m148/167\u001b[0m [bitsandbytes]]\n",
      "\u001b[2K    Found existing installation: accelerate 1.10.0[0m\u001b[90m\u001b[0m \u001b[32m148/167\u001b[0m [bitsandbytes]\n",
      "\u001b[2K    Uninstalling accelerate-1.10.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m148/167\u001b[0m [bitsandbytes]\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.10.0\u001b[0m\u001b[90m\u001b[0m \u001b[32m148/167\u001b[0m [bitsandbytes]\n",
      "\u001b[2K  Attempting uninstall: torch-fidelity\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K    Found existing installation: torch-fidelity 0.3.0\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K    Uninstalling torch-fidelity-0.3.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K      Successfully uninstalled torch-fidelity-0.3.00m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K  Attempting uninstall: timm\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K    Found existing installation: timm 1.0.19[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K    Uninstalling timm-1.0.19:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K      Successfully uninstalled timm-1.0.19m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m149/167\u001b[0m [accelerate]\n",
      "\u001b[2K  Attempting uninstall: pytorch-lightning\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m151/167\u001b[0m [timm]e]\n",
      "\u001b[2K    Found existing installation: pytorch-lightning 2.5.390m\u001b[0m \u001b[32m151/167\u001b[0m [timm]\n",
      "\u001b[2K    Uninstalling pytorch-lightning-2.5.3:[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m151/167\u001b[0m [timm]\n",
      "\u001b[2K      Successfully uninstalled pytorch-lightning-2.5.3\u001b[90m\u001b[0m \u001b[32m151/167\u001b[0m [timm]\n",
      "\u001b[2K  Attempting uninstall: piq\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K    Found existing installation: piq 0.8.00m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K    Uninstalling piq-0.8.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K      Successfully uninstalled piq-0.8.0\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K  Attempting uninstall: optimum\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K    Found existing installation: optimum 1.27.00m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K    Uninstalling optimum-1.27.0:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K      Successfully uninstalled optimum-1.27.0[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m152/167\u001b[0m [pytorch-lightning]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp-proto-httpm\u001b[0m \u001b[32m154/167\u001b[0m [optimum]tning]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.36.07\u001b[0m [optimum]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-proto-http-1.36.0:[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.36.0167\u001b[0m [optimum]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.36.07\u001b[0m [optimum]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.36.0:[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.36.0167\u001b[0m [optimum]\n",
      "\u001b[2K  Attempting uninstall: hqq\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K    Found existing installation: hqq 0.2.7.post1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K    Uninstalling hqq-0.2.7.post1:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K      Successfully uninstalled hqq-0.2.7.post191m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K  Attempting uninstall: gliner\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K    Found existing installation: gliner 0.2.2191m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K    Uninstalling gliner-0.2.21:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K      Successfully uninstalled gliner-0.2.21\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m154/167\u001b[0m [optimum]\n",
      "\u001b[2K  Attempting uninstall: deepcache\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K    Found existing installation: DeepCache 0.1.11m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K    Uninstalling DeepCache-0.1.1:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K      Successfully uninstalled DeepCache-0.1.1[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K  Attempting uninstall: datasets\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K    Found existing installation: datasets 3.5.091m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K    Uninstalling datasets-3.5.0:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K      Successfully uninstalled datasets-3.5.0\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m158/167\u001b[0m [gliner]\n",
      "\u001b[2K  Attempting uninstall: compressed-tensors\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K    Found existing installation: compressed-tensors 0.10.290m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K    Uninstalling compressed-tensors-0.10.2:[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K      Successfully uninstalled compressed-tensors-0.10.2\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K  Attempting uninstall: whisper-s2t\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K    Found existing installation: whisper-s2t 1.3.1m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K    Uninstalling whisper-s2t-1.3.1:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K      Successfully uninstalled whisper-s2t-1.3.190m\u001b[0m\u001b[90m\u001b[0m \u001b[32m160/167\u001b[0m [datasets]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-exporter-otlp[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K    Found existing installation: opentelemetry-exporter-otlp 1.36.0\u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K    Uninstalling opentelemetry-exporter-otlp-1.36.0:\u001b[0m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-exporter-otlp-1.36.0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K  Attempting uninstall: llmcompressor\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K    Found existing installation: llmcompressor 0.6.0.10m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K    Uninstalling llmcompressor-0.6.0.1:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K      Successfully uninstalled llmcompressor-0.6.0.1\u001b[0m\u001b[90m\u001b[0m \u001b[32m162/167\u001b[0m [whisper-s2t]\n",
      "\u001b[2K  Attempting uninstall: codecarbon\u001b[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]2t]\n",
      "\u001b[2K    Found existing installation: codecarbon 3.0.490m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K    Uninstalling codecarbon-3.0.4:\u001b[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K      Successfully uninstalled codecarbon-3.0.4\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K  Attempting uninstall: pruna\u001b[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K    Found existing installation: pruna 0.2.9[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K    Uninstalling pruna-0.2.9:\u001b[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K      Successfully uninstalled pruna-0.2.9\u001b[0m\u001b[90m\u001b[0m \u001b[32m164/167\u001b[0m [llmcompressor]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m167/167\u001b[0m [pruna]m [pruna]sor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.10.0 aenum-3.1.16 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 arrow-1.3.0 async-timeout-5.0.1 attrs-25.3.0 audioread-3.0.1 bitsandbytes-0.47.0 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 click-8.2.1 codecarbon-3.0.4 colorama-0.4.6 coloredlogs-15.0.1 compressed-tensors-0.10.2 configspace-1.2.1 cryptography-45.0.6 ctranslate2-4.6.0 datasets-3.5.0 decorator-5.2.1 deepcache-0.1.1 diffusers-0.34.0 dill-0.3.8 einops-0.8.1 exceptiongroup-1.3.0 fief-client-0.20.0 filelock-3.19.1 flatbuffers-25.2.10 frozenlist-1.7.0 fsspec-2024.12.0 gliner-0.2.21 googleapis-common-protos-1.70.0 grpcio-1.74.0 h11-0.16.0 hf-xet-1.1.7 hqq-0.2.7.post1 httpcore-1.0.9 httpx-0.27.2 huggingface-hub-0.34.4 humanfriendly-10.0 idna-3.10 importlib_metadata-8.7.0 jinja2-3.1.6 joblib-1.5.1 jwcrypto-1.5.6 kernels-0.9.0 lazy_loader-0.4 librosa-0.11.0 lightning-utilities-0.15.2 llmcompressor-0.6.0.1 llvmlite-0.44.0 loguru-0.7.3 markdown-it-py-4.0.0 mdurl-0.1.2 more_itertools-10.7.0 mpmath-1.3.0 msgpack-1.1.1 multidict-6.6.4 multiprocess-0.70.16 networkx-3.4.2 ninja-1.13.0 numba-0.61.2 numpy-1.26.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-ml-py-12.575.51 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 onnxruntime-1.22.1 openai-whisper-20250625 opencv-python-4.11.0.86 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 openvino-2025.2.0 openvino-telemetry-2025.2.0 optimum-1.27.0 optimum-quanto-0.2.7 packaging-25.0 pandas-2.3.1 pillow-11.3.0 piq-0.8.0 platformdirs-4.3.8 pooch-1.8.2 prometheus_client-0.22.1 prompt_toolkit-3.0.51 propcache-0.3.2 protobuf-6.32.0 pruna-0.2.9 psutil-7.0.0 py-cpuinfo-9.0.0 pyarrow-21.0.0 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pygments-2.19.2 pynvml-12.0.0 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytorch-lightning-2.5.3 pytz-2025.2 pyyaml-6.0.2 questionary-2.1.0 rapidfuzz-3.13.0 regex-2025.7.34 requests-2.32.4 rich-14.1.0 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.15.3 sentencepiece-0.2.1 setuptools-80.9.0 shellingham-1.5.4 six-1.17.0 sniffio-1.3.1 soundfile-0.13.1 soxr-0.5.0.post1 sympy-1.14.0 termcolor-2.3.0 thop-0.1.1.post2209072238 threadpoolctl-3.6.0 tiktoken-0.11.0 timm-1.0.19 tokenizers-0.21.4 tomli-2.2.1 torch-2.7.0 torch-fidelity-0.3.0 torch-pruning-1.6.0 torchao-0.12.0 torchmetrics-1.7.4 torchvision-0.22.0 tqdm-4.67.1 transformers-4.52.4 triton-3.3.0 typer-0.16.0 types-python-dateutil-2.9.0.20250809 typing-extensions-4.14.1 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 wcwidth-0.2.13 whisper-s2t-1.3.1 xxhash-3.5.0 yarl-1.20.1 yaspin-3.1.0 zipp-3.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall git+https://github.com/PrunaAI/pruna.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hqq in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (0.2.7.post1)\n",
      "Collecting hqq\n",
      "  Downloading hqq-0.2.8.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (4.67.1)\n",
      "Requirement already satisfied: einops in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (0.8.1)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.36.1 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (4.52.4)\n",
      "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (0.34.4)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from hqq) (2.3.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (2025.7.34)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from transformers>=4.36.1->hqq) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from huggingface_hub->hqq) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from huggingface_hub->hqq) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from huggingface_hub->hqq) (1.1.7)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from accelerate->hqq) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from accelerate->hqq) (2.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from torch>=2.0.0->accelerate->hqq) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate->hqq) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate->hqq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate->hqq) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from requests->transformers>=4.36.1->hqq) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from requests->transformers>=4.36.1->hqq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from requests->transformers>=4.36.1->hqq) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages (from requests->transformers>=4.36.1->hqq) (2025.8.3)\n",
      "Building wheels for collected packages: hqq\n",
      "  DEPRECATION: Building 'hqq' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'hqq'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  Building wheel for hqq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hqq: filename=hqq-0.2.8-py3-none-any.whl size=68424 sha256=4a3b4a33523554539d1f0b74a2cd37617d1bfa3ab44885d8f9910e47c4193553\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/27/5c/30e8d87478cecd6b28dca83bd2d3e27724b55f565fdba980d9\n",
      "Successfully built hqq\n",
      "Installing collected packages: hqq\n",
      "  Attempting uninstall: hqq\n",
      "    Found existing installation: hqq 0.2.7.post1\n",
      "    Uninstalling hqq-0.2.7.post1:\n",
      "      Successfully uninstalled hqq-0.2.7.post1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pruna 0.2.9 requires hqq==0.2.7.post1, but you have hqq 0.2.8 which is incompatible.\n",
      "Successfully installed hqq-0.2.8\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U hqq\n",
    "# (and transformers/accelerate if your stack needs them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.4\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /root/miniconda3/envs/pruna-tutorials/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: compressed-tensors, DeepCache, gliner, hqq, llmcompressor, optimum, pruna, whisper-s2t\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how to install Pruna, please refer to the [Installation](https://docs.pruna.ai/en/stable/setup/install.html) page.\n",
    "\n",
    "Then, we will set the device to the best available option to maximize the optimization process's benefits. However, in this case, we recommend using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we will load the original model and tokenizer using the transformers library. In our case, we will use one of the small versions of Qwen3, [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) just as a starting point. However, Pruna works at least as well with larger models, so feel free to use a bigger version of Qwen3 or any other [reasoning model available on Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c371a9a42de841f5b6a0dfcd5b0c3b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've loaded the model and tokenizer, we can try to generate a response from the model and parse the response to get the reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Give me a short introduction to large language model.\",\n",
    "    },\n",
    "]\n",
    "messages = pipe(messages, max_new_tokens=32768)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Give me a short introduction to large language model.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Large language models (LLMs) are AI systems designed to understand, generate, and interact with human language. They are trained on massive datasets of text, enabling them to grasp complex patterns and produce coherent, context-aware responses. These models, often based on transformer architecture, excel in tasks like translation, writing, and answering questions. While they offer remarkable capabilities, they also face challenges such as data bias and the need for continuous refinement. LLMs are revolutionizing industries by enhancing productivity and innovation in areas like customer service, content creation, and research.',\n",
       "  'reasoning_content': 'Okay, the user wants a short introduction to large language models. Let me start by defining what they are. Large language models (LLMs) are AI systems trained on vast amounts of text data. I should mention their key features like natural language understanding and generation.\\n\\nWait, I need to make sure it\\'s concise. Maybe start with a definition, then talk about their training, the components like transformer architecture, and their applications. Also, mention that they\\'re used in various fields like customer service, content creation, etc. Oh, and maybe touch on their limitations, like data bias or lack of real-world knowledge. But since it\\'s a short intro, maybe keep it positive and highlight the benefits first.\\n\\nLet me check if I\\'m missing anything. The user might be a student or someone new to AI. They need a clear, straightforward explanation without too much jargon. Make sure to explain key terms like \"training data\" and \"transformer architecture\" in simple terms. Avoid technical details that might confuse them. Alright, structure it as a brief overview with key points.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_thinking_content(messages):  # noqa: D103\n",
    "    messages = copy.deepcopy(messages)\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\" and (\n",
    "            m := re.match(\n",
    "                r\"<think>\\n(.+)</think>\\n\\n\", message[\"content\"], flags=re.DOTALL\n",
    "            )\n",
    "        ):\n",
    "            message[\"content\"] = message[\"content\"][len(m.group(0)) :]\n",
    "            if thinking_content := m.group(1).strip():\n",
    "                message[\"reasoning_content\"] = thinking_content\n",
    "    return messages\n",
    "\n",
    "\n",
    "parse_thinking_content(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the SmashConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our base model is lodaded and tested, we can specify the `SmashConfig` to customize the optimizations applied during smashing.\n",
    "\n",
    "Not every optimization algorithm works with every model. You can learn about the requirements and compatibility in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html).\n",
    "\n",
    "In this example, we will enable `hqq` quantization to improve the performance of the model and `torch_compile` compilation to improve the speed of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n",
      "INFO - Using best available device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig(cache_dir_prefix=\"/scratch/.cache\")\n",
    "smash_config[\"quantizer\"] = \"hqq\"\n",
    "smash_config[\"hqq_weight_bits\"] = 8\n",
    "smash_config[\"hqq_compute_dtype\"] = \"torch.bfloat16\"\n",
    "\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "smash_config[\"torch_compile_fullgraph\"] = True\n",
    "smash_config[\"torch_compile_dynamic\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smash the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `SmashConfig` defined, its time to apply it to our base model. Well call the `smash` function with the base model and our `SmashConfig`\n",
    "\n",
    "Ready to smash? This operation typically takes around 20 seconds, depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting quantizer hqq...\n",
      "100%|| 143/143 [00:00<00:00, 937.14it/s]\n",
      "100%|| 197/197 [00:03<00:00, 55.35it/s]\n",
      "INFO - quantizer hqq was applied successfully.\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n"
     ]
    }
   ],
   "source": [
    "from pruna import smash\n",
    "import copy\n",
    "\n",
    "copy_model = copy.deepcopy(pipe.model).to(\"cpu\")\n",
    "smashed_model = smash(\n",
    "    model=pipe.model,\n",
    "    smash_config=smash_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have our optimized smashed model. Let's check how it works by running some inference.\n",
    "\n",
    "Consider that if you are using `torch_compile` as a compiler, you can expect the first inference warmup to take a bit longer than the actual inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Give me a short introduction to large language models.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Large language models (LLMs) are advanced AI systems designed to understand and generate human-like text. They learn from vast amounts of data using deep learning techniques, enabling them to produce coherent and contextually relevant responses. These models excel in tasks like language translation, content creation, and customer service chatbots. While they're powerful, they're not infallible and rely on data quality. Their integration into daily life has transformed how we interact with technology, making tasks faster and more efficient.\",\n",
       "  'reasoning_content': \"Okay, the user wants a short introduction to large language models. Let me start by defining what they are. Large language models are AI systems that can understand and generate human-like text. I should mention their training with vast amounts of data and their use in various applications like chatbots and content creation.\\n\\nWait, maybe I should break it down into key points: what they are, how they work, and their applications. Keep it concise. Also, the user might be a beginner, so avoid technical jargon. Maybe mention neural networks and deep learning. Oh, and the difference between models like GPT and others. But since it's a short intro, maybe just a few examples. \\n\\nI need to ensure the explanation is clear but not too detailed. Maybe start with a simple definition, then talk about their training, then their uses. Also, note that they're not just data processors but have some level of understanding. Make sure it's friendly and easy to grasp. Let me check the key points again: definition, training, applications, and maybe a sentence on their limitations. But since it's a short intro, maybe keep it to the basics. Alright, time to put it all together in a few sentences.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Give me a short introduction to large language models.\",\n",
    "    },\n",
    "]\n",
    "messages = pipe(messages, max_new_tokens=32768)[0][\"generated_text\"]\n",
    "parse_thinking_content(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model still generates a similar response with a thinking process.\n",
    "\n",
    "If you notice a significant difference, it might be due to the model, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case. However, feel free to reach out to us on [Discord]([https://discord.gg/Tun8YgzxZ9](https://discord.gg/Tun8YgzxZ9)) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Smashed Model\n",
    "\n",
    "As our smashed model is working, we can evaluate how much it has improved with our optimization. For this, we can run an evaluation of the performance using the `EvaluationAgent`. In this case, we will include metrics like `total time`,`perplexity`, `throughput` and `energy_consumed`.\n",
    "\n",
    "A complete list of the available metrics can be found in [Evaluation](https://docs.pruna.ai/en/stable/reference/evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using call_type: y_gt for metric perplexity\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Loaded only training, splitting train 80/10/10 into train, validation and test...\n",
      "INFO - Testing compatibility with text_generation_collate...\n",
      "INFO - Using provided list of metric instances.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.data.utils import split_train_into_train_val_test\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    EnergyConsumedMetric,\n",
    "    ThroughputMetric,\n",
    "    TorchMetricWrapper,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics. Increment the number of iterations\n",
    "# and warmup iterations to get a more accurate result.\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "    ThroughputMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "    TorchMetricWrapper(\"perplexity\", call_type=\"single\"),\n",
    "    EnergyConsumedMetric(n_iterations=50, n_warmup_iterations=5),\n",
    "]\n",
    "\n",
    "# Load custom datasets\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "\n",
    "train_ds = load_dataset(\"zwhe99/DeepMath-103K\", split=\"train\")\n",
    "train_ds = train_ds.rename_column(\"question\", \"text\")\n",
    "train_ds, val_ds, test_ds = split_train_into_train_val_test(train_ds, seed=42)\n",
    "\n",
    "# Create the data module\n",
    "datamodule = PrunaDataModule.from_datasets(\n",
    "    datasets=(train_ds, val_ds, test_ds),\n",
    "    collate_fn=\"text_generation_collate\",\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    collate_fn_args={\"max_seq_len\": 512},\n",
    "    dataloader_args={\"batch_size\": 16, \"num_workers\": 4},\n",
    ")\n",
    "datamodule.limit_datasets(100)\n",
    "\n",
    "inference_args = {\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "# Define the task and the evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Evaluating a smashed model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon WARNING @ 15:17:31] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:17:31] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:17:31] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:17:32] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:17:32] CPU Model on constant consumption mode: AMD EPYC 9334 32-Core Processor\n",
      "[codecarbon WARNING @ 15:17:32] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 15:17:32] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:17:32] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:17:32] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 15:17:32] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:17:32]   Platform system: Linux-6.8.0-71-generic-x86_64-with-glibc2.39\n",
      "[codecarbon INFO @ 15:17:32]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 15:17:32]   CodeCarbon version: 3.0.4\n",
      "[codecarbon INFO @ 15:17:32]   Available RAM : 235.943 GB\n",
      "[codecarbon INFO @ 15:17:32]   CPU count: 24 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 15:17:32]   CPU model: AMD EPYC 9334 32-Core Processor\n",
      "[codecarbon INFO @ 15:17:32]   GPU count: 1\n",
      "[codecarbon INFO @ 15:17:32]   GPU model: 1 x NVIDIA H100 PCIe\n",
      "[codecarbon INFO @ 15:17:35] Emissions data (if any) will be saved to file /root/sdiazlor/prunatree/pruna/docs/tutorials/emissions.csv\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n",
      "INFO - Could not load HQQ model using pipeline, trying generic HQQ pipeline...\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n",
      "100%|| 143/143 [00:00<00:00, 76895.57it/s]\n",
      "100%|| 197/197 [00:00<00:00, 14937.41it/s]\n",
      "WARNING - Model and SmashConfig have different devices. Model: cuda, SmashConfig: cuda:0. Casting model to cuda:0.If this is not desired, please use SmashConfig(device='cuda').\n",
      "INFO - Starting compiler torch_compile...\n",
      "INFO - compiler torch_compile was applied successfully.\n",
      "[codecarbon WARNING @ 15:17:38] Background scheduler didn't run for a long period (2s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:17:38] Energy consumed for RAM : 0.000041 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:17:38] Delta energy consumed for CPU with cpu_load : 0.000013 kWh, power : 21.030890349000003 W\n",
      "[codecarbon INFO @ 15:17:38] Energy consumed for All CPU : 0.000013 kWh\n",
      "[codecarbon INFO @ 15:17:38] Energy consumed for all GPUs : 0.000058 kWh. Total GPU Power : 77.1106485139789 W\n",
      "[codecarbon INFO @ 15:17:38] 0.000112 kWh of electricity used since the beginning.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon WARNING @ 15:17:48] Background scheduler didn't run for a long period (8s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:17:48] Energy consumed for RAM : 0.000189 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:17:48] Delta energy consumed for CPU with cpu_load : 0.000047 kWh, power : 21.016179744000002 W\n",
      "[codecarbon INFO @ 15:17:48] Energy consumed for All CPU : 0.000060 kWh\n",
      "[codecarbon INFO @ 15:17:48] Energy consumed for all GPUs : 0.000805 kWh. Total GPU Power : 313.14043945610945 W\n",
      "[codecarbon INFO @ 15:17:48] 0.001054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:17:48] Energy consumed for RAM : 0.000189 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:17:49] Delta energy consumed for CPU with cpu_load : 0.000000 kWh, power : 21.000000189 W\n",
      "[codecarbon INFO @ 15:17:49] Energy consumed for All CPU : 0.000060 kWh\n",
      "[codecarbon INFO @ 15:17:49] Energy consumed for all GPUs : 0.000819 kWh. Total GPU Power : 98.61115931663755 W\n",
      "[codecarbon INFO @ 15:17:49] 0.001068 kWh of electricity used since the beginning.\n",
      "/root/miniconda3/envs/pruna0/lib/python3.10/site-packages/codecarbon/output_methods/file.py:90: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_model.move_to_device(device)\n",
    "smashed_model.inference_handler.model_args.update(inference_args)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_model)\n",
    "smashed_model.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 2.823002815246582\n",
      "total_time 6869.606880187988\n",
      "throughput 0.11645498992194264\n",
      "energy_consumed 0.001068044841136194\n"
     ]
    }
   ],
   "source": [
    "for result in smashed_model_results:\n",
    "    print(result.name, result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Using best available device: 'cuda'\n",
      "INFO - Evaluating a base model.\n",
      "INFO - Detected transformers model. Using TransformerHandler.\n",
      "- The first element of the batch is passed as input.\n",
      "- The generated outputs are expected to have .logits attribute.\n",
      "INFO - Evaluating stateful metrics.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO - Evaluating isolated inference metrics.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon WARNING @ 15:19:04] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:19:04] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:19:04] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:19:05] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:19:05] CPU Model on constant consumption mode: AMD EPYC 9334 32-Core Processor\n",
      "[codecarbon WARNING @ 15:19:05] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 15:19:05] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:19:05] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:19:05] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 15:19:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:19:05]   Platform system: Linux-6.8.0-71-generic-x86_64-with-glibc2.39\n",
      "[codecarbon INFO @ 15:19:05]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 15:19:05]   CodeCarbon version: 3.0.4\n",
      "[codecarbon INFO @ 15:19:05]   Available RAM : 235.943 GB\n",
      "[codecarbon INFO @ 15:19:05]   CPU count: 24 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 15:19:05]   CPU model: AMD EPYC 9334 32-Core Processor\n",
      "[codecarbon INFO @ 15:19:05]   GPU count: 1\n",
      "[codecarbon INFO @ 15:19:05]   GPU model: 1 x NVIDIA H100 PCIe\n",
      "[codecarbon INFO @ 15:19:08] Emissions data (if any) will be saved to file /root/sdiazlor/prunatree/pruna/docs/tutorials/emissions.csv\n",
      "INFO - Using best available device: 'cuda'\n",
      "WARNING - Argument cache_dir not found in config file. Skipping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dede44f21634cd8982c9700046c6d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:19:10] Background scheduler didn't run for a long period (1s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:19:10] Energy consumed for RAM : 0.000026 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:19:10] Delta energy consumed for CPU with cpu_load : 0.000008 kWh, power : 21.006048756000002 W\n",
      "[codecarbon INFO @ 15:19:10] Energy consumed for All CPU : 0.000008 kWh\n",
      "[codecarbon INFO @ 15:19:10] Energy consumed for all GPUs : 0.000042 kWh. Total GPU Power : 78.18919233641185 W\n",
      "[codecarbon INFO @ 15:19:10] 0.000077 kWh of electricity used since the beginning.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon WARNING @ 15:20:02] Background scheduler didn't run for a long period (47s), results might be inaccurate\n",
      "[codecarbon INFO @ 15:20:02] Energy consumed for RAM : 0.000903 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:20:03] Delta energy consumed for CPU with cpu_load : 0.000279 kWh, power : 21.02335595850001 W\n",
      "[codecarbon INFO @ 15:20:03] Energy consumed for All CPU : 0.000288 kWh\n",
      "[codecarbon INFO @ 15:20:03] Energy consumed for all GPUs : 0.004665 kWh. Total GPU Power : 344.4841233934866 W\n",
      "[codecarbon INFO @ 15:20:03] 0.005855 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:20:03] Energy consumed for RAM : 0.000907 kWh. RAM Power : 66.0 W\n",
      "[codecarbon INFO @ 15:20:04] Delta energy consumed for CPU with cpu_load : 0.000001 kWh, power : 21.000000189 W\n",
      "[codecarbon INFO @ 15:20:04] Energy consumed for All CPU : 0.000289 kWh\n",
      "[codecarbon INFO @ 15:20:04] Energy consumed for all GPUs : 0.004710 kWh. Total GPU Power : 218.40251802937132 W\n",
      "[codecarbon INFO @ 15:20:04] 0.005907 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base model and offload it to CPU\n",
    "base_pipe = PrunaModel(model=copy_model)\n",
    "base_pipe.move_to_device(device)\n",
    "base_pipe.inference_handler.model_args.update(inference_args)\n",
    "base_model_results = eval_agent.evaluate(base_pipe)\n",
    "base_pipe.move_to_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 3.332951068878174\n",
      "total_time 42390.90364074707\n",
      "throughput 0.01887197326057995\n",
      "energy_consumed 0.005906576510335687\n"
     ]
    }
   ],
   "source": [
    "for result in base_model_results:\n",
    "    print(result.name, result.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results of the evaluation and compare the performance of the original and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Metric | Base Model | Compressed Model | Relative Difference |\n",
       "|--------|----------|-----------|------------|\n",
       "| perplexity | 3.3330  | 2.8230  | -15.30% |\n",
       "| total_time | 42390.9036  ms | 6869.6069  ms | -83.79% |\n",
       "| throughput | 0.0189  num_iterations/ms | 0.1165  num_iterations/ms | +517.08% |\n",
       "| energy_consumed | 0.0059  kWh | 0.0011  kWh | -81.92% |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display  # noqa\n",
    "\n",
    "\n",
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate differences and prepare table data\n",
    "table_data = []\n",
    "for base_metric_result, smashed_metric_result in zip(\n",
    "    base_model_results, smashed_model_results\n",
    "):\n",
    "    diff = calculate_percentage_diff(\n",
    "        base_metric_result.result, smashed_metric_result.result\n",
    "    )\n",
    "    table_data.append(\n",
    "        {\n",
    "            \"Metric\": base_metric_result.name,\n",
    "            \"Base Model\": f\"{base_metric_result.result:.4f}\",\n",
    "            \"Compressed Model\": f\"{smashed_metric_result.result:.4f}\",\n",
    "            \"Relative Difference\": f\"{diff:+.2f}%\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create and display markdown table manually\n",
    "markdown_table = \"| Metric | Base Model | Compressed Model | Relative Difference |\\n\"\n",
    "markdown_table += \"|--------|----------|-----------|------------|\\n\"\n",
    "for row in table_data:\n",
    "    metric_obj = [metric for metric in metrics if metric.metric_name == row[\"Metric\"]][\n",
    "        0\n",
    "    ]\n",
    "    unit = f\" {metric_obj.metric_units}\" if hasattr(metric_obj, \"metric_units\") else \"\"\n",
    "    markdown_table += f\"| {row['Metric']} | {row['Base Model']} {unit} | {row['Compressed Model']} {unit} | {row['Relative Difference']} |\\n\"  # noqa: E501\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can observe a slight improvement of the model. So, we can save the optimized model to disk or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "smashed_model.save_pretrained(\"Qwen3-0.6B-smashed\")\n",
    "# Load the model from disk\n",
    "# smashed_model = PrunaModel.from_pretrained(\"Qwen3-0.6B-smashed/\")\n",
    "\n",
    "# Save the model to HuggingFace\n",
    "# smashed_model.save_to_hub(\"PrunaAI/Qwen3-0.6B-smashed\")\n",
    "# smashed_model = PrunaModel.from_hub(\"PrunaAI/Qwen3-0.6B-smashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we have seen how to optimize and evaluate a reasoning Large Language Model using Pruna. We have seen how to use the `SmashConfig` to customize the optimizations applied during smashing and how to evaluate the performance of the optimized model using the `EvaluationAgent`.\n",
    "\n",
    "The results show that\n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on how to optimize and evaluate image/video generation models or LLM models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruna0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
