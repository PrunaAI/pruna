{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress and Evaluate a Reasoning LLM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrunaAI/pruna/blob/v|version|/docs/tutorials/reasoning_llm.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Component | Details |\n",
    "|-----------|---------|\n",
    "| **Goal** | Showcase a standard workflow for optimizing and evaluating a reasoning Large Language Model |\n",
    "| **Model** |[Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) |\n",
    "| **Dataset** |   |\n",
    "| **Optimization Algorithms** |  |\n",
    "| **Evaluation Metrics** | `total time`, `latency`, `perplexity`, `throughput`, `energy_consumed`, `co2_emissions` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To install the required dependencies, you can run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pruna\n",
    "%pip install ftfy imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how to install Pruna, please refer to the [Installation](https://docs.pruna.ai/en/stable/setup/install.html) page.\n",
    "\n",
    "Then, we will set the device to the best available option to maximize the optimization process's benefits. However, in this case, we recommend using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we will load the original model and tokenizer using the transformers library. In our case, we will use one of the small versions of Qwen3, [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) just as a starting point. However, Pruna works at least as well with larger models, so feel free to use a bigger version of Qwen3 or any other [reasoning model available on Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfoat16, device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've loaded the model and tokenizer, we can try to generate a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model_name_or_path,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Give me a short introduction to large language models.\",\n",
    "    },\n",
    "]\n",
    "messages = generator(messages, max_new_tokens=32768)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_thinking_content(messages):\n",
    "    messages = copy.deepcopy(messages)\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\" and (\n",
    "            m := re.match(\n",
    "                r\"<think>\\n(.+)</think>\\n\\n\", message[\"content\"], flags=re.DOTALL\n",
    "            )\n",
    "        ):\n",
    "            message[\"content\"] = message[\"content\"][len(m.group(0)) :]\n",
    "            if thinking_content := m.group(1).strip():\n",
    "                message[\"reasoning_content\"] = thinking_content\n",
    "    return messages\n",
    "\n",
    "\n",
    "parse_thinking_content(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the SmashConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our base model is lodaded and tested, we can specify the `SmashConfig` to customize the optimizations applied during smashing.\n",
    "\n",
    "Not every optimization algorithm works with every model. You can learn about the requirements and compatibility in the [Algorithms Overview](https://docs.pruna.ai/en/stable/compression.html).\n",
    "\n",
    "In this example, we will enable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import SmashConfig\n",
    "\n",
    "smash_config = SmashConfig()\n",
    "# Select the quantizer\n",
    "smash_config[\"quantizer\"] = \"hqq\"\n",
    "smash_config[\"hqq_weight_bits\"] = (\n",
    "    4  # can work with 2, 8 also (but 4 is the best performance)\n",
    ")\n",
    "smash_config[\"hqq_compute_dtype\"] = (\n",
    "    \"torch.bfloat16\"  # can work with float16, but better performance with bfloat16\n",
    ")\n",
    "\n",
    "# Select torch_compile for the compilation\n",
    "smash_config[\"compiler\"] = \"torch_compile\"\n",
    "# smash_config['torch_compile_max_kv_cache_size'] = 400 # uncomment if you want to use a custom kv cache size\n",
    "smash_config[\"torch_compile_fullgraph\"] = True\n",
    "smash_config[\"torch_compile_mode\"] = \"max-autotune\"\n",
    "# If the model is not compatible with cudagraphs, you can try to comment the line above and uncomment the line below\n",
    "# smash_config['torch_compile_mode'] = 'max-autotune-no-cudagraphs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smash the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `SmashConfig` defined, it’s time to apply it to our base model. We’ll call the `smash` function with the base model and our `SmashConfig`\n",
    "\n",
    "Ready to smash? This operation typically takes around 20 seconds, depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from pruna import smash\n",
    "\n",
    "copy_pipe = copy.deepcopy(pipe).to(\"cpu\")\n",
    "smashed_pipe = smash(\n",
    "    model=pipe,\n",
    "    smash_config=smash_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have our optimized smashed model. Let's check how it works by running some inference.\n",
    "\n",
    "Consider that if you are using `torch_compile` as a compiler, you can expect the first inference warmup to take a bit longer than the actual inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model_name_or_path,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Give me a short introduction to large language models.\",\n",
    "    },\n",
    "]\n",
    "messages = generator(messages, max_new_tokens=32768)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model still generates a similar response with a thinking process.\n",
    "\n",
    "If you notice a significant difference, it might be due to the model, the configuration, the hardware, etc. As optimization can be non-deterministic, we encourage you to retry the optimization process or try out different configurations and models to find the best fit for your use case. However, feel free to reach out to us on [Discord]([https://discord.gg/Tun8YgzxZ9](https://discord.gg/Tun8YgzxZ9)) if you have any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Smashed Model\n",
    "\n",
    "As our smashed model is working, we can evaluate how much it has improved with our optimization. For this, we can run an evaluation of the performance using the `EvaluationAgent`. In this case, we will include metrics like `total time`, `latency`, `perplexity`, `throughput`, `energy_consumed` and `co2_emissions`.\n",
    "\n",
    "A complete list of the available metrics can be found in [Evaluation](https://docs.pruna.ai/en/stable/reference/evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruna import PrunaModel\n",
    "from pruna.data.pruna_datamodule import PrunaDataModule\n",
    "from pruna.evaluation.evaluation_agent import EvaluationAgent\n",
    "from pruna.evaluation.metrics import (\n",
    "    Co2EmissionsMetric,\n",
    "    EnergyConsumedMetric,\n",
    "    LatencyMetric,\n",
    "    ThroughputMetric,\n",
    "    TotalTimeMetric,\n",
    ")\n",
    "from pruna.evaluation.task import Task\n",
    "\n",
    "# Define the metrics. Increment the number of iterations and warmup iterations to get a more accurate result.\n",
    "metrics = [\n",
    "    TotalTimeMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    LatencyMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    ThroughputMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    Co2EmissionsMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "    EnergyConsumedMetric(n_iterations=3, n_warmup_iterations=1),\n",
    "]\n",
    "\n",
    "# Define the datamodule\n",
    "datamodule = PrunaDataModule.from_string(\"LAION256\")\n",
    "datamodule.limit_datasets(10)\n",
    "\n",
    "# Define the task and the evaluation agent\n",
    "task = Task(metrics, datamodule=datamodule, device=device)\n",
    "eval_agent = EvaluationAgent(task)\n",
    "\n",
    "# Evaluate base model and offload it to CPU\n",
    "base_pipe = PrunaModel(model=copy_pipe)\n",
    "base_pipe.move_to_device(device)\n",
    "base_model_results = eval_agent.evaluate(base_pipe)\n",
    "base_pipe.move_to_device(\"cpu\")\n",
    "\n",
    "# Evaluate smashed model and offload it to CPU\n",
    "smashed_pipe.move_to_device(device)\n",
    "smashed_model_results = eval_agent.evaluate(smashed_pipe)\n",
    "smashed_pipe.move_to_device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results of the evaluation and compare the performance of the original and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display  # noqa\n",
    "\n",
    "\n",
    "# Calculate percentage differences for each metric\n",
    "def calculate_percentage_diff(original, optimized):  # noqa\n",
    "    return ((optimized - original) / original) * 100\n",
    "\n",
    "\n",
    "# Calculate differences and prepare table data\n",
    "table_data = []\n",
    "for base_metric_result, smashed_metric_result in zip(\n",
    "    base_model_results, smashed_model_results\n",
    "):\n",
    "    diff = calculate_percentage_diff(\n",
    "        base_metric_result.result, smashed_metric_result.result\n",
    "    )\n",
    "    table_data.append(\n",
    "        {\n",
    "            \"Metric\": base_metric_result.name,\n",
    "            \"Base Model\": f\"{base_metric_result.result:.4f}\",\n",
    "            \"Compressed Model\": f\"{smashed_metric_result.result:.4f}\",\n",
    "            \"Relative Difference\": f\"{diff:+.2f}%\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create and display markdown table manually\n",
    "markdown_table = \"| Metric | Base Model | Compressed Model | Relative Difference |\\n\"\n",
    "markdown_table += \"|--------|----------|-----------|------------|\\n\"\n",
    "for row in table_data:\n",
    "    metric_obj = [metric for metric in metrics if metric.metric_name == row[\"Metric\"]][\n",
    "        0\n",
    "    ]\n",
    "    unit = f\" {metric_obj.metric_units}\" if hasattr(metric_obj, \"metric_units\") else \"\"\n",
    "    markdown_table += f\"| {row['Metric']} | {row['Base Model']} {unit} | {row['Compressed Model']} {unit} | {row['Relative Difference']} |\\n\"  # noqa: E501\n",
    "\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can observe a slight improvement of the model. So, we can save the optimized model to disk or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "smashed_pipe.save_pretrained(\"Qwen3-1.7B-smashed\")\n",
    "# Load the model from disk\n",
    "# smashed_pipe = PrunaModel.from_pretrained(\"Qwen3-1.7B-smashed/\")\n",
    "\n",
    "# Save the model to HuggingFace\n",
    "# smashed_pipe.save_to_hub(\"PrunaAI/Qwen3-1.7B-smashed\")\n",
    "# smashed_pipe = PrunaModel.from_hub(\"PrunaAI/Qwen3-1.7B-smashed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we have seen how to optimize and evaluate a reasoning Large Language Model using Pruna. We have seen how to use the `SmashConfig` to customize the optimizations applied during smashing and how to evaluate the performance of the optimized model using the `EvaluationAgent`.\n",
    "\n",
    "The results show that\n",
    "\n",
    "Check out our other [tutorials](https://docs.pruna.ai/en/stable/docs_pruna/tutorials/index.html) for more examples on how to optimize and evaluate image/video generation models or LLM models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
